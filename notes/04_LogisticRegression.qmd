---
title: "**Logistic Regression and Neural Networks**"
format:
  html:
    toc: true
    math: true
---


## Neural Networks

We have seen how gradient descent can be used to efficiently optimize linear models, but what if the relationship between the inputs and outputs is not linear?
We can still use gradient descent to optimize non-linear models, but we need to expand our model class to include non-linear functions.

<center><img src="images/regression_vs_network.pdf" width="500"></center>

In the right of the figure, we have a linear model, which uses a linear combination of the inputs to make predictions.
We can generalize this approach by repeatedly combining linear models to create a more complex model.
In particular, we create several *neurons*, each of which is a linear model that takes the inputs and produces an output.
Each neuron computes a linear combination of the inputs, and then we linearly combine the outputs of the neurons, and so on.
The resulting model is a *neural network*, which combines linear models to create a more complex model.

We can think of a fully connected layer (all neurons in one layer are connected to all neurons in the next layer) as matrix multiplication, where the matrix $\mathbf{W}^{(\ell)} \in \mathbb{R}^{d_{\ell} \times d_{\ell+1}}$ is the weight matrix and $\mathbf{x} \in \mathbb{R}^{d_\text{in}}$ is the input to the $\ell$th layer.
Then multiplying several weight matrices together gives
$$
f(\mathbf{x}) = \mathbf{W}^{(k)} \mathbf{W}^{(k-1)} \cdots \mathbf{W}^{(1)} \mathbf{x} 
$$
where $k$ is the number of layers in the neural network.
But, there's an issue with this approach: the output is still a linear combination of the inputs, which means that the model is still linear.
Put differently, we could just multiply all the weight matrices together to get a single weight matrix $\mathbf{W} = \mathbf{W}_k \mathbf{W}_{k-1} \cdots \mathbf{W}_1$ before ever seeing the input $\mathbf{x}$.
Our solution is to add a *non-linear activation function* after each layer.
Formally, we apply activation functions $\sigma: \mathbb{R} \to \mathbb{R}$ element-wise to the output of each layer.
Examples of common activation functions include:

- **ReLU (Rectified Linear Unit):** $\sigma(x) = \max(0, x)$, which is a piecewise linear function that is $0$ for negative inputs and linear for positive inputs.

- **Sigmoid:** $\sigma(x) = \frac{1}{1 + e^{-x}}$, which is a smooth function that maps inputs to the range $(0, 1)$.

- **Tanh (Hyperbolic Tangent):** $\sigma(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$, which is another smooth function that maps inputs to the range $(-1, 1)$.

Now we can write the output of the neural network as
$$
f(\mathbf{x}) = \sigma(\mathbf{W}_k \sigma(\mathbf{W}_{k-1} \cdots \sigma(\mathbf{W}_1 \mathbf{x}))).
$$
Thanks to the non-linear activation functions, the output is no longer a linear combination of the inputs, but rather a complex function that can capture non-linear relationships between the inputs and outputs.
Neural networks are so complex, in fact, that we don't really understand how they work.
But we *can* still apply gradient descent. In summary, we can think of neural networks in following empirical risk minimization formulation:

- **Model Class:** The model class is the set of neural networks with a given architecture (type of layer, number of layers, number of neurons per layer, and activation function).

- **Loss:** The loss function is the mean squared error loss.

- **Optimizer:** The optimizer is gradient descent.