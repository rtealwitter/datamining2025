---
title: "**Probability and Logistic Regression**"
format:
  html:
    toc: true
    math: true
    include-after-body: readtime.html
---

Instead of predicting a continuous value, there are many applications where we want to predict a discrete value. For example, we might want to predict whether an email is spam or not, whether a patient has a disease or not, or whether an image contains a cat or not. In these cases, we can use a **classification** model instead of a regression model.

## Probability

We will review some basic concepts of probability that are useful for understanding classification models. We will also introduce Bayes' Rule, and see how useful it can be for making predictions.

Let $A$ and $B$ be two events. For example, $A$ could be the event that it rains tomorrow, and $B$ could be the event that I wear a raincoat tomorrow.
We will use the following notation:  
• $\Pr(A)$ represents the probability that event $A$ occurs,  
• $\Pr(A \cap B)$ represents the probability that both events $A$ and $B$ occur,  
• $\Pr(A \cup B)$ represents the probability that either event $A$ or event $B$ occurs,  
• $\Pr(A | B)$ represents the probability that event $A$ occurs given that event $B$ has occurred.

Let's see it in a Venn diagram.

[image here]

We can reason through several properties of probabilities:

**Probability Range** The probability of an event is always between 0 and 1, inclusive: $0 \leq \Pr(A) \leq 1$. An event with probability 0 never occurs, while an event with probability 1 is certain to occur.  

**Conditional Probabilities** We can write the probability of both events $A$ and $B$ occurring in terms of conditional probabilities:
  $$\Pr(A \cap B) = \Pr(A | B) \Pr(B) = \Pr(B | A) \Pr(A).$$
  This means that the probability of both events occurring can be expressed as the product of the probability of one event given the other and the probability of the other event.  

**Union of Events** The probability that either event $A$ or event $B$ occurs is given by:
  $$\Pr(A \cup B) = \Pr(A) + \Pr(B) - \Pr(A \cap B).$$
  This means that the probability of either event occurring is the sum of the probabilities of each event minus the probability of both events occurring together, to avoid double counting.  

**Complement Rule** The *complement* of an event $A$ is the event that $A$ does not occur, denoted as $\neg A$. Since either $A$ occurs or $\neg A$ occurs, we have that $Pr(A) + \Pr(\neg A) = 1$. Rearranging this gives us that $\Pr(\neg A) = 1 - \Pr(A)$.  

**Independence** Two events $A$ and $B$ are *independent* if the occurrence of one event does not affect the probability of the other event occurring. In this case, we have that $\Pr(A \cap B) = \Pr(A) \Pr(B)$. Equivalently, $\Pr(A | B) = \Pr(A)$ and $\Pr(B | A) = \Pr(B)$, do you see why this follows?

We will often model random events with *random variables*. A random variable is a function that maps outcomes to real numbers. For example, we could define a random variable $X$ that takes the outcome of a die roll and maps it to the number on the die. The probability distribution of a random variable describes the probabilities of each possible outcome. In our die example, if we roll a fair six-sided die, the probability distribution of the random variable $X$ is given by $\Pr(X = x) = \frac{1}{6}$ for $x \in \{1, 2, 3, 4, 5, 6\}$.

**Bayes' Rule** A particularly useful rule in machine learning is **Bayes' Rule**, which allows us to more easily compute conditional probabilities. Formally, Bayes' Rule states that for any two events $A$ and $B$ with $\Pr(B) > 0$,
$$\Pr(A | B) = \frac{\Pr(B | A) \Pr(A)}{\Pr(B)}.$$

Based on what we have so far learned about probability, can you prove Bayes' Rule?

### Maximum A Posteriori (MAP) Estimation

When we justified the mean squared error loss for regression, we said that it was the maximum likelihood estimate (MLE) of the parameters of a linear model. We can extend this idea to classification models as well.
Suppose we have a *binary classification* problem, where we want to predict whether the random variable $Y=0$ or $Y=1$.
We observe evidence $\mathbf{X} = \mathbf{x}$, e.g., $\mathbf{X}$ is the random variable that takes on the value of the features of an email, and we want to predict whether the email is spam or not.
We will compare the posteriors
$$\Pr(Y = 1 | \mathbf{X} = \mathbf{x})$$
and
$$\Pr(Y = 0 | \mathbf{X} = \mathbf{x})$$
to determine which class is more likely given the evidence.

However, it's not immediately clear how to compute these probabilities.
Luckily, we can use Bayes' Rule to rewrite the posteriors.
Without loss of generality, consider the event that $Y = 1$.
Then
$$
\begin{align*}
\Pr(Y = 1 | \mathbf{X} = \mathbf{x})
&= \frac{\Pr(\mathbf{X} = \mathbf{x} | Y = 1) \Pr(Y = 1)}{\Pr(\mathbf{X} = \mathbf{x})} \\
&= \frac{\textnormal{likelihood} \cdot \textnormal{prior}}{\textnormal{evidence}}
\end{align*}.
$$

Let's get some familiarity through a medical example.
Suppose we have a medical test that can detect whether a patient has a particular disease.
The disease is rare, affecting only 1% of the population.
The test is unfortunately not perfect: it has a false positive rate of 5% and a false negative rate of 10%.
Suppose $X=1$ i.e., the test is positive.
Is it more likely that the patient has the disease or not?

In this medical example, we were explicitly given the likelihood of the disease. What can we do when our only information is the labelled data?

### Naive Bayes Classifier

The **Naive Bayes Classifier** is a simple yet effective classification algorithm that uses Bayes' Rule to make predictions. The key assumption of the Naive Bayes Classifier is that the features are conditionally independent given the class label. This means that the presence or absence of a feature does not affect the presence or absence of another feature, given the class label.

Let's see an example with a spam classifier.
Suppose we have a dataset of emails, each labelled as either spam or not spam. We want to predict whether a new email is spam or not based on its features, such as the presence of certain words.
In particular, let $\mathbf{X} = (X_1, X_2, \ldots, X_d)$ be the features of the email, where $X_i$ is a binary variable indicating whether the $i$th word is present in the email.
Let $p_i^{(1)} = \Pr(X_i=1 | Y=0)$ be the probability that the $i$th word is present in a spam email, while $p_i^{(0)} = \Pr(X_i=1 | Y=1)$ be the probability that the $i$th word is present in a non-spam email.
We will use the independence assumption to compute the likelihood of the features given the class label.
For example,
$$
\Pr(\mathbf{X} = (0, 1, 0, 0, 1) | Y = 1) =
(1-p_1^{(1)}) p_2^{(1)} (1-p_3^{(1)}) (1-p_4^{(1)}) p_5^{(1)}.
$$

Beyond the likelihood, we also need the prior probability of the class label.
This is even easier to compute, e.g., we can simply compute the fraction of spam and non-spam emails in the training set.
Then we can use Bayes' Rule to determine whether the posterior probability of the email being spam is greater than the posterior probability of the email being non-spam.

More formally, we can use the Naive Bayes Classifier by

1. Computing $\Pr(Y = 1)$ and $\Pr(Y = 0)$ from the training data.

2. Computing the observed probabilities $\Pr(X_i = 1 | Y = 1)$ and $\Pr(X_i = 1 | Y = 0)$ for each feature $X_i$ from the training data.

3. Computing the likelihoods $\Pr(\mathbf{X} = \mathbf{x} | Y=1)$ and $\Pr(\mathbf{X} = \mathbf{x} | Y=0)$ using the independence assumption e.g.,
$$
\Pr(\mathbf{X} = \mathbf{x} | Y=1) = \prod_{i=1}^d \Pr(X_i = x_i | Y=1).
$$

4. Using Bayes' Rule to compute the posteriors, and predicting the class label $y \in \{0,1\}$ with the highest posterior probability:
$$
\Pr(Y = y | \mathbf{X} = \mathbf{x}) = \frac{\Pr(\mathbf{X} = \mathbf{x} | Y=y) \Pr(Y=y)}{\Pr(\mathbf{X} = \mathbf{x})}.
$$

## Logistic Regression

With a little probability, we saw how the Naive Bayes Classifier can be used to make predictions.
However, the Naive Bayes Classifier makes a strong independence assumption that is often violated in practice.
As an alternative approach to classification problems, we will see how we can generalize linear regression, with a little non-linearity.

Our setup will be the standard supervised learning setting, where we have labelled data $(\mathbf{x}^{(1)}, y^{(1)}), \ldots, (\mathbf{x}^{(n)}, y^{(n)})$, for $\mathbf{x}^{(i)} \in \mathbb{R}^d$ the feature vector for the $i$th example. 
However, unlike regression where we predict a continuous value $y^{(i)} \in \mathbb{R}$, we will now predict a binary value $y^{(i)} \in \{0, 1\}$.
We can use the same linear model as before, i.e., we will predict the output as $\langle \mathbf{w}, \mathbf{x}^{(i)} \rangle$, where $\mathbf{w} \in \mathbb{R}^d$ is the weight vector.
But, we run into an issue: the output of the linear model can take on any real value, but we want to predict a binary value.

Attempt #1: We could simply threshold the output of the linear model, i.e., predict $y^{(i)} = 1$ if $\langle \mathbf{w}, \mathbf{x}^{(i)} \rangle > 0$ and $y^{(i)} = 0$ otherwise. The loss could be the difference between the predicted value and the true value, i.e., $\mathcal{L}(\mathbf{w}) = \sum_{i=1}^n |y^{(i)} - \langle \mathbf{w}, \mathbf{x}^{(i)} \rangle|^2$. However, this loss is not differentiable, so we cannot use gradient descent to optimize it.

Attempt #2: We could use the mean squared error loss, i.e., $\mathcal{L}(\mathbf{w}) = \sum_{i=1}^n (y^{(i)} - \langle \mathbf{w}, \mathbf{x}^{(i)} \rangle)^2$. This loss is differentiable, but it does not work well for classification problems: if we have a large positive value for $\langle \mathbf{w}, \mathbf{x}^{(i)} \rangle$, the loss will be large even if $y^{(i)} = 1$.

Attempt #3: We can apply the *sigmoid function* to the output of the linear model to map it to the range $(0, 1)$, i.e., we will predict $f(\mathbf{x}^{(i)}) = \sigma(\langle \mathbf{w}, \mathbf{x}^{(i)} \rangle)$, where $\sigma(z) = \frac{1}{1 + e^{-z}}$ is the sigmoid function. The sigmoid function is a smooth, non-linear function that maps any real number to the range $(0, 1)$. We can then interpret $f(\mathbf{x}^{(i)})$ as the probability that $y^{(i)} = 1$ given the features $\mathbf{x}^{(i)}$. If we need to report a class label, we can threshold the predicted probability, e.g., predict $y^{(i)} = 1$ if $f(\mathbf{x}^{(i)}) > \frac12$ and $y^{(i)} = 0$ otherwise.

To train our model, we need a loss function that measures how well our predicted probabilities match the true labels. A common choice is the *binary cross-entropy loss*, which is defined as
$$\mathcal{L}(\mathbf{w}) = -\frac{1}{n} \sum_{i=1}^n \left[y^{(i)} \log(f(\mathbf{x}^{(i)})) + (1 - y^{(i)}) \log(1 - f(\mathbf{x}^{(i)}))\right].$$
This loss function measures the distance (in a sense we'll explore later in the course) between the predicted probabilities and the true labels. It is differentiable, so we can use gradient descent to optimize it.

[image here] Comparing the predictions by $\langle \mathbf{w}, \mathbf{x}^{(i)} \rangle$

[image here] Comparing the loss when $y=1$ by $\langle \mathbf{w}, \mathbf{x}^{(i)} \rangle$

### Non-linear Transformations

Often, our data is not linearly separable, i.e., we cannot draw a straight line to separate the two classes.
In this case, we can use non-linear transformations to map the data to a higher-dimensional space, where it *is* linearly separable.
For example, as we saw for linear regression, we can add polynomial features to the data.

[image here] Circle data that is not linearly separable, and the cone transformation that makes it linearly separable.

It is not a priori clear which non-linear transformation will work best for a given dataset.
In several lectures, we will explore how to use kernel methods to implicitly map the data to a higher-dimensional space, which captures many of the non-linear transformations we might want to use.

### Measuring Error in Classification

The simplest way to measure the error of a classification model is to compute the *error rate*, which is the fraction of examples that are misclassified. For example, if we have $n$ examples and our model misclassifies $k$ of them, the error rate is $\frac{k}{n}$.

However, the error rate does not take into account *which* points are misclassified.
We will often break down the accuracy of a classification model into four categories:  
• **True Positives (TP):** The model correctly predicts a positive class.  
• **True Negatives (TN):** The model correctly predicts a negative class.  
• **False Positives (FP):** The model incorrectly predicts a positive class when the true class is negative.    
• **False Negatives (FN):** The model incorrectly predicts a negative class when the true class is positive.

[image here] Venn diagram of true positives, true negatives, false positives, and false negatives.

The raw counts of these four categories can be summarized in a **confusion matrix**. The confusion matrix is a square matrix with dimensions equal to the number of classes, where the rows represent the true classes and the columns represent the predicted classes.

But, these raw counts themselves are not very informative.

We often report the **True Positive Rate (TPR)**, also known as *recall*, which is the fraction of true positives out of all actual positives:
$\text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}.$
The TPR measures how well the model identifies positive examples (higher is better).

We also report the **False Positive Rate (FPR)**, which is the fraction of false positives out of all actual negatives:
$\text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}.$
The FPR measures how often the model incorrectly identifies negative examples as positive (lower is better).

Finally, we can report the **Precision**, which is the fraction of true positives out of all predicted positives:
$\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}.$
Precision measures how well the model identifies positive examples among all predicted positives (higher is better).

If we have a model that does not achieve the desired TPR or FPR, we have a hidden lever we can pull: the threshold for predicting a positive class.
By default, we predict a positive class if the predicted probability is greater than $\frac12$.
But, we can change this threshold to an arbitrary value $\tau \in [0, 1]$.
Increasing the threshold can only decrease the TPR, since we are less likely to predict a positive class;
simultaneously, the FPR can only increase, since we are more likely to predict a negative class.

[image here] 2D plots with linearly separable data, with different thresholds $\tau$.

We can visualize the trade-off between TPR and FPR by plotting the **Receiver Operating Characteristic (ROC) curve**. The ROC curve is a plot of the TPR against the FPR for different threshold values $\tau$.
Because a higher TPR is better and a lower FPR is better, we want the ROC curve to be as close to the top-left corner as possible.
The area under the ROC curve (AUC) is a single number that summarizes the performance of the model across all threshold values.
A model with an AUC of 1 is perfect, while a model with an AUC of 0.5 is no better than random guessing.

[image here] ROC curve with different thresholds $\tau$.

### Multiple Classes

### Optimization
