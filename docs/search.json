[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "MATH 166: Syllabus",
    "section": "",
    "text": "Course Description: Data mining is the process of discovering patterns in large data sets using techniques from mathematics, computer science and statistics with applications ranging from biology and neuroscience to history and economics. The goal of the course is to teach students fundamental data mining techniques that are commonly used in practice. Students will learn advanced data mining techniques (including linear classifiers, clustering, dimension reduction, transductive learning and topic modeling).\nPrerequisites: I expect familiarity with calculus, linear regression, probability, and Python. In particular, I expect you are comfortable with derivatives, the chain rule, gradients, matrix multiplication, and probability distributions. If this isn’t the case, please contact me as soon as possible.\nStructure: We will meet on Tuesdays and Thursdays. The first section is from 2:45 to 4pm and the second section is from 4:15 to 5:30pm. I will hold my office hours TBD. If you would like to meet outside of these times, please email me.\nResources: The primary resource for this class are the typed notes on the homepage. I highly recommend reading them before each class (it should take about 15 minutes). In addition, I will post my preparation for the slides the night before each class.\nDiscussion: Please post all your course related questions on Canvas. If your question reveals your solution to a homework problem, please email me instead.\n\nGrading\nYour grade in the class will be based on the number of points \\(p\\) that you earn. You will receive an A if \\(p \\geq 93\\), an A- if \\(93 &gt; p \\geq 90\\), a B+ if \\(90 &gt; p \\geq 87\\), and so on. You may earn points through the following assignments:\n\nParticipation (10 points): The classes at CMC are intentionally small. Unless you have a reasonable excuse (e.g. sickness, family emergency), I expect you to attend every class. Whether you are able to attend or not, I expect you to fill out the form linked from the home page to receive credit for participation (one point per lecture day that you fill it out). Of course, if you are not able to attend in person, you should read the notes before filling out the form.\nProblem Sets (10 Points): Learning requires practice. Your main opportunity to practice the concepts we cover in this class will be on the problem sets. Your grade will be based on turning in solutions to each problem and, so that you engage with the solutions, a self grade of your own work. Because I do not want to incentivize the use of LLMs, I will not grade your solutions for correctness; instead, your problem set grade is based on completion and the accuracy of your own self grade.\nQuizzes (20 Points): In lieu of grading for correctness on the problem sets, I will give short quizzes at the beginning of our Tuesday classes. These quizzes will be based on the problem sets and will test your understanding of the concepts we cover in class. The quizzes will be short (10-15 minutes) and will be graded for correctness.\nWritten Exam (20 Points): The first midterm will be a written exam. It will cover the material we have covered in class up to that point. The exam will be open book and open notes, but you will not be allowed to use any electronic devices (including your phone). The exam will be graded for correctness.\nVerbal Exam (20 Points): The second midterm will be a verbal exam. I will individually ask you questions about the concepts we have covered in class during a 30-minute meeting. The goal is to simultaneously assess your understanding of the material and give you a chance to practice explaining the concepts, as you would in a technical interview. I will provide a list of topics that I will ask about in advance.\nProject (20 Points): The final project will be a chance for you to apply the concepts we have covered in class to a real-world problem. You will select a topic we cover in class and implement an algorithm we discussed on a data set of your choosing. You will write a report describing your results and what you learned. You will also give a presentation showcasing your results to the class. Except in special circumstances, you will complete your project as an individual.\nExtra Credit: This is the first time I am teaching this class, so my typed notes are work in progress and I would love your help improving them! If you find an issue, please email me. I will give extra credit to the first person to find each typo (worth 1/4 point), ambiguous statement (worth 1/2 point), and mistake (worth 1 point)\n\nLate Policy: I expect all assignments to be turned in on time. If you are unable to turn in an assignment on time, you must email me 24 hours before the assignment is due to request an extension.\n\n\nHonor Code\nAcademic integrity is an important part of your learning experience. You are welcome to use online material and discuss problems with others but you must explicitly acknowledge the outside resources (website, person, or LLM) on the work you submit.\nLarge Language Models: LLMs are a powerful tool. However, while they are very good at producing human-like text, they have no inherent sense of ‘correctness’. You may use LLMs (as detailed below) but you are wholly responsible for the material you submit.\nYou may use LLMs for:\n\nImplementing short blocks of code that you can easily check.\nAnswering simple questions whose answers you can easily verify.\n\nDo not use LLMS for:\n\nImplementing extensive blocks of code or code that you don’t understand.\nAnswering complicated questions (like those on the problem sets) that you cannot easily verify.\n\nUltimately, the point of the assignments in this class are for you to practice the concepts. If you use an LLM in lieu of practice, then you deny yourself the chance to learn.\n\n\nAcademic Accommodations\nIf you have a Letter of Accommodation, please contact me as early in the semester as possible. If you do not have a Letter of Accommodation and you believe you are eligible, please reach out to Accessibility Services at accessibilityservices@cmc.edu."
  },
  {
    "objectID": "notes/05_SupportVectorMachines.html",
    "href": "notes/05_SupportVectorMachines.html",
    "title": "Support Vector Machines and Convex Optimization",
    "section": "",
    "text": "Nice link: https://www.cs.cmu.edu/~aarti/Class/10701_Spring21/Lecs/svm_dual_kernel_inked.pdf"
  },
  {
    "objectID": "notes/05_SupportVectorMachines.html#classification",
    "href": "notes/05_SupportVectorMachines.html#classification",
    "title": "Support Vector Machines and Convex Optimization",
    "section": "Classification",
    "text": "Classification"
  },
  {
    "objectID": "notes/05_SupportVectorMachines.html#support-vector-machines",
    "href": "notes/05_SupportVectorMachines.html#support-vector-machines",
    "title": "Support Vector Machines and Convex Optimization",
    "section": "Support Vector Machines",
    "text": "Support Vector Machines\n\nHard Margin\n\\[\n\\begin{align}\n\\min_{\\mathbf{w}, b} \\| \\mathbf{w} \\|^2 \\textnormal{ such that }\n1 - y_i(\\mathbf{w}^\\top \\mathbf{x}^{(i)} - b) \\leq 0 \\text{ } \\forall i\n\\end{align}\n\\]\n\n\nSoft Margin\nAdd slack variables \\(\\xi_i\\) to allow for some misclassification:"
  },
  {
    "objectID": "notes/05_SupportVectorMachines.html#convex-optimization",
    "href": "notes/05_SupportVectorMachines.html#convex-optimization",
    "title": "Support Vector Machines and Convex Optimization",
    "section": "Convex Optimization",
    "text": "Convex Optimization\nOptimizing an objective in a constrained problem is challenging, we’ll attempt to convert it into an unconstrained problem by putting the constraint in the objective\nOur first attempt: We could add a term to the objective which is infinity if any of the constraints are satisfied and 0 otherwise. This would certainly yield the same solution: every constraint must be satisfied (so the objective is finite), and we have the minimum objective when the constraint is satisfied. The issue is that solving this problem is difficult because the objective is not continuous: a slight violation of the constraints results in a jump to infinity.\n\nKernel Trick\nConsider polynomial features of power \\(p\\): There are \\(d^p\\) such features, so representing the data in this way is not feasible for large \\(d\\). Instead, we can use the kernel trick to compute the inner product in this space without explicitly computing the features. $$ \\[\\begin{align}\n\\langle \\phi(\\mathbf{x}^{(i)}), \\phi(\\mathbf{x}^{(j)}) \\rangle = K(\\mathbf{x}^{(i)}, \\mathbf{x}^{(j)}) = \\left( \\mathbf{x}^{(i)} \\cdot \\mathbf{x}^{(j)} \\right)^p\n\\end{align}\\]"
  },
  {
    "objectID": "notes/03_GradientDescent.html",
    "href": "notes/03_GradientDescent.html",
    "title": "Gradient Descent and Polynomial Regression",
    "section": "",
    "text": "Today, we continue our discussion of supervised learning, where we have labeled training data and our goal is to train a model that accurately predicts the labels of unseen testing data. Recall that our general approach to supervised learning is to use empirical risk minimization: We focus on a class of models, define a loss function that quantifies how accurately a particular model explains the training data, and search for a model with low loss.\nLast week, we considered the class of linear models, i.e., the prediction is a weighted linear combination of the input features. We chose to measure loss via mean squared error, a choice both rooted in convenience and a compelling modeling assumption (if the data is generated by a linear process with Gaussian, then the mean squared error is the maximum likelihood estimator). In order to find the best parameters of the linear model, we used our knowledge of gradients to exactly compute the parameters that minimize the mean squared error loss.\nThis week, we will address two of the nagging issues with computing the best parameters of a linear model. We begin with the issue of runtime; computing the optimal parameters requires building a large matrix and inverting it, which can be computationally expensive. We will now see how we can use gradient descent to speed up this process.\n\nGradient Descent\nThe mean squared error of a linear model is particularly well-behaved because it is convex i.e., there is a single minimum. Previously, we computed the parameters where the gradient is 0, which, by convexity, immediately gave us the single optimal point. However, we could instead use a more relaxed approach; rather than jumping immediately to the best parameters, we can iterate towards better parameters by taking steps towards lower loss.\n\n\n\nGradient descent is an iterative method for moving in the direction of steepest descent. Concretely, the process produces a sequence of parameters \\(\\mathbf{w}^{(1)}, \\mathbf{w}^{(2)}, \\ldots\\). At each step, we compute the direction of steepest ascent i.e., the gradient of the loss function with respect to each parameter. The gradient quantifies how the loss function responds as we tweak each parameter. If the partial derivative is positive (increasing the parameter increases the loss), then we will want to decrease the parameter. Analogously, if the partial derivative is negative (increasing the parameter decreases the loss), then we will want to increase the parameter. In both cases, we are moving in the direction away from the gradient. Hence, we reach the next parameter vector by subtracting the gradient from the current parameter vector: \\[\n\\begin{align*}\n\\mathbf{w}^{(t+1)} \\gets \\mathbf{w}^{(t)} - \\alpha \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}^{(t)}),\n\\end{align*}\n\\] where \\(\\alpha\\) is a small positive constant called the step size or learning rate.\nNotice that, for one, this approach stops when we reach a point where the gradient is 0, i.e., we have reached a local minimum.\nBeyond the stopping condition, why does this work? Consider the one dimensional setting. The derivative of the loss function is \\[\n\\begin{align*}\n\\mathcal{L}'(w) = \\lim_{\\Delta \\to 0} \\frac{\\mathcal{L}(w + \\Delta) - \\mathcal{L}(w)}{\\Delta}.\n\\end{align*}\n\\] so, for small \\(\\Delta\\), we can approximate the loss function as \\[\n\\begin{align*}\n\\mathcal{L}(w+\\Delta) - \\mathcal{L}(w) &\\approx \\mathcal{L}'(w) \\cdot \\Delta \\\\\n\\end{align*}\n\\] We want \\(\\mathcal{L}(w+\\Delta)\\) to be smaller than \\(\\mathcal{L}(w)\\), so we want \\(\\mathcal{L}'(w) \\Delta &lt; 0\\). This can be achieved by setting \\(\\Delta = -\\eta \\mathcal{L}'(w)\\), where \\(\\eta\\) is a small positive constant. Then \\(w^{(t+1)} = w^{(t)} - \\eta \\mathcal{L}'(w^{(t)})\\) is a step in the direction of descent.\nIn the multi-dimensional setting, the partial derivative of the loss function with respect to each parameter is given by the gradient: \\[\n\\frac{\\partial \\mathcal{L}}{\\partial w_i} = \\lim_{\\Delta \\to 0} \\frac{\\mathcal{L}(\\mathbf{w} + \\Delta \\mathbf{e}_i) - \\mathcal{L}(\\mathbf{w})}{\\Delta},\n\\] where \\(\\mathbf{e}_i\\) is the \\(i\\)-th standard basis vector. Then, for small \\(\\Delta\\), we can approximate the loss function as \\[\n\\mathcal{L}(\\mathbf{w} + \\Delta \\mathbf{e}_i) - \\mathcal{L}(\\mathbf{w}) \\approx \\frac{\\partial \\mathcal{L}}{\\partial w_i} \\cdot \\Delta\n= \\langle \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}), \\Delta \\mathbf{e}_i \\rangle.\n\\] For a general vector \\(\\mathbf{v}\\), we can write \\[\n\\mathcal{L}(\\mathbf{w} + \\Delta \\mathbf{v}) - \\mathcal{L}(\\mathbf{w}) \\approx \\langle \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}), \\Delta \\mathbf{v} \\rangle.\n\\] If we want to move in the direction of steepest descent, we can set \\(\\Delta \\mathbf{v} = -\\eta \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w})\\), where \\(\\eta\\) is a small positive constant. Then, we have \\(\\mathcal{L}(\\mathbf{w} - \\eta \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w})) - \\mathcal{L}(\\mathbf{w}) \\approx -\\eta \\langle \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}), \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}) \\rangle = -\\eta \\|\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w})\\|^2\\). Why is this the right choice? Well, recall for any vectors \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\), we have \\(\\langle \\mathbf{a}, \\mathbf{b} \\rangle = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos(\\theta)\\), where \\(\\theta\\) is the angle between the two vectors. The largest value of \\(\\cos(\\theta)\\) is \\(1\\), which occurs when the two vectors are in the same direction. Notice we achieve the largest magnitude of the inner product when we take the step in the direction of the gradient, i.e., \\(- \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w})\\).\nFor linear models, we already know the gradient of the mean squared error loss: \\[\n\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}) = \\frac2{n} \\mathbf{X}^\\top (\\mathbf{X w - y}).\n\\] In contrast to the \\(O(nd^2 + d^3)\\) time required to compute the exact solution, we can now compute the gradient in \\(O(nd)\\) time, as long as we restrict ourselves to matrix-vector multiplications rather than matrix-matrix multiplications. The final time complexity of gradient descent is \\(O(T nd)\\), where \\(T\\) is the number of iterations of gradient descent.\nWhile we have achieved a significant speedup, \\(O(T nd)\\) could still be prohibitively large when we have a large number of data points \\(n\\) and/or a large number of features \\(d\\). Our solution will be a stochastic approach, where we only use a small random subset of the data to compute the gradient.\n\n\nStochastic Gradient Descent\nOur approach will be similar to gradient descent, except now we will compute the gradient using only the data in the batch. For the mean squared error loss, we can write the loss function for a random subset \\(S\\) of the data as \\[\n\\mathcal{L}_S(\\mathbf{w}) = \\frac1{|S|} \\sum_{i \\in S} (f(\\mathbf{x}^{(i)}) - y^{(i)})^2.\n\\] Then, for our linear model, the gradient of the loss function with respect to the parameters \\(\\mathbf{w}\\) is given by \\[\n\\nabla_\\mathbf{w} \\mathcal{L}_S(\\mathbf{w}) = \\frac2{|S|} \\mathbf{X}_S^\\top (\\mathbf{X}_S \\mathbf{w} - \\mathbf{y}_S),\n\\] where \\(\\mathbf{X}_S\\) is the data matrix for the subset \\(S\\) and \\(\\mathbf{y}_S\\) is the target vector for the subset \\(S\\). One iteration of stochastic gradient descent takes time \\(O(|S|d)\\), which can be much faster than the \\(O(nd)\\) time required to compute the gradient for the full dataset.\n\n\nAdaptive Step Sizes\nThe step size \\(\\alpha\\) is a crucial hyperparameter in gradient descent. If \\(\\alpha\\) is too small, then the algorithm will take a long time to converge because it will take small steps towards the minimum. If \\(\\alpha\\) is too large, then the algorithm may overshoot the minimum and diverge by repeatedly moving in the right direction but by too much. Instead, we want to choose a step size \\(\\alpha\\) that is just right, allowing us to make progress towards the minimum without overshooting.\n\n\n\nThere are several strategies for choosing the step size:\n\nWhen searching manually, we often exponentially increase and decrease the step size i.e., multiply by a factor of \\(2\\) or \\(1/2\\). If the loss consistently improves over several iterations of gradient descent, then we try increasing the step size; if the loss is unstable, then we try decreasing the step size.\nLearning rate schedules offer a more automated approach, where we start with a large step size and then decrease it over time. This is often done by multiplying the step size by a factor less than \\(1\\) after each iteration. The intuition is that we want to take large steps at the beginning to quickly find a good region of the parameter space, and then take smaller steps so as to not overshoot the minima as we get closer.\nAn even more automated approach is to use an adaptive learning rate, where we adjust the step size based on the gradient. If the gradient is large, then we can decrease the step size to avoid overshooting; if the gradient is small, then we can increase the step size to speed up convergence. One implementation of this idea is as follows: \\[\n\\alpha^{(t+1)} \\gets \\frac{\\alpha^{(t)}}{(\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}^{(t)}))^2}.\n\\] Notice that the division is element-wise, so we are adjusting the step size for each parameter individually based on the squared partial derivative of that parameter.\n\nIn addition the step size, the direction of each step is also important.\n\n\nMomentum\nThe idea of gradient descent is to converge to a local minimum of the loss function, but things can go wrong even if we have the right step size: The gradient may not point in the direction of the minima if, for example, the loss function is not symmetric. The plot illustrates this issue for a convex loss function on two parameters \\({w}_1\\) and \\({w}_2\\), where the loss function is given by level sets. In the plot, a standard gradient descent approach takes many steps but the directions cancel out, resulting in a zig-zag pattern that slows convergence.\n\n\n\nOur solution is to keep track of the direction we have been moving in and use that to inform our next step. This idea, called momentum, retains a running average of the gradients, which allows us to smooth out the direction of the steps. We can think of momentum as a ball rolling down a hill, even when the ball is pushed left or right, it will continue to roll downwards. An implementation of momentum is as follows: \\[\n\\begin{align}\n\\mathbf{m}^{(t+1)} &= \\beta \\mathbf{m}^{(t)} + (1 - \\beta) \\nabla_\\mathbf{w} \\mathcal{L}_S(\\mathbf{w}^{(t)}) \\\\\n\\mathbf{w}^{(t+1)} &= \\mathbf{w}^{(t)} - \\alpha \\mathbf{m}^{(t+1)},\n\\end{align}\n\\] where \\(\\beta\\) is a hyperparameter that controls the amount of history we keep in the momentum vector \\(\\mathbf{m}^{(t)}\\)."
  },
  {
    "objectID": "notes/02_LinearRegression.html",
    "href": "notes/02_LinearRegression.html",
    "title": "Linear Regression and Optimization",
    "section": "",
    "text": "Machine learning is incredibly popular. Seen extensive progress in the past several decades, and especially recently with the advent of generative AI.\nThere are many problems that fall under the umbrella of machine learning:\n\nPredicting temperature based on present weather conditions,\nIdentifying whether an image contains a cat or dog, and\nGenerating the next word in a sentence.\n\nThis course is about how we go about solving these problems. The first half of the course will cover supervised learning, where we are given labeled data, and our goal is to train a function to approximately match the labels.\nConcretely, we are given \\(n\\) data points \\(\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(n)} \\in \\mathbb{R}^d\\), each with \\(d\\) dimensions, and associated labels \\(y^{(1)}, y^{(2)}, \\ldots, y^{(n)} \\in \\mathbb{R}\\). Our goal is to learn a function \\(f: \\mathbb{R}^d \\to \\mathbb{R}\\) so that \\(f(\\mathbf{x}^{(i)}) \\approx y^{(i)}\\) for all data points \\(i \\in \\{1,2,\\ldots,n\\}\\).\nOur general approach to solving supervised learning problems will be to use empirical risk minimization, which gives a flexible scaffolding that encompasses many of the topics we’ll discuss in this course. Given a function class (e.g., linear functions or neural networks), the idea is to select the function that most closely explains the data. In particular, there are three components to empirical risk minimization:\n\nFunction Class: The function class \\(\\mathcal{F}\\) from which we will select the function \\(f\\) that most closely fits the observed data.\nLoss: The loss function that measures how well a function \\(f\\) fits the observed data. (Without loss of generality, we will assume that lower is better.)\nOptimizer: The method of selecting the function from the function class.\n\nEmpirical risk minimization is an abstract idea. Luckily, we will revisit it again and again. Our first example will be linear regression, where the function class is the set of linear functions and the loss is the squared difference between the true label and our prediction. Let’s dive in!"
  },
  {
    "objectID": "notes/02_LinearRegression.html#supervised-learning",
    "href": "notes/02_LinearRegression.html#supervised-learning",
    "title": "Linear Regression and Optimization",
    "section": "",
    "text": "Machine learning is incredibly popular. Seen extensive progress in the past several decades, and especially recently with the advent of generative AI.\nThere are many problems that fall under the umbrella of machine learning:\n\nPredicting temperature based on present weather conditions,\nIdentifying whether an image contains a cat or dog, and\nGenerating the next word in a sentence.\n\nThis course is about how we go about solving these problems. The first half of the course will cover supervised learning, where we are given labeled data, and our goal is to train a function to approximately match the labels.\nConcretely, we are given \\(n\\) data points \\(\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(n)} \\in \\mathbb{R}^d\\), each with \\(d\\) dimensions, and associated labels \\(y^{(1)}, y^{(2)}, \\ldots, y^{(n)} \\in \\mathbb{R}\\). Our goal is to learn a function \\(f: \\mathbb{R}^d \\to \\mathbb{R}\\) so that \\(f(\\mathbf{x}^{(i)}) \\approx y^{(i)}\\) for all data points \\(i \\in \\{1,2,\\ldots,n\\}\\).\nOur general approach to solving supervised learning problems will be to use empirical risk minimization, which gives a flexible scaffolding that encompasses many of the topics we’ll discuss in this course. Given a function class (e.g., linear functions or neural networks), the idea is to select the function that most closely explains the data. In particular, there are three components to empirical risk minimization:\n\nFunction Class: The function class \\(\\mathcal{F}\\) from which we will select the function \\(f\\) that most closely fits the observed data.\nLoss: The loss function that measures how well a function \\(f\\) fits the observed data. (Without loss of generality, we will assume that lower is better.)\nOptimizer: The method of selecting the function from the function class.\n\nEmpirical risk minimization is an abstract idea. Luckily, we will revisit it again and again. Our first example will be linear regression, where the function class is the set of linear functions and the loss is the squared difference between the true label and our prediction. Let’s dive in!"
  },
  {
    "objectID": "notes/02_LinearRegression.html#univariate-linear-regression",
    "href": "notes/02_LinearRegression.html#univariate-linear-regression",
    "title": "Linear Regression and Optimization",
    "section": "Univariate Linear Regression",
    "text": "Univariate Linear Regression\nLinear regression is a simple but powerful tool that we will use to understand the basics of machine learning. For simplicity, we will first consider the univariate case where the inputs are all one-dimensional i.e., \\(x^{(1)}, \\ldots, x^{(n)} \\in \\mathbb{R}\\).\n\nLinear functions\nAs its name suggests, linear regression uses a linear function to process the input into an approximation of the output. Let \\(w \\in \\mathbb{R}\\) be a weight parameter. The linear function (for one-dimensional inputs) is given by \\(f(x) = wx\\).\nUnlike many machine learning functions, we can visualize the linear function since it is given by a line. In the plot, we have the \\(n=10\\) data points plotted in two dimensions. There is one linear function \\(f(x) = 2x\\) that closely approximates the data and another linear function \\(f(x)=\\frac12 x\\) that poorly approximates the data.\n\n\n\nOur goal is to learn how to find a linear function that fits the data well. Before we can do this, though, we will need to define what it means for a function to “fit the data well”.\n\n\nMean Squared Error Loss\nOur goal for the loss function is to measure how closely the data fits the prediction made by our function. Intuitively, we should take the difference between the prediction and the true outcome \\(f(x^{(i)})-y^{(i)}\\).\nThe issue with this approach is that \\(f(x^{(i)})-y^{(i)}\\) can be small (negative) even when \\(f(x^{(i)}) \\neq y^{(i)}\\). A natural fix is to take the absolute value \\(|f(x^{(i)}) - y^{(i)}|\\). The benefit of the absolute value is that the loss is \\(0\\) if and only if \\(f(x^{(i)}) = y^{(i)}\\). However, the absolute value function is not differentiable, which is a property we’ll need for optimization. Instead, we use the squared loss:\n\\(\\mathcal{L}(w) = \\frac1{n} \\sum_{i=1}^n (f(x^{(i)}) - y^{(i)})^2.\\)\nHere, we use the mean squared error loss, which is the average squared difference between the prediction and the true output over the dataset. Unlike the absolute value function, the squared function is differentiable everywhere. In addition, the squared error disproportionately penalizes predictions that are far from the true labels, a property that may be desirable when we want all of our predictions to be reasonably accurate.\n\n\n\nThe plot above compares the squared function to the absolute value function. While both are \\(0\\) if and only if their input is \\(0\\), the squared function is differentiable everywhere and penalizes large errors more.\n\n\nExact Optimization\nWe now have our function class and loss function: linear functions and mean squared error loss. The question becomes how to update the weights of the function to minimize the loss. In particular, we want to find \\(w\\) that minimizes \\(\\mathcal{L}(w)\\). While the language we’re using is new, the problem is not. We’ve actually been studying how to do this since pre-calculus!\nThe squared loss is convex (a bowl facing up versus the downward facing cave of concave); see the plot above for a ‘proof’ by example. In this case, we know there is only one minimum. Not only that but we can find the minimum by setting the derivative to \\(0\\).\nAs such, our game plan is to set \\(\\frac{\\partial \\mathcal{L}}{\\partial w}\\) to \\(0\\) and solve for \\(w\\). Recall that \\(f(x) = wx\\). We will use the linearity of the derivative, the chain rule, and the power rule to compute the derivative of \\(\\mathcal{L}\\) with respect to \\(w\\):\n\\[\n\\begin{align}\n\\frac{\\partial}{\\partial w}[\\mathcal{L}(w)]\n&= \\frac1{n} \\sum_{i=1}^n \\frac{\\partial}{\\partial w} [(f(x^{(i)}) - y^{(i)})^2]\n\\notag \\\\&= \\frac1{n} \\sum_{i=1}^n 2(f(x^{(i)}) - y^{(i)}) \\frac{\\partial}{\\partial w} [(f(x^{(i)}) - y^{(i)})]\n\\notag \\\\&= \\frac1{n} \\sum_{i=1}^n 2(w x^{(i)} - y^{(i)}) x^{(i)}.\n\\end{align}\n\\]\nSetting the derivative to \\(0\\) and solving for \\(w\\), we get \\(\\frac2{n} \\sum_{i=1}^n w \\cdot (x^{(i)})^2 = \\frac2{n} \\sum_{i=1}^n y^{(i)} x^{(i)}\\) and so \\[\nw = \\frac{\\sum_{i=1}^n y^{(i)} \\cdot x^{(i)}}{\\sum_{i=1}^n (x^{(i)})^2}.\n\\]\nThis is the exact solution to the univariate linear regression problem! We can now use this formula to find the best linear function for our univariate data. However, we’ll have to work slightly harder for the general case with multidimensional data."
  },
  {
    "objectID": "notes/02_LinearRegression.html#multivariate-linear-regression",
    "href": "notes/02_LinearRegression.html#multivariate-linear-regression",
    "title": "Linear Regression and Optimization",
    "section": "Multivariate Linear Regression",
    "text": "Multivariate Linear Regression\nConsider the more general setting where the input is \\(d\\)-dimensional. As before, we observe \\(n\\) training observations \\((\\mathbf{x}^{(1)}, y^{(1)}), \\ldots, (\\mathbf{x}^{(n)}, y^{(n)})\\) but now \\(\\mathbf{x}^{(i)} \\in \\mathbb{R}^d\\). We will generalize the ideas from univariate linear regression to the multivariate setting.\n\nLinear function\nInstead of using a single weight \\(w \\in \\mathbb{R}\\), we will use \\(d\\) weights \\(\\mathbf{w} \\in \\mathbb{R}^d\\). Then the function is given by \\(f(x) = \\langle \\mathbf{w}, \\mathbf{x} \\rangle\\).\nInstead of using a line to fit the data, we use a hyperplane. While visualizing the function is difficult in high dimensions, we can still see the function when \\(d=2\\).\n\n\n\nIn the plot above, we have \\(n=10\\) data points in 3 dimensions. There is one linear function \\(\\mathbf{w} = \\begin{bmatrix} 2 \\\\ \\frac12 \\end{bmatrix}\\) that closely approximates the data and another linear function \\(\\mathbf{w} = \\begin{bmatrix} \\frac12 \\\\ 0 \\end{bmatrix}\\) that poorly approximates the data.\n\n\nMean Squared Error\nSince the output of \\(f\\) is still a single real number, we do not have to change the loss function. However, we can use our linear algebra notation to write the mean squared error in an elegant way.\nLet \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) be the data matrix where the \\(i\\)th row is \\((\\mathbf{x}^{(i)})^\\top\\). Similarly, let \\(\\mathbf{y} \\in \\mathbb{R}^n\\) be the target vector where the \\(i\\)th entry is \\(y^{(i)}\\). We can then write the mean squared error loss as \\[\n\\mathcal{L}(\\mathbf{w}) = \\frac1{n} \\| \\mathbf{X w - y} \\|_2^2.\n\\]\n\n\nExact Optimization\nJust like computing the derivative and setting it to \\(0\\), we can compute the gradient and set it to the zero vector \\(\\mathbf{0} \\in \\mathbb{R}^d\\). In mathematical notation, we will set \\(\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}^*) = \\mathbf{0}\\) and solve for \\(\\mathbf{w}^*\\). The intuition is that such a point is a local minimum in every direction; that is, we cannot improve the loss by moving in any of the dimensions. Since the loss is convex (i.e., there can be only one minima), such a point is the unique global minimum and achieves the optimal loss.\nAs you may recall from multivariate calculus, the gradient \\(\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w})\\) is simply a vector with the same dimension as \\(\\mathbf{w}\\); the value in the \\(i\\)th dimension is \\(\\frac{\\partial \\mathcal{L}(\\mathbf{w})}{\\partial w_i}\\).\nLet’s compute this quantity \\[\n\\begin{align}\n\\frac{\\partial \\mathcal{L}(\\mathbf{w})}{\\partial w_i}\n&= \\lim_{\\Delta \\to 0} \\frac{\\mathcal{L}(\\mathbf{w}+\\Delta \\mathbf{e}_i) - \\mathcal{L}(\\mathbf{w})}{\\Delta}\n\\\\&=\\lim_{\\Delta \\to 0} \\frac{\\| \\mathbf{X} (\\mathbf{w}+\\Delta \\mathbf{e}_i) - \\mathbf{y}\\|^2 - \\| \\mathbf{X} \\mathbf{w} - \\mathbf{y}\\|^2 }{\\Delta}.\n\\end{align}\n\\] Let \\(\\mathbf{a}, \\mathbf{b}\\) be two vectors in the same dimensional space. We have \\(\\|\\mathbf{a} + \\mathbf{b} \\|^2 = \\langle \\mathbf{a} + \\mathbf{b} , \\mathbf{a} + \\mathbf{b} \\rangle = \\| \\mathbf{a} \\|^2 + 2\\langle \\mathbf{a}, \\mathbf{b} \\rangle + \\| \\mathbf{b}\\|^2\\), where we foiled to reach the final equality, and used that \\(\\langle \\mathbf{a}, \\mathbf{b} \\rangle = \\langle \\mathbf{a}, \\mathbf{b} \\rangle\\). By letting \\(\\mathbf{a} = \\mathbf{X w - b}\\) and \\(\\mathbf{b} = \\Delta \\mathbf{X} \\mathbf{e}_i\\), we reach \\[\n\\begin{align}\n\\frac{\\partial \\mathcal{L}(\\mathbf{w})}{\\partial w_i}\n&=\\lim_{\\Delta \\to 0} \\frac{\\| \\mathbf{X} \\mathbf{w} - \\mathbf{y}\\|^2\n+ 2 \\langle \\mathbf{X} \\mathbf{w} - \\mathbf{y}, \\Delta \\mathbf{X e}_i \\rangle\n+\\|\\Delta \\mathbf{X e}_i\\|^2\n- \\| \\mathbf{X} \\mathbf{w} - \\mathbf{y}\\|^2 }{\\Delta}.\n\\\\&= \\lim_{\\Delta \\to 0} \\frac{2 \\Delta \\langle \\mathbf{X} \\mathbf{w} - \\mathbf{y}, \\mathbf{X e}_i \\rangle\n+\\Delta^2 \\|\\mathbf{X e}_i\\|^2}{\\Delta}\n\\\\&= \\lim_{\\Delta \\to 0} 2 \\langle \\mathbf{X} \\mathbf{w} - \\mathbf{y}, \\mathbf{X e}_i \\rangle + \\Delta \\|\\mathbf{X e}_i\\|^2\n\\\\&=\\langle \\mathbf{X} \\mathbf{w} - \\mathbf{y}, \\mathbf{X e}_i \\rangle.\n\\end{align}\n\\] Let \\(\\mathbf{X}_i = \\mathbf{Xe}_i\\) be the \\(i\\)th row of \\(\\mathbf{X}\\). Then, the full gradient is given by \\[\n\\begin{align}\n\\nabla_\\mathbf{w}\\mathcal{L}(\\mathbf{w})\n= 2 \\begin{bmatrix}\n\\mathbf{X}_1^\\top (\\mathbf{X} \\mathbf{w} - \\mathbf{y}) \\\\ \\mathbf{X}_2^\\top (\\mathbf{X} \\mathbf{w} - \\mathbf{y}) \\\\ \\vdots\n\\end{bmatrix}\n\\end{align}\n= 2 \\mathbf{X}^\\top (\\mathbf{X} \\mathbf{w} - \\mathbf{y}).\n\\]\nBy the convexity of the loss function \\(\\mathcal{L}\\), we know that \\(\\nabla_\\mathbf{w}\\mathcal{L}(\\mathbf{w}^*)=0\\) at the optimal weights \\(\\mathbf{w}^*\\). Solving for \\(\\mathbf{w}^*\\) yields \\[\n\\begin{align}\n\\nabla_\\mathbf{w}\\mathcal{L}(\\mathbf{w}^*)\n&= 2 \\mathbf{X}^\\top (\\mathbf{X} \\mathbf{w}^* - \\mathbf{y})\n= 0\n\\\\ \\Leftrightarrow\n\\mathbf{X}^\\top \\mathbf{X} \\mathbf{w}^* &= \\mathbf{X}^\\top \\mathbf{y}\n\\\\ \\Leftrightarrow\n\\mathbf{w}^* &= (\\mathbf{X}^\\top \\mathbf{X})^+ \\mathbf{X}^\\top \\mathbf{y}\n\\end{align}\n\\] where \\((\\cdot)^+\\) is the pseudoinverse i.e., \\((\\mathbf{M})^+ \\mathbf{M} = \\mathbf{I}\\) for all symmetric matrices \\(\\mathbf{M}\\).\nQuestion: Is the pseudoinverse also defined for non-symmetric square matrices?"
  },
  {
    "objectID": "notes/02_LinearRegression.html#empirical-risk-minimization",
    "href": "notes/02_LinearRegression.html#empirical-risk-minimization",
    "title": "Linear Regression and Optimization",
    "section": "Empirical Risk Minimization",
    "text": "Empirical Risk Minimization\nWe have now seen how to fit a linear function to data using the mean squared error loss. However, we have not given a satisfying answer to the question:\n\nWhy use mean squared error as our loss function?\n\n\nSo far, our answer has been that the quadratic function is differentiable (which we use to find the optimal solution), and that it naturally penalizes predictions which are farther away more. The first point is one of convenience and, a priori, should not be particularly persuasive. The second seems somewhat arbitrary, why penalize at a quadratic rate rather than an e.g., quartic rate? We’ll now consider a more compelling answer.\nOn our way to the answer, let’s take a step back and consider another question:\n\nWhy fit the data with a linear function?\n\n\nWell, we may do so when we expect the data truly has a linear relationship with the labels. To make things interesting, we will assume that there is random noise added to the labels, but that this noise is mean-centered so that, on average, the labels come from the linear model. Concretely, we observe some point \\(\\mathbf{x}\\) with a label that comes from a linear model \\(\\mathbf{w}^*\\) but with added noise, i.e., \\[\ny= \\langle \\mathbf{w}^*, \\mathbf{x} \\rangle + \\eta.\n\\] We will model this noise as distributed from a normal distribution, i.e., \\(\\eta \\sim \\mathcal{N}(0, \\sigma^2)\\) for some unknown standard deviation \\(\\sigma\\). (To justify this choice, we imagine the noise as a sum of random variables from some other distribution(s) which, by the law of large numbers, will follow the normal distribution when the sum contains sufficiently many terms.)\nRecall that the goal of our empirical risk minimization strategy is to find the function which most closely aligns with the data, or, put differently, we want the function that most likely generated the data we observed. In order to compute this likelihood, we will use the probability density function of the normal distribution: The probability we observe a random variable \\(y\\) drawn from a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) is given by \\[\n\\frac1{\\sqrt{2\\pi \\sigma}} \\exp\\left( - \\frac{(y- \\mu)^2}{2\\sigma^2} \\right).\n\\] If the noisy linear model \\(\\mathbf{w}\\) did generate the training data \\((\\mathbf{x}^{(i)}, y^{(i)})\\), then the expectation of the generation would be \\(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle\\). Then, combined with the assumption that the training data was drawn independently, the probability of observing the training data is given by the product of the probabilities of each individual observation: \\[\n\\prod_{i=1}^n\n\\frac1{\\sqrt{2\\pi \\sigma}} \\exp\\left( - \\frac{(y^{(i)}- \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)^2}{2\\sigma^2} \\right).\n\\] Our goal is to find the function \\(\\mathbf{w}\\) that maximizes this likelihood i.e., \\[\n\\begin{align}\n&{\\arg\\max}_{\\mathbf{w} \\in \\mathbb{R}^d}\n\\prod_{i=1}^n\n\\frac{1}{\\sqrt{2\\pi \\sigma}} \\exp\\left( - \\frac{(y^{(i)}- \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)^2}{2\\sigma^2} \\right)\n\\notag \\\\\n&= {\\arg\\min}_{\\mathbf{w} \\in \\mathbb{R}^d}\n- \\log \\left(\n\\frac{1}{\\sqrt{2\\pi \\sigma}} \\exp\\left(\\sum_{i=1}^n - \\frac{(y^{(i)}- \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)^2}{2\\sigma^2} \\right)\\right)\n\\notag \\\\\n&= {\\arg\\min}_{\\mathbf{w} \\in \\mathbb{R}^d}\n- \\sum_{i=1}^n - (y^{(i)}- \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)^2.\n\\end{align}\n\\] Here, we used the following facts: maximizing an objective is equivalent to minimizing the negative of that objective, the logarithmic function is monotonically increasing so minimizing the likelihood is equivalent to minimizing the log-likelihood, the product of exponentials is the exponential of the sum, and removing a constant scalar factor or additive constant does not change the minimum.\nThe punchline is that the function \\(\\mathbf{w}\\) that maximizes the likelihood of observing the training data is the same function that minimizes the mean squared error loss. This is a powerful result that justifies our use of the mean squared error loss."
  },
  {
    "objectID": "notes/02_LinearRegression.html#looking-forward",
    "href": "notes/02_LinearRegression.html#looking-forward",
    "title": "Linear Regression and Optimization",
    "section": "Looking Forward",
    "text": "Looking Forward\nWhile we have seen the benefits of exactly optimizing linear regression functions, there are several limitations that we will address.\nComputational Complexity We saw (and you will prove) that the exact solution to linear regression is given by \\(\\mathbf{w}^* = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\\). This requires building the matrix \\(\\mathbf{X}^\\top \\mathbf{X}\\), which takes \\(O(nd^2)\\) time, and then inverting it, which takes \\(O(d^3)\\). When we have a large number of data points \\(n\\) and/or a large number of features \\(d\\), this can be prohibitively expensive.\nFunction Class Misspecification We have assumed that the data has a linear relationship (or close to linear relationship) with the labels. What happens when this is not true? That is, even the best linear function gives a poor approximation?"
  },
  {
    "objectID": "notes/04_LogisticRegression.html",
    "href": "notes/04_LogisticRegression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "With a little probability, we saw how the Naive Bayes Classifier can be used to make predictions. However, the Naive Bayes Classifier assumes that the value of each feature is independent of the others, which is often not the case in practice (e.g., if the word “king” appears in an email, it is more likely that the word “queen” also appears). As an alternative approach to classification problems, we will see how we can generalize linear regression, with a little non-linearity.\nOur setup will be the standard supervised learning setting, where we have labelled data \\((\\mathbf{x}^{(1)}, y^{(1)}), \\ldots, (\\mathbf{x}^{(n)}, y^{(n)})\\), for \\(\\mathbf{x}^{(i)} \\in \\mathbb{R}^d\\) the feature vector for the \\(i\\)th example. However, unlike regression where we predict a continuous value \\(y^{(i)} \\in \\mathbb{R}\\), we will now predict a binary value \\(y^{(i)} \\in \\{0, 1\\}\\). We can use the same linear model as before, i.e., we will predict the output as \\(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle\\), where \\(\\mathbf{w} \\in \\mathbb{R}^d\\) is the weight vector. But, we run into an issue: the output of the linear model can take on any real value, but we want to predict a binary value.\n\nModel and Loss\nWe’ll explore several attempts to convert our linear model into a binary classifier.\nAttempt #1: We could simply apply a step function to the output of the linear model, i.e., predict \\(y^{(i)} = 1\\) if \\(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle &gt; 0\\) and \\(y^{(i)} = 0\\) otherwise. The loss could be the difference between the predicted value and the true value, i.e., \\(\\mathcal{L}(\\mathbf{w}) = \\sum_{i=1}^n |y^{(i)} - \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle|\\). However, this loss is not differentiable, so we cannot use gradient descent to optimize it.\nAttempt #2: We could use the mean squared error loss, i.e., \\(\\mathcal{L}(\\mathbf{w}) = \\sum_{i=1}^n (y^{(i)} - \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)^2\\). This loss is differentiable, but it does not work well for classification problems: if we have a large positive value for \\(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle\\), the loss will be large even if \\(y^{(i)} = 1\\).\nAttempt #3: We can apply the sigmoid function to the output of the linear model to map it to the range \\((0, 1)\\), i.e., we will predict \\(f(\\mathbf{x}^{(i)}) = \\sigma(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)\\), where \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. The sigmoid function is a smooth, non-linear function that maps any real number to the range \\((0, 1)\\). We can then interpret \\(f(\\mathbf{x}^{(i)})\\) as the probability that \\(y^{(i)} = 1\\) given the features \\(\\mathbf{x}^{(i)}\\). If we need to report a class label, we can threshold the predicted probability, e.g., predict \\(y^{(i)} = 1\\) if \\(f(\\mathbf{x}^{(i)}) &gt; \\frac12\\) and \\(y^{(i)} = 0\\) otherwise.\n\n\n\nTo train our model, we need a loss function that measures how well our predicted probabilities match the true labels. A common choice is the binary cross-entropy loss, which is defined as \\[\\mathcal{L}(\\mathbf{w}) = - \\sum_{i=1}^n \\left[y^{(i)} \\log(f(\\mathbf{x}^{(i)})) + (1 - y^{(i)}) \\log(1 - f(\\mathbf{x}^{(i)}))\\right].\\] This loss function measures the distance (in a sense we’ll explore later in the course) between the predicted probabilities and the true labels. It is differentiable, so we can use gradient descent to optimize it.\n\n\n\n\n\nOptimization\nOnce we have a model and loss function, we have seen two ways to optimize the model: Exact optimization, where we compute the gradient of the loss function with respect to the model parameters and set the gradient to zero to find the optimal parameters. Gradient descent, where we iteratively update the model parameters in the direction of the negative gradient of the loss function. Both approaches require the gradient of the loss function with respect to the model parameters, so let’s compute the gradient of \\(\\mathcal{L}(\\mathbf{w})\\) with respect to \\(\\mathbf{w}\\).\nPlugging in the sigmoid function, we have \\[\n\\begin{align*}\n\\mathcal{L}(\\mathbf{w}) &= - \\sum_{i=1}^n \\left[y^{(i)} \\log\\left(\\frac{1}{1 + e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}}\\right) + (1 - y^{(i)}) \\log\\left(1 - \\frac{1}{1 + e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}}\\right)\\right] \\\\\n\\end{align*}\n\\]\nObserve that \\(\\frac1{1 + e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}} = \\frac{e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}}{e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} + 1}\\), so \\(1-\\frac1{1 + e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}} = \\frac{1}{e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} + 1}\\).\nThen, we can rewrite the loss as \\[\n\\begin{align*}\n\\mathcal{L}(\\mathbf{w}) &= \\sum_{i=1}^n \\left[y^{(i)} \\log\\left(1+e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} \\right) + (1 - y^{(i)}) \\log\\left(e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} + 1\\right) \\right].\n\\end{align*}\n\\]\nLet’s compute the partial derivative of the loss with respect to \\(w_j\\). Applying the chain rule, we have \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial w_j} \\mathcal{L}(\\mathbf{w}) &= \\sum_{i=1}^n\n\\left[y^{(i)} \\frac1{1 + e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}} \\cdot e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} (-x_j^{(i)}) + (1 - y^{(i)}) \\frac1{e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} + 1} \\cdot e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} x_j^{(i)}\\right]\\\\\n\\end{align*}\n\\]\nObserve that \\(\\frac{e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}}{1 + e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}} = \\frac1{e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} + 1} = 1-\\sigma(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)\\), and \\(\\frac{e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}}{e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} + 1} = \\sigma(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)\\). Then, we can rewrite the partial derivative as \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial w_j} \\mathcal{L}(\\mathbf{w}) &= \\sum_{i=1}^n\n\\left[-y^{(i)} (1 - \\sigma(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)) x_j^{(i)} + (1 - y^{(i)}) \\sigma(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle) x_j^{(i)}\\right]\\\\\n&= \\sum_{i=1}^n\nx_j^{(i)} \\left[\\sigma(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle) - y^{(i)}\\right]\\\\\n&= \\mathbf{X}_j^\\top \\left(\\sigma(\\mathbf{X} \\mathbf{w}) - \\mathbf{y}\\right),\n\\end{align*}\n\\] where \\(\\sigma(\\cdot)\\) is applied element-wise to the vector \\(\\mathbf{X} \\mathbf{w}\\), and \\(\\mathbf{X}_j\\) is the \\(j\\)th column of the design matrix \\(\\mathbf{X}\\). Finally, we can write the gradient of the loss with respect to the weight vector \\(\\mathbf{w}\\) as \\[\n\\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}) =  \\mathbf{X}^\\top \\left(\\sigma(\\mathbf{X} \\mathbf{w}) - \\mathbf{y}\\right).\n\\]\nOur exact optimization approach would be to set the gradient to zero and solve for \\(\\mathbf{w}\\). Do you see why this doesn’t work with the non-linear sigmoid function?\nInstead of exact optimization, we will use gradient descent!\n\n\nNon-linear Transformations\nOften, our data is not linearly separable, i.e., we cannot draw a straight line to separate the two classes. In this case, we can use non-linear transformations to map the data to a higher-dimensional space, where it is linearly separable. One approach: As we saw for linear regression, we can add polynomial features to the data. In the image below, we add a new feature \\(x_1^2 + x_2^2\\) to the data, which allows us to separate the two classes with a linear decision boundary in the transformed feature space.\n\n\n\nIt is not a priori clear which non-linear transformation will work best for a given dataset. In several lectures, we will explore how to use kernel methods to implicitly map the data to a higher-dimensional space, which captures many of the non-linear transformations we might want to use.\n\n\nMeasuring Error in Binary Classification\nThe simplest way to measure the error of a classification model is to compute the error rate, which is the fraction of examples that are misclassified. For example, if we have \\(n\\) examples and our model misclassifies \\(k\\) of them, the error rate is \\(\\frac{k}{n}\\).\nHowever, the error rate does not take into account which points are misclassified. We will often break down the accuracy of a classification model into four categories:\n• True Positives (TP): The model correctly predicts a positive class.\n• True Negatives (TN): The model correctly predicts a negative class.\n• False Positives (FP): The model incorrectly predicts a positive class when the true class is negative.\n• False Negatives (FN): The model incorrectly predicts a negative class when the true class is positive.\n\n\n\nThe raw counts of these four categories can be summarized in a confusion matrix. The confusion matrix is a square matrix with dimensions equal to the number of classes, where the rows represent the true classes and the columns represent the predicted classes.\nBut, these raw counts themselves are not very informative.\nWe often report the True Positive Rate (TPR), also known as recall, which is the fraction of true positives out of all actual positives: \\(\\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}.\\) The TPR measures how well the model identifies positive examples (higher is better).\nWe also report the False Positive Rate (FPR), which is the fraction of false positives out of all actual negatives: \\(\\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}.\\) The FPR measures how often the model incorrectly identifies negative examples as positive (lower is better).\nFinally, we can report the Precision, which is the fraction of true positives out of all predicted positives: \\(\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}.\\) Precision measures how well the model identifies positive examples among all predicted positives (higher is better).\nIf we have a model that does not achieve the desired TPR or FPR, we have a hidden lever we can pull: the threshold for predicting a positive class. By default, we predict a positive class if the predicted probability is greater than \\(\\frac12\\). But, we can change this threshold to an arbitrary value \\(\\tau \\in [0, 1]\\). Increasing the threshold can only decrease the TPR, since we are less likely to predict a positive class; simultaneously, the FPR can only increase, since we are more likely to predict a negative class.\n[image here] 2D plots with linearly separable data, with different thresholds \\(\\tau\\).\n\n\n\nWe can visualize the trade-off between TPR and FPR by plotting the Receiver Operating Characteristic (ROC) curve. The ROC curve is a plot of the TPR against the FPR for different threshold values \\(\\tau\\). Because a higher TPR is better and a lower FPR is better, we want the ROC curve to be as close to the top-left corner as possible. The area under the ROC curve (AUC) is a single number that summarizes the performance of the model across all threshold values. A model with an AUC of 1 is perfect, while a model with an AUC of 0.5 is no better than random guessing.\n\n\n\n\n\nMultiple Classes\nIn many settings, we are interested in classifying data into more than two classes. For example, we might want to classify images into different categories, such as cats, dogs, and birds. In this case, we need to extend our binary classification model to handle multiple classes. Our supervised learning setup remains the same, where we have labelled data \\((\\mathbf{x}^{(1)}, y^{(1)}), \\ldots, (\\mathbf{x}^{(n)}, y^{(n)})\\), but now \\(y^{(i)} \\in \\{1, 2, \\ldots, k\\}\\), for \\(k\\) the number of classes.\nWe can still use the same ideas that we used for binary logistic regression, but we need to extend the output of the model to predict a probability distribution over each of the \\(k\\) classes. Instead of a single output, which we can interpret as the probability of the positive class, we will have a vector of outputs \\(\\mathbf{f}(\\mathbf{x}^{(i)}) \\in \\mathbb{R}^k\\), where \\(k\\) is the number of classes.\nTo ensure this vector is a valid probability distribution, we can use the softmax function, defined as \\[\n\\begin{align*}\n\\text{softmax}(\\mathbf{z}) &= \\begin{bmatrix}\n\\frac{e^{z_1}}{\\sum_{j=1}^k e^{z_j}} \\\\\n\\frac{e^{z_2}}{\\sum_{j=1}^k e^{z_j}} \\\\\n\\vdots \\\\\n\\frac{e^{z_k}}{\\sum_{j=1}^k e^{z_j}} \\\\\n\\end{bmatrix}\n\\end{align*}\n\\] Softmax applies the exponential function to each element of the vector, and then normalizes the resulting vector so that the sum of the resulting vector is 1. This ensures that the output is a valid probability distribution, where each probability is between 0 and 1 and the sum of all elements is 1.\nThe loss function for multi-class classification is the cross-entropy loss, which is a generalization of the binary cross-entropy loss. \\[\n\\begin{align*}\n\\mathcal{L}(\\mathbf{w}) &= - \\sum_{i=1}^n \\sum_{j=1}^k \\mathbb{1}[y^{(i)}=j] \\log\\left(f_j(\\mathbf{x}^{(i)})\\right),\n\\end{align*}\n\\] where \\(f_j(\\mathbf{x}^{(i)})\\) is the \\(j\\)th element of the softmax output vector \\(\\mathbf{f}(\\mathbf{x}^{(i)})\\), and \\(\\mathbb{1}[y^{(i)}=j]\\) is an indicator function that is 1 if \\(y^{(i)} = j\\) and 0 otherwise."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH 166: Data Mining",
    "section": "",
    "text": "A course on the mathematical foundations of machine learning.\n\n\n\n\nInstructor: R. Teal Witter. Please call me Teal.\nClass Times: We meet Tuesdays and Thursdays; Sec. 1 is scheduled from 2:45 to 4:00pm, and Sec. 2 from 4:15 to 5:30pm.\nOffice Hours: Before I schedule office hours, please fill out this when2meet so we can find times that work for all of us.\nParticipation: I expect you to engage in class, ask questions, and make connections. To receive credit, please fill out this form after every lecture.\nQuizzes: There will be short quizzes at the beginning of our Tuesday classes. These quizzes will test your understanding of the problem sets and the concepts from the prior week.\n\n\nProblem Sets: Your primary opportunity to learn the material will be on problem sets. You may work with others to solve the problems, but you must write your solutions by yourself, and explicitly acknowledge any outside help (websites, people, LLMs).\nExams: The two midterm exams are designed to give you a multiple ways of demonstrating your understanding. The first is a written midterm focused on supervised learning. The second is a verbal exam, à la a technical interview.\nProject: The project offers a chance to explore an area that interests you, practice writing high quality code, and develop your ability to communicate technical ideas to an audience. In addition to your codebase, you will write a report and give a short presentation at the end of the semester.\n\n\n\n\n\nWeek\n\n\nTuesday\n\n\nThursday\n\n\nSlides\n\n\nAssignments\n\n\n\n\nWarmup\n\n\n\n\nWeek 1 (8/27 and 8/29)\n\n\nLinear Algebra\n\n\nPageRank\n\n\n\n\n\n\n\n\nSupervised Learning\n\n\n\n\nWeek 2 (9/2 and 9/4)\n\n\nLinear Regression\n\n\nOptimization\n\n\n\n\n\n\n\n\nWeek 3 (9/9 and 9/11)\n\n\nGradient Descent\n\n\nPolynomial Regression\n\n\n\n\n\n\n\n\nWeek 4 (9/16 and 9/18)\n\n\nProbability\n\n\nLogistic Regression\n\n\n\n\n\n\n\n\nWeek 5 (9/23 and 9/25)\n\n\nSupport Vector Machines\n\n\nConstrained Optimization\n\n\n\n\n\n\n\n\nWeek 6 (9/30 and 10/2)\n\n\nKernel Methods\n\n\nNeural Networks\n\n\n\n\n\n\n\n\nWeek 7 (10/7 and 10/9)\n\n\nDecision Trees\n\n\nGradient Boosting\n\n\n\n\n\n\n\n\nWeek 8 (10/14 and 10/16)\n\n\nFall Break (No Class)\n\n\nAutoencoders\n\n\n\n\n\n\n\n\nBeyond Supervised Learning\n\n\n\n\nWeek 9 (10/21 and 10/23)\n\n\nMidterm Exam\n\n\nVariational Autoencoders\n\n\n\n\n\n\n\n\nWeek 10 (10/28 and 10/30)\n\n\nPrincipal Component Analysis\n\n\nSemantic Embeddings\n\n\n\n\n\n\n\n\nWeek 11 (11/4 and 11/6)\n\n\nReinforcement Learning\n\n\nReinforcement Learning\n\n\n\n\n\n\n\n\nWeek 12 (11/11 and 11/13)\n\n\nPAC Learning\n\n\nPAC Learning\n\n\n\n\n\n\n\n\nWeek 13 (11/18 and 11/20)\n\n\nActive Learning\n\n\nInterpretability\n\n\n\n\n\n\n\n\nWeek 14 (11/25 and 11/27)\n\n\nFinal Exam\n\n\nThanksgiving (No Class)\n\n\n\n\n\n\n\n\nWeek 15 (12/2 and 12/4)\n\n\nProject Preparation\n\n\nProject Preparation (No Class)\n\n\n\n\n\n\n\n\nWeek 16 (12/9 and 12/11)\n\n\nSec. 2 Presents 7–10pm\n\n\nSec. 1 Presents 2–5pm"
  },
  {
    "objectID": "notes/code.html",
    "href": "notes/code.html",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport scienceplots\n\nplt.style.use('science')\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D"
  },
  {
    "objectID": "notes/code.html#linear-regression-figures",
    "href": "notes/code.html#linear-regression-figures",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "Linear Regression Figures",
    "text": "Linear Regression Figures\n\nnp.random.seed(0) # Seed randomness\nplt.figure(figsize=(6, 3))\n\nn = 10 # Number of observations\nw = 2 # True parameter\nX = np.random.rand(n) # x-values\ny = X.dot(w).T + np.random.normal(size=n) * .2 #y-values\n\nplt.scatter(X,y, color='black', label=r'Data: $(x^{(i)}, y^{(i)})$')\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\nxaxis = np.arange(0,1,.01)\nplt.plot(xaxis, xaxis*.5, label=r'Line: $f(x) = .5x$', color='red')\nplt.plot(xaxis, xaxis*w, label=r'Line: $f(x) = 2x$', color='green')\nplt.legend()\nplt.title(r'Linear Regression in $\\mathbb{R}^1$')\nplt.savefig('images/regression_1d.pdf')\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(6, 3))\nplt.xlabel(r'$z$')\nplt.ylabel(r'$\\mathcal{L}(z)$')\nxaxis = np.arange(-1.5,1.5,.001)\nplt.plot(xaxis, xaxis**2, label=r'Squared Loss: $\\mathcal{L}(z)=z^2$', color='blue')\nplt.plot(xaxis, np.abs(xaxis), label=r'Absolute Loss: $\\mathcal{L}(z)=|z|$', color='purple', linestyle='dotted')\nplt.legend()\nplt.title(r'Squared and Absolute Losses')\nplt.savefig('images/regression_losses.pdf')\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\n\n# Seed randomness\nnp.random.seed(1234)\nn = 10  # Number of observations\nw = np.array([2, .5])  # True parameter\nX = np.random.rand(n, 2)  # x-values\ny = X.dot(w).T + np.random.normal(size=n) * .1  # y-values\n\n# Create figure and 3D axis\nfig = plt.figure(figsize=(5, 5))\nax = fig.add_subplot(111, projection='3d')\n\n# Scatter plot for data points\nax.scatter(X[:, 0], X[:, 1], y, color='black', label=r'Data: $(x_1^{(i)}, x_2^{(i)}, y^{(i)})$')\n\n# Hyperplane 1: Green\nx1 = np.arange(0, 1, .01)\nx2 = np.arange(0, 1, .01)\nX1, X2 = np.meshgrid(x1, x2)\nZ = w[0] * X1 + w[1] * X2\nax.plot_surface(X1, X2, Z, alpha=.5, color='green')\n\n# Hyperplane 2: Red\nax.plot_surface(X1, X2, .5 * X1 + 0 * X2, alpha=.5, color='red')\n\n# Labels and title\nax.set_xlabel(r'$x_1$')\nax.set_ylabel(r'$x_2$')\nax.set_zlabel(r'$y$')\nax.set_title(r'Linear Regression in $\\mathbb{R}^2$')\nax.grid(False)\n\n# Manually create custom legend handles for the surfaces\nhandles = [\n    Line2D([0], [0], marker='o', color='black', markerfacecolor='black', markersize=6, label=r'Data: $(x_1^{(i)}, x_2^{(i)}, y^{(i)})$'),\n    Line2D([0], [0], color='green', lw=4, label=r'Hyperplane: $f(x) = 2x_1 + .5x_2$'),\n    Line2D([0], [0], color='red', lw=4, label=r'Hyperplane: $f(x) = .5x_1 + 0x_2$')\n]\n\n# Add legend\nplt.legend(handles=handles, loc='upper left', framealpha=1)\n\n# Save the figure\nplt.savefig('images/regression_2d.pdf', bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "notes/code.html#non-linear-regression-figures",
    "href": "notes/code.html#non-linear-regression-figures",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "Non-linear Regression Figures",
    "text": "Non-linear Regression Figures\n\n## Gradient descent\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom matplotlib import cm\nimport torch\n\n%matplotlib inline\n\nclass QuadFunc:\n  def __init__(self, a, b, c, d, e):\n    self.a = a\n    self.b = b\n    self.c = c\n    self.d = d\n    self.e = e\n\n  def getParams(self, x, y):\n    if y is None:\n      y = x[1]\n      x = x[0]\n    return x,y\n\n  def __call__(self, x, y=None):\n    x,y = self.getParams(x,y)\n    return 0.5 * (self.a*x**2 + self.b*y**2) + self.c * x * y + self.d * x + self.e * y\n\n  def grad(self, x, y=None):\n    #df/dx = ax + cy + d\n    #df/dy = by + cx + e\n    x,y = self.getParams(x,y)\n    return torch.tensor([self.a * x + self.c * y + self.d, self.b * y + self.c * x + self.e])\n\n  def hess(self, x, y=None):\n    #d2f/dx2 = a\n    #d2f/dy2 = b\n    #d2f/dxdy = c\n    #d2f/dydx = c\n    x, y = self.getParams(x,y)\n    return torch.tensor([[self.a, self.c], [self.c, self.b]])\n\nclass GradientDescent:\n    def __init__(self, lr=1, b1=0.9, b2=0.999):\n        # b1 -&gt; Momentum\n        # b2 -&gt; ADAM\n        # ADAM Paper -&gt; https://arxiv.org/abs/1412.6980\n        self.lr = lr # learning rate\n        self.b1 = b1 # grad aggregation param (for Momentum)\n        self.b2 = b2 # grad^2 aggregation param (for ADAM)\n\n        self.v = 0 # grad aggregation param\n        self.w = 0 # grad^2 aggregation param\n        self.t = 0\n\n        self.eps = 1e-9\n\n    def __call__(self, grad,hess):\n\n        self.t += 1\n\n\n        # aggregation\n        self.v = self.b1*self.v + (1-self.b1)*grad\n        self.w = self.b2*self.w + (1-self.b2)*grad**2\n\n        # bias correction\n        vcorr = self.v/(1-self.b1**self.t)\n        wcorr = self.w/(1-self.b2**self.t) if self.b2 != 0 else 1\n\n        return -1*self.lr*vcorr/(wcorr**0.5 + self.eps)\n\nclass Newtons:\n    # https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization\n    def __init__(self, lr=1):\n        self.lr = lr\n\n    def __call__(self,grad,hess):\n        return -1*self.lr*torch.matmul(torch.inverse(hess), grad)\n\ndef runOptim(init,optim,func,steps):\n\n    curpos = init # current position\n    path = [curpos]\n\n\n    for _ in range(steps):\n\n        grad = func.grad(curpos)\n        hess = func.hess(curpos)\n\n        dx = optim(grad,hess)\n        curpos = curpos + dx\n        path.append(curpos)\n\n    return path\n\n\ndef showPath(func,init,paths,labels,colors,levels):\n\n    x = torch.arange(-10,10,0.05)\n    y = torch.arange(-10,10,0.05)\n\n    # create meshgrid\n    xx, yy = torch.meshgrid(x,y)\n    zz = func(xx,yy)\n\n    # create contour\n    fig, ax = plt.subplots(1,1,figsize=(10,4))\n    cp = ax.contourf(xx,yy,zz,levels)\n    fig.colorbar(cp)\n\n    # mark initial point\n    ax.plot(init[0],init[1],'ro')\n    ax.text(init[0]+0.5,init[1],'Intial Point',color='white')\n\n    # Plot paths\n    for pnum in range(len(paths)):\n        for i in range(len(paths[pnum])-1):\n            curpos = paths[pnum][i]\n            d = paths[pnum][i+1] - curpos\n            ax.arrow(curpos[0],curpos[1],d[0],d[1],color=colors[pnum],head_width=0.2)\n            ax.text(curpos[0]+d[0],curpos[1]+d[1],str(i),color='white')\n\n    # Add legend\n    legends = []\n    for col in colors:\n        legends.append(mpatches.Patch(color=col))\n    # Put legend in top left corner\n    ax.legend(legends,labels, loc='upper left')\n\n\nplt.figure(figsize=(4, 2))\na = 1/torch.sqrt(torch.tensor(2.0))\ninit = torch.matmul(torch.tensor([[a,a],[-a,a]]),torch.tensor([-5.0,7.5]))\nell = QuadFunc(a,a,-0.8*a,a,a)\nsteps = 7\nlr = 1.5\nregGD = GradientDescent(lr,0,0) # Without Momentum\nmomGD = GradientDescent(lr,0.9,0) # Momentum\npath1 = runOptim(init,regGD,ell,steps)\npath2 = runOptim(init,momGD,ell,steps)\n# Set figure size\nshowPath(ell,init,[path1,path2],['Gradient Descent','Momentum'],['r','y'], 15)\n# Turn off axis ticks\nplt.xticks([])\nplt.yticks([])\nplt.xlabel(r'$w_1$', fontsize=14)\nplt.ylabel(r'$w_2$', fontsize=14)\nplt.savefig('images/regression_momentum.pdf', bbox_inches='tight', dpi=300)\n\n&lt;Figure size 400x200 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Step 1: Generate noisy linear data\nn = 10 # Number of observations\nnp.random.seed(0)\nX = np.linspace(0, 1, n).reshape(-1, 1)\ntrue_slope = 5\ntrue_intercept = 0\nnoise = np.random.normal(0, 1, n).reshape(-1, 1)  # Gaussian noise\ny = true_slope * X + true_intercept + noise\ny = y.ravel()\n\ny_preds = {}\n\n# Step 2: Fit linear regression model\nweights = np.linalg.lstsq(X, y, rcond=None)[0]  # Get weights from least squares solution\ny_preds['Linear'] = X.dot(weights).ravel()  # Recompute predictions using weights\n\nfor power in [5, 10]:\n    X_powers = X ** np.arange(0, power)\n    weights = np.linalg.lstsq(X_powers, y, rcond=None)[0]\n    y_preds[f'Degree {power}'] = X_powers.dot(weights).ravel()\n\n# Step 4: Plotting\nplt.figure(figsize=(5, 3))\n\nlinestyles = ['-', '--', '-.']\n\ncolors = ['#a8ddb5', '#41ab5d', '#005a32']\n\nfor idx, (label, y_pred) in enumerate(y_preds.items()):\n    plt.plot(X, y_pred, label=label, linewidth=3, alpha=0.7, zorder=1, linestyle=linestyles[idx], color=colors[idx])\n\nplt.scatter(X, y, label='Training Data', marker='o', color='#3B5998', s=60, zorder=2)  # Deep Cornflower Blue\n\nplt.legend()\nplt.title('Linear Regression vs Neural Network (Linear Data)')\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\nplt.tight_layout()\nplt.savefig('images/regression_overfitting.pdf', bbox_inches='tight', dpi=300)\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Step 1: Generate noisy quadratic data\nn = 10 # Number of observations\nnp.random.seed(0)\nX = np.linspace(0, 1, n).reshape(-1, 1)\ntrue_slope = 5\nnoise = np.random.normal(0, 1, n).reshape(-1, 1)  # Gaussian noise\ny = 20 * (X-.5)**2 + noise\ny = y.ravel()\n\ny_preds = {}\n\n# Step 2: Fit linear regression model\nweights = np.linalg.lstsq(X, y, rcond=None)[0]  # Get weights from least squares solution\ny_preds['Linear'] = X.dot(weights).ravel()  # Recompute predictions using weights\n\nfor power in [5, 10]:\n    X_powers = X ** np.arange(0, power)\n    weights = np.linalg.lstsq(X_powers, y, rcond=None)[0]\n    y_preds[f'Degree {power}'] = X_powers.dot(weights).ravel()\n\n# Step 4: Plotting\nplt.figure(figsize=(5, 3))\n\nlinestyles = ['-', '--', '-.']\ncolors = ['#a8ddb5', '#41ab5d', '#005a32']\n\nfor idx, (label, y_pred) in enumerate(y_preds.items()):\n    plt.plot(X, y_pred, label=label, linewidth=3, alpha=0.7, zorder=1, linestyle=linestyles[idx], color=colors[idx])\n\nplt.scatter(X, y, label='Training Data', marker='o', color='#3B5998', s=60, zorder=2)  # Deep Cornflower Blue\n\nplt.legend()\nplt.title('Linear Regression vs Neural Network (Quadratic Data)')\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\nplt.tight_layout()\nplt.savefig('images/regression_regularization.pdf', bbox_inches='tight', dpi=300)"
  },
  {
    "objectID": "notes/code.html#logistic-regression",
    "href": "notes/code.html#logistic-regression",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(5, 3))\nX = np.linspace(-5, 5, 1000).reshape(-1, 1)\nys = {}\n\nys['Step'] = np.where(X &gt; 0, 1, 0)  # Step function\nys['Logistic'] = 1 / (1 + np.exp(-X))  # Logistic function\n\ncolors = ['red', 'green']\nlinestyles = ['--', '-']\nfor idx, (label, y) in enumerate(ys.items()):\n    plt.plot(X, y, label=label, linewidth=3, alpha=0.7, zorder=1, linestyle=linestyles[idx], color=colors[idx])\n\nplt.legend()\nplt.title('Classification Outputs')\nplt.xlabel(r'$\\langle \\mathbf{x}, \\mathbf{w} \\rangle$')\nplt.ylabel(r'Output')\nplt.tight_layout()\nplt.savefig('images/classification_outputs.pdf', bbox_inches='tight', dpi=300)\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(5, 3))\nX = np.linspace(-5, 5, 1000).reshape(-1, 1)\nys = {}\n\nys[r'$\\ell_1$'] = np.abs(X - 1)\nys[r'$\\ell_2$'] =  (X-1)**2 /4 # Squared loss\nys['Cross Entropy'] = -np.log(1 / (1 + np.exp(-X)))  # Logistic function\n\ncolors = ['red', 'blue', 'green']\nlinestyles = ['--', '-.', '-']\nfor idx, (label, y) in enumerate(ys.items()):\n    plt.plot(X, y, label=label, linewidth=3, alpha=0.7, zorder=1, linestyle=linestyles[idx], color=colors[idx])\n\nplt.legend()\nplt.title('Classification Loss')\nplt.xlabel(r'$\\langle \\mathbf{x}, \\mathbf{w} \\rangle$')\nplt.ylabel(r'Loss if $y=1$')\nplt.tight_layout()\nplt.savefig('images/classification_loss.pdf', bbox_inches='tight', dpi=300)\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Set seed and generate balanced data\nnp.random.seed(42)\nn = 1000\n\n# Positive class: inside circle (radius &lt; 1.5)\npos = np.random.normal(0, 0.8, size=(n * 2, 2))\nr_pos = np.linalg.norm(pos, axis=1)\npos = pos[r_pos &lt; 1.4][:n // 2]\n\n# Negative class: just outside circle (radius &gt; 1.7)\nneg = np.random.normal(0, 1.2, size=(n * 2, 2))\nr_neg = np.linalg.norm(neg, axis=1)\nneg = neg[r_neg &gt; 1.6][:len(pos)]\n\n# Combine\nX = np.vstack([pos, neg])\ny = np.hstack([np.ones(len(pos)), np.zeros(len(neg))])\n\n# Colors\ncolors = np.array(['blue', 'red'])\nmarker_size = 10\n\n# Create figure\nfig = plt.figure(figsize=(9, 4))\n\n# --- 2D plot ---\nax1 = fig.add_subplot(1, 2, 1)\nax1.scatter(X[y == 0, 0], X[y == 0, 1], s=marker_size, c='blue', marker='^', edgecolors='none', label='Negative')\nax1.scatter(X[y == 1, 0], X[y == 1, 1], s=marker_size, c='green', marker='s', edgecolors='none', label='Positive')\n# Decision boundary\ncircle = plt.Circle((0, 0), 1.5, edgecolor='gray', fill=False, linewidth=2)\nax1.add_artist(circle)\n\n# Axis settings\nax1.set_xlim(-4, 4)\nax1.set_ylim(-4, 4)\nax1.set_aspect('equal')\n#ax1.set_title(\"Input Space\")\nax1.set_xlabel('$x_1$')\nax1.set_ylabel('$x_2$')\nax1.tick_params(left=True, bottom=True)\nfor spine in ax1.spines.values():\n    spine.set_visible(True)\n\n# --- 3D plot ---\nax2 = fig.add_subplot(1, 2, 2, projection='3d')\n\n# Feature transformation: z = - (x1^2 + x2^2)\nX3 = np.sum(X**2, axis=1)\nX_3d = np.hstack([X, X3[:, np.newaxis]])\n\nax2.scatter(X_3d[y == 0, 0], X_3d[y == 0, 1], X_3d[y == 0, 2],\n            c='blue', s=marker_size, marker='^', label='Negative', alpha=0.6)\nax2.scatter(X_3d[y == 1, 0], X_3d[y == 1, 1], X_3d[y == 1, 2],\n            c='green', s=marker_size, marker='s', label='Positive', alpha=0.6)\n\n\n# Separating plane: z = -2.25\nx1_range = np.linspace(-2, 2, 50)\nx2_range = np.linspace(-2, 2, 50)\nx1_grid, x2_grid = np.meshgrid(x1_range, x2_range)\nz_plane = 2.25 * np.ones_like(x1_grid)\n\nax2.plot_surface(x1_grid, x2_grid, z_plane, color='grey', alpha=0.5, edgecolor='none')\n\n# View and axis cleanup\nax2.view_init(elev=25, azim=135)\nax2.set_xlim(-4, 4)\nax2.set_ylim(-4, 4)\nax2.set_zlim(-20, 2)\n#ax2.set_title(\"Transformed Feature Space\")\nax2.set_xlabel(\"$x_1$\")\nax2.set_ylabel(\"$x_2$\")\nax2.set_zlabel(r\"$(x_1^2 + x_2^2)$\")\nax2.grid(False)\n\n# Adjust view and axis limits for a pointed cone effect\nax2.view_init(elev=10, azim=45)\nax2.set_xlim(-3, 3)\nax2.set_ylim(-3, 3)\nax2.set_zlim(-2, 10)  # Compress vertical scale to make cone sharper\n\n# Final layout adjustments\n# Set white background and black cube lines for 3D plot\nax2.set_facecolor('white')\nax2.xaxis.pane.set_edgecolor('black')\nax2.yaxis.pane.set_edgecolor('black')\nax2.zaxis.pane.set_edgecolor('black')\n\nplt.tight_layout()\nplt.savefig('images/classification_transformation.pdf', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\n\n# 1. Generate overlapping data\nnp.random.seed(0)\nn = 200\nmean_pos = [1, 1]\nmean_neg = [-1, -1]\ncov = [[1.5, 0.5], [0.5, 1.5]]\n\nX_pos = np.random.multivariate_normal(mean_pos, cov, size=n//2)\nX_neg = np.random.multivariate_normal(mean_neg, cov, size=n//2)\nX = np.vstack((X_pos, X_neg))\ny = np.hstack((np.ones(n//2), np.zeros(n//2)))\n\n# 2. Fit logistic regression\nclf = LogisticRegression()\nclf.fit(X, y)\n\n# 3. Create meshgrid\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 400),\n                     np.linspace(y_min, y_max, 400))\ngrid = np.c_[xx.ravel(), yy.ravel()]\nprobs = clf.predict_proba(grid)[:, 1].reshape(xx.shape)\n\n# 4. Plot\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Scatter data points\nax.scatter(X[y == 1, 0], X[y == 1, 1], c='green', s=10, marker='s', label='Positive')\nax.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', s=10, marker='^', label='Negative')\n\n\n# Thresholds and colors\nthresholds = [0.3, 0.5, 0.7]\nimport matplotlib\noranges = matplotlib.colormaps.get_cmap('Oranges')\ncolors = [oranges(i) for i in [0.4, 0.65, 0.9]] \n\nfor tau, color in zip(thresholds, colors):\n    CS = ax.contour(xx, yy, probs, levels=[tau], colors=[color], linewidths=2)\n    \n    # If there's a contour segment, label it manually\n    if len(CS.allsegs[0]) &gt; 0:\n        seg = CS.allsegs[0][0]\n        if len(seg) &gt; 0:\n            # Pick the right-most point (max x)\n            x_text, y_text = seg[np.argmax(seg[:, 0])]\n            ax.text(x_text + 0.2, y_text, f'τ={tau:.1f}',\n                    color=color, fontsize=9, ha='left', va='center',\n                    bbox=dict(boxstyle='round,pad=0.2', fc='white', ec='none', alpha=0.7))\n\n\n# Final formatting\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.legend()\nax.set_title(\"Decision Boundaries with Varying Thresholds\")\nplt.tight_layout()\nplt.savefig('images/classification_boundaries.pdf', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\n# --- 1. Generate overlapping data ---\nnp.random.seed(0)\nn = 200\nmean_pos = [1, 1]\nmean_neg = [-1, -1]\ncov = [[1.5, 0.5], [0.5, 1.5]]\n\nX_pos = np.random.multivariate_normal(mean_pos, cov, size=n//2)\nX_neg = np.random.multivariate_normal(mean_neg, cov, size=n//2)\nX = np.vstack((X_pos, X_neg))\ny = np.hstack((np.ones(n//2), np.zeros(n//2)))\n\n# --- 2. Fit logistic regression ---\nclf = LogisticRegression()\nclf.fit(X, y)\nprobs = clf.predict_proba(X)[:, 1]\n\n# --- 3. Compute ROC curve and AUC ---\nfpr, tpr, thresholds = roc_curve(y, probs)\nauc = roc_auc_score(y, probs)\n\n# --- 4. Plot ROC curve ---\nplt.figure(figsize=(6, 4))\nplt.plot(fpr, tpr, label=f\"Linear Regression (AUC = {auc:.2f})\", color='steelblue', linewidth=2)\nplt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Guessing (AUC = 0.5)')\n\n# --- 5. Highlight specific τ values ---\ntau_values = [0.3, 0.5, 0.7]\nimport matplotlib\noranges = matplotlib.colormaps.get_cmap('Oranges')\ncolors = [oranges(i) for i in [0.4, 0.65, 0.9]] \n\nfor tau, color in zip(tau_values, colors):\n    # Find closest index in thresholds array\n    idx = np.argmin(np.abs(thresholds - tau))\n    plt.scatter(fpr[idx], tpr[idx], color=color, edgecolor='black', zorder=5)\n    plt.text(fpr[idx]+0.02, tpr[idx]-0.02, f'τ={tau}', fontsize=9, color=color, verticalalignment='center')\n\n# --- 6. Format plot ---\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend()\nplt.tight_layout()\n\nplt.savefig('images/classification_roc.pdf', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "notes/03_PolynomialRegression.html",
    "href": "notes/03_PolynomialRegression.html",
    "title": "Polynomial Regression",
    "section": "",
    "text": "Introduce variables that give better fit to the data, manipulate the data to create new features, e.g., polynomial regression.\nClaim: The fit of the regression model can only be improved by adding additional features.\n(Add picture here of polynomial regression with various degrees of polynomial, e.g., linear, quadratic, cubic, etc.)\nIn picture, we can tell which gives the right fit\nHow do we do this automatically?\n\n\nTraining data versus testing data\nGeneralization error\nIf we withhold data, this gives us an unbiased estimate of the generalization error: Expectation of performance on random sample, is performance on true data set.\nCan we use data more efficiently? \\(k\\)-fold cross validation\nIn practice, we often bias this process by training, testing, updating the model (hyperparameters, architecture, training method), training, testing, updating the model, etc. The repeated use of the test data means our model depends on the test data and eventually overfits to it, even though we are not using the test data to train the model.\n\n\n\nNon-linear models are incredibly powerful models that can approximate any function, given enough data and features. However, this power comes with a cost: non-linear models can be very complex models and can easily overfit the training data. That is, they can learn to memorize, but fail to generalize to unseen data.\n(Redo this figures with polynomial regression)\n\n\n\nWhen we believe our data comes from a simpler generating process, it makes sense to use a simpler model. Even when that simpler generating process is not a linear model, we can attempt to find simpler models through regularization. Regularization is a technique that adds a penalty to the loss function to discourage the model from fitting the training data too closely.\n\\[\n\\begin{align*}\n\\frac1n \\sum_{i=1}^n (f(\\mathbf{x}^{(i)}) - y^{(i)})^2 + \\lambda \\|\\mathbf{w}\\|^2_2,\n\\end{align*}\n\\] where \\(\\lambda\\) is a hyperparameter that controls the strength of the penalty and \\(\\|\\mathbf{w}\\|_2\\) is the \\(\\ell-2\\) norm of the weights, which is the square root of the sum of the squares of the weights.\nThe idea is that we can keep the model “simple” by penalizing large weights, which would otherwise allow the model to achieve large changes in the output for small changes in the input.\n\n\n\nIn the plot, we see data generated from a quadratic function. The linear model is too simple and fails to capture the underlying relationship, while the standard neural network is too complex and overfits the training data. The regularized neural network, however, is able to capture the underlying relationship while still being simple enough to generalize to unseen data.\nHow to control the strength of the regularization \\(\\lambda\\)?\nHow do we minimize the regularized loss function for regression?\nWe can also use a regularizaiton with \\(\\ell_1\\) norm, which penalizes the absolute value of the weights. This is called Lasso regression, and it has the effect of encouraging sparsity in the model, i.e., some weights will be exactly zero. Useful when we have many features, but we believe only a few of them are actually relevant to the prediction."
  },
  {
    "objectID": "notes/03_PolynomialRegression.html#polynomial-regression",
    "href": "notes/03_PolynomialRegression.html#polynomial-regression",
    "title": "Polynomial Regression",
    "section": "",
    "text": "Introduce variables that give better fit to the data, manipulate the data to create new features, e.g., polynomial regression.\nClaim: The fit of the regression model can only be improved by adding additional features.\n(Add picture here of polynomial regression with various degrees of polynomial, e.g., linear, quadratic, cubic, etc.)\nIn picture, we can tell which gives the right fit\nHow do we do this automatically?\n\n\nTraining data versus testing data\nGeneralization error\nIf we withhold data, this gives us an unbiased estimate of the generalization error: Expectation of performance on random sample, is performance on true data set.\nCan we use data more efficiently? \\(k\\)-fold cross validation\nIn practice, we often bias this process by training, testing, updating the model (hyperparameters, architecture, training method), training, testing, updating the model, etc. The repeated use of the test data means our model depends on the test data and eventually overfits to it, even though we are not using the test data to train the model.\n\n\n\nNon-linear models are incredibly powerful models that can approximate any function, given enough data and features. However, this power comes with a cost: non-linear models can be very complex models and can easily overfit the training data. That is, they can learn to memorize, but fail to generalize to unseen data.\n(Redo this figures with polynomial regression)\n\n\n\nWhen we believe our data comes from a simpler generating process, it makes sense to use a simpler model. Even when that simpler generating process is not a linear model, we can attempt to find simpler models through regularization. Regularization is a technique that adds a penalty to the loss function to discourage the model from fitting the training data too closely.\n\\[\n\\begin{align*}\n\\frac1n \\sum_{i=1}^n (f(\\mathbf{x}^{(i)}) - y^{(i)})^2 + \\lambda \\|\\mathbf{w}\\|^2_2,\n\\end{align*}\n\\] where \\(\\lambda\\) is a hyperparameter that controls the strength of the penalty and \\(\\|\\mathbf{w}\\|_2\\) is the \\(\\ell-2\\) norm of the weights, which is the square root of the sum of the squares of the weights.\nThe idea is that we can keep the model “simple” by penalizing large weights, which would otherwise allow the model to achieve large changes in the output for small changes in the input.\n\n\n\nIn the plot, we see data generated from a quadratic function. The linear model is too simple and fails to capture the underlying relationship, while the standard neural network is too complex and overfits the training data. The regularized neural network, however, is able to capture the underlying relationship while still being simple enough to generalize to unseen data.\nHow to control the strength of the regularization \\(\\lambda\\)?\nHow do we minimize the regularized loss function for regression?\nWe can also use a regularizaiton with \\(\\ell_1\\) norm, which penalizes the absolute value of the weights. This is called Lasso regression, and it has the effect of encouraging sparsity in the model, i.e., some weights will be exactly zero. Useful when we have many features, but we believe only a few of them are actually relevant to the prediction."
  },
  {
    "objectID": "notes/03_PolynomialRegression.html#going-forward",
    "href": "notes/03_PolynomialRegression.html#going-forward",
    "title": "Polynomial Regression",
    "section": "Going Forward",
    "text": "Going Forward\nToday, we got a taste of how to use gradient descent to optimize non-linear models, particularly neural networks. This is a rich area that has seen incredible recent advancements, particularly in the context of generative AI. In fact, I teach an entire course dedicated to deep learning. In this course, however, we will instead focus on the mathematical foundations of machine learning.\nWe have so far explored supervised learning in the regression setting, where the labels are real numbers. going forward, we will consider how to handle the case where the labels are categorical, e.g., we want to classify the data into two categories such as “cat” and “dog” or “spam” and “not spam”."
  },
  {
    "objectID": "notes/01_LinearAlgebra.html",
    "href": "notes/01_LinearAlgebra.html",
    "title": "Linear Algebra Review",
    "section": "",
    "text": "When I first heard about machine learning, I imagined a computer that was rewarded every time it gave the right answer. Maybe there were electric carrots and sticks that no one had bothered to tell me about? While I now know as little as I did then about computer hardware, I have learned that machine learning is fundamentally a mathematical process.\nLuckily, we’ve been learning about the very mathematical ideas that make machine learning work for years! We’ll review the basics of these concepts here.\n\nDerivatives\nConsider a function \\({f}: \\mathbb{R} \\to \\mathbb{R}\\). The mapping notation means that \\({f}\\) takes a single real number as input and outputs a single real number. In general, mathematicians tell us to be careful about whether we can differentiate a function. But, we’re computer scientists so we’ll risk it for the biscuit.\nLet \\(x \\in \\mathbb{R}\\) be the input to \\({f}\\). The derivative of \\({f}\\) with respect to its input \\(x\\) is mathematically denoted by \\(\\frac{\\partial}{\\partial x}[{f}(x)]\\).\nFormally, the derivative is defined as \\[\n\\frac{\\partial}{\\partial x}[{f}(x)]\n= \\lim_{h \\to 0} \\frac{{f}(x + h) - {f}(x)}{h}.\n\\] If we were to plot \\({f}\\), the derivative at a point \\(x\\) would be the slope of the tangent line to the curve at that point.\nHere are several examples of functions and their derivatives that you might remember from calculus.\n\n\n\nFunction: \\({f}(x)\\) \n\n\nDerivative: \\(\\frac{\\partial}{\\partial x}[{f}(x)]\\)\n\n\n\n\n\\[x^2\\]\n\n\n\\[2x\\]\n\n\n\n\n\\[x^a\\]\n\n\n\\[a x^{a-1}\\]\n\n\n\n\n\\[ax + b\\]\n\n\n\\[a\\]\n\n\n\n\n\\[\\ln(x)\\]\n\n\n\\[\\frac{1}{x}\\]\n\n\n\n\n\\[e^x\\]\n\n\n\\[e^x\\]\n\n\n\n\n\nChain Rule and Product Rule\nWhile working with a simple basic function is easy, we’re not always so lucky. Modern machine learning chains many many complicated functions together. Fortunately, we will think of these operations modularly.\nLet \\(g: \\mathbb{R} \\to \\mathbb{R}\\) be another function. Consider the composite function \\(g({f}(x))\\).\nBy the chain rule, the derivative of \\(g({f}(x))\\) with respect to \\(x\\) is \\[\n\\frac{\\partial }{\\partial x}[g({f}(x))]\n= \\frac{\\partial g}{\\partial x}({f}(x))\n\\frac{\\partial}{\\partial x}[{f}(x)].\n\\]\nOften, we will also multiply functions together. The product rule tells us that \\[\n\\frac{\\partial }{\\partial x}[g(x) {f}(x)]\n= g(x) \\frac{\\partial}{\\partial x}[{f}(x)]\n+ {f}(x) \\frac{\\partial}{\\partial x}[g(x)].\n\\]\n\n\nGradients\nIn machine learning, we process high-dimensional data so we are interested in functions with multivariate input. Consider \\({f}: \\mathbb{R}^d \\to \\mathbb{R}\\). The output of the function is still a real number but the input consists of \\(d\\) real numbers. We will use the vector \\(\\mathbf{x} \\in \\mathbb{R}^d\\) to represent all \\(d\\) inputs \\(x_1, \\ldots, x_d\\).\nInstead of the derivative, we will talk use the partial derivative. The partial derivative with respect to \\(x_i\\) is denoted by \\(\\frac{\\partial}{\\partial x_i}[{f}(\\mathbf{x})]\\). In effect, the partial derivative tells us how \\({f}\\) changes when we change \\(x_i\\), while keeping all other inputs fixed.\nThe gradient stores all the partial derivatives in a vector. The \\(i\\)th entry of this vector is given by the partial derivative of \\({f}\\) with respect to \\(x_i\\). In mathematical notation, \\[\n\\nabla_\\mathbf{x} {f} = \\left[\\begin{smallmatrix} \\frac{\\partial}{\\partial x_1}[{f}(\\mathbf{x})] \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_d}[{f}(\\mathbf{x})] \\\\ \\end{smallmatrix}\\right]\n\\]\nJust like the derivative in one dimension, the gradient contains information about the slope of \\({f}\\) with respect to each of the \\(d\\) dimensions in its input.\n\n\nVector and Matrix Multiplication\nVector and matrix multiplication lives at the heart of deep learning. In fact, deep learning really started to take off when researchers realized that the Graphical Processing Unit (GPU) could be used to perform gradient updates in addition to the matrix multiplication it was designed to do for gaming.\nConsider two vectors \\(\\mathbf{u} \\in \\mathbb{R}^d\\) and \\(\\mathbf{v} \\in \\mathbb{R}^d\\). We will use \\(\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^d u_i v_i\\) to denote the inner product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). We can also write the inner product as \\(\\mathbf{u}^\\top \\mathbf{v}\\), where \\(\\mathbf{u}^\\top \\in \\mathbb{R}^{1\\times d}\\) is the transpose of \\(\\mathbf{u}\\). The \\(\\mathcal{\\ell}_2\\)-norm of \\(\\mathbf{v}\\) is given by \\(\\|\\mathbf{v}\\|_2 = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}}\\).\nConsider two matrices: \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times m}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{m \\times n}\\) where \\(d \\neq n\\). We can only multiply two matrices when their inner dimension agrees. Because the number of columns in \\(\\mathbf{A}\\) is the same as the number of rows in \\(\\mathbf{B}\\), we can compute \\(\\mathbf{AB}\\). However, because the number of columns in \\(\\mathbf{B}\\) is not the same as the number of rows in \\(\\mathbf{A}\\), the product \\(\\mathbf{BA}\\) is not defined.\nWhen we can multiply two matrices, the \\((i,j)\\) entry in \\(\\mathbf{AB}\\) is given by the inner product between the \\(i\\)th row of \\(\\mathbf{A}\\) and the \\(j\\)th column of \\(\\mathbf{B}\\). The resulting dimensions of the matrix product will be the number of rows in \\(\\mathbf{A}\\) by the number of columns in \\(\\mathbf{B}\\).\n\n\nEigenvalues and Eigenvectors\nAn eigenvector of a square matrix \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times d}\\) is a vector \\(\\mathbf{v} \\in \\mathbb{R}^d\\) such that \\(\\mathbf{Av} = \\lambda \\mathbf{v}\\) for some scalar \\(\\lambda\\). Let \\(r\\) be the number of eigenvectors of \\(\\mathbf{A}\\). Then, we can write the eigenvectors as \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_r\\). The eigenvectors are orthonormal, meaning that \\(\\mathbf{v}_i \\cdot \\mathbf{v}_j = 0\\) for \\(i \\neq j\\) and \\(\\|\\mathbf{v}_i\\|_2 = 1\\) for all \\(i\\). The corresponding eigenvalues are \\(\\lambda_1 \\geq \\ldots \\geq \\lambda_r\\).\nGiven these properties, we can write \\[\\mathbf{A} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^\\top\\] where \\(\\mathbf{V} = [\\mathbf{v}_1, \\ldots, \\mathbf{v}_r] \\in \\mathbb{R}^{d \\times r}\\) is the matrix of eigenvectors and \\(\\mathbf{\\Lambda} = \\text{diag}(\\lambda_1, \\ldots, \\lambda_r) \\in \\mathbb{R}^{r \\times r}\\) is the diagonal matrix of eigenvalues. This decomposition is known as the eigenvalue decomposition of \\(\\mathbf{A}\\). We can equivalently write the eigenvalue decomposition as \\[\\mathbf{A} = \\sum_{i=1}^r \\lambda_i \\mathbf{v}_i \\mathbf{v}_i^\\top\\] where \\(\\mathbf{v}_i \\mathbf{v}_i^\\top\\) is the outer product of the eigenvector \\(\\mathbf{v}_i\\) with itself.\nWe can check that the eigenvalue decomposition is correct by multiplying by each eigenvector. Since the eigenvectors are orthonormal, we have \\(\\mathbf{A} \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i\\) for each \\(i\\).\n\n\nInverse Matrices\nIf we have a scalar equation \\(ax = b\\), we can simply solve for \\(x\\) by dividing both sides by \\(a\\). In effect, we are applying the inverse of \\(a\\) to \\(a\\) i.e., \\(\\frac1{a} a =1\\). The same principle applies to matrices. The \\(n \\times n\\) identity matrix generalizes the scalar identity \\(1\\). This identity matrix is denoted by \\(\\mathbf{I}_{n \\times n} \\in \\mathbb{R}^{n \\times n}\\): the on-diagonal entries \\((i,i)\\) are 1 while the off-diagonal entries \\((i,j)\\) for \\(i\\neq j\\) are 0.\nConsider the matrix equation \\(\\mathbf{Ax} = \\mathbf{b}\\) where \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times d}\\), \\(\\mathbf{x} \\in \\mathbb{R}^d\\), and \\(\\mathbf{b} \\in \\mathbb{R}^d\\). (Notice that the inner dimensions of \\(\\mathbf{A}\\) and \\(\\mathbf{x}\\) agree so their multiplication is well-defined, and the resulting vector is the same dimension as \\(\\mathbf{b}\\).)\nIf we want to solve for \\(\\mathbf{x}\\), we can use the matrix inverse. For a matrix \\(\\mathbf{A}\\), we use \\(\\mathbf{A}^{-1}\\) to denote its inverse. (When \\(\\mathbf{A}\\) does not have full rank, i.e., \\(r &lt; d\\), the inverse is not defined, but we can still use the eigenvalue decomposition to find a pseudo-inverse.) The inverse is defined so that \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\) where \\(\\mathbf{I}_{n \\times n}\\) is the identity matrix. In terms of our eigenvalue decomposition, the inverse of \\(\\mathbf{A}\\) is given by \\[\\mathbf{A}^{-1} = \\mathbf{V} \\mathbf{\\Lambda}^{-1} \\mathbf{V}^\\top\\] where \\(\\mathbf{\\Lambda}^{-1} = \\text{diag}(\\frac{1}{\\lambda_1}, \\ldots, \\frac{1}{\\lambda_r})\\) is the diagonal matrix of the inverses of the eigenvalues. Equivalently, we can write the inverse as \\[\\mathbf{A}^{-1} = \\sum_{i=1}^r \\frac{1}{\\lambda_i} \\mathbf{v}_i \\mathbf{v}_i^\\top\\] where \\(\\mathbf{v}_i\\) are the eigenvectors of \\(\\mathbf{A}\\). Given these two equivalent descriptions of \\(\\mathbf{A}^{-1}\\), do you see why \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\)?\nWith the inverse in hand, we can solve for \\(\\mathbf{x}\\) by multiplying both sides of the equation by \\(\\mathbf{A}^{-1}\\). \\[\n\\mathbf{A}^{-1} \\mathbf{Ax} = \\mathbf{A}^{-1} \\mathbf{b}\n\\] Since \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\), we have that \\(\\mathbf{I}_{n \\times n} \\mathbf{x} = \\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}\\)."
  },
  {
    "objectID": "notes/04_Probability.html",
    "href": "notes/04_Probability.html",
    "title": "Probability",
    "section": "",
    "text": "Instead of predicting a continuous value, there are many applications where we want to predict a discrete value. For example, we might want to predict whether an email is spam or not, whether a patient has a disease or not, or whether an image contains a cat or not. In these cases, we can use a classification model instead of a regression model.\nWe will review some basic concepts of probability that are useful for understanding classification models. We will also introduce Bayes’ Rule, and see how useful it can be for making predictions.\nLet \\(A\\) and \\(B\\) be two events. For example, \\(A\\) could be the event that it rains tomorrow, and \\(B\\) could be the event that I wear a raincoat tomorrow. We will use the following notation:\n• \\(\\Pr(A)\\) represents the probability that event \\(A\\) occurs,\n• \\(\\Pr(A \\cup B)\\) represents the probability that either event \\(A\\) or event \\(B\\) occurs,\n• \\(\\Pr(A \\cap B)\\) represents the probability that both events \\(A\\) and \\(B\\) occur,\n• \\(\\Pr(A | B)\\) represents the probability that event \\(A\\) occurs given that event \\(B\\) has occurred.\n\n\n\nWe can reason through several properties of probabilities:\nProbability Range The probability of an event is always between 0 and 1, i.e., \\(0 \\leq \\Pr(A) \\leq 1\\) for all events \\(A\\). An event with probability 0 never occurs, while an event with probability 1 is certain to occur.\nConditional Probabilities We can write the probability of both events \\(A\\) and \\(B\\) occurring in terms of conditional probabilities: \\[\\Pr(A \\cap B) = \\Pr(B) \\Pr(A | B)  = \\Pr(A) \\Pr(B | A).\\] This means that the probability of both events occurring is the product of the probability of one event and the conditional probability of the other event given that the first event has occurred.\nUnion of Events The probability that either event \\(A\\) or event \\(B\\) occurs is given by: \\[\\Pr(A \\cup B) = \\Pr(A) + \\Pr(B) - \\Pr(A \\cap B).\\] This means that the probability of either event occurring is the sum of the probabilities of each event minus the probability of both events occurring together (we subtract the event that both occur because it is counted twice in the sum).\nComplement Rule The complement of an event \\(A\\) is the event that \\(A\\) does not occur, denoted as \\(\\neg A\\). Since either \\(A\\) occurs or \\(\\neg A\\) occurs, we have that \\(Pr(A) + \\Pr(\\neg A) = 1\\). Rearranging this gives us that \\(\\Pr(\\neg A) = 1 - \\Pr(A)\\).\nIndependence Two events \\(A\\) and \\(B\\) are independent if the occurrence of one event does not affect the probability of the other event occurring. In this case, we have that \\(\\Pr(A \\cap B) = \\Pr(A) \\Pr(B)\\). Equivalently, \\(\\Pr(A | B) = \\Pr(A)\\) and \\(\\Pr(B | A) = \\Pr(B)\\), do you see why this follows?\nWe will often model random events with random variables. A random variable is a function that maps outcomes to real numbers. For example, we could define a random variable \\(X\\) that takes the outcome of a die roll and maps it to the number on the die. The probability distribution of a random variable describes the probabilities of each possible outcome. In our die example, if we roll a fair six-sided die, the probability distribution of the random variable \\(X\\) is given by \\(\\Pr(X = x) = \\frac{1}{6}\\) for \\(x \\in \\{1, 2, 3, 4, 5, 6\\}\\).\nBayes’ Rule is a particularly useful rule in machine learning: For any two events \\(A\\) and \\(B\\) with \\(\\Pr(B) &gt; 0\\), \\[\\Pr(A | B) = \\frac{\\Pr(B | A) \\Pr(A)}{\\Pr(B)}.\\]\nBased on what we have seen so far, can you prove Bayes’ Rule?\n\nMaximum A Posteriori (MAP) Estimation\nWhen we justified the mean squared error loss for regression, we said that it was the maximum likelihood estimate (MLE) of the parameters of a linear model. We can extend this idea to classification models as well. Suppose we have a binary classification problem, where we want to predict whether the random variable \\(Y=0\\) or \\(Y=1\\). We observe evidence \\(\\mathbf{X} = \\mathbf{x}\\), e.g., \\(\\mathbf{X}\\) is the random variable that takes on the value of the features of an email, and we want to predict whether the email is spam or not. We will compare the posteriors \\[\\Pr(Y = 1 | \\mathbf{X} = \\mathbf{x})\\] and \\[\\Pr(Y = 0 | \\mathbf{X} = \\mathbf{x})\\] to determine which class is more likely given the evidence.\nHowever, it’s not immediately clear how to compute these probabilities. Luckily, we can use Bayes’ Rule to rewrite the posteriors. Without loss of generality, consider the event that \\(Y = 1\\). Then \\[\n\\begin{align*}\n\\Pr(Y = 1 | \\mathbf{X} = \\mathbf{x})\n&= \\frac{\\Pr(\\mathbf{X} = \\mathbf{x} | Y = 1) \\Pr(Y = 1)}{\\Pr(\\mathbf{X} = \\mathbf{x})} \\\\\n&= \\frac{\\textnormal{likelihood} \\cdot \\textnormal{prior}}{\\textnormal{evidence}}\n\\end{align*}.\n\\]\nLet’s get some familiarity through a medical example. Suppose we have a medical test that can detect whether a patient has a particular disease. The disease is rare, affecting only 1% of the population. The test is unfortunately not perfect: it has a false positive rate of 5% and a false negative rate of 10%. Suppose \\(X=1\\) i.e., the test is positive. Is it more likely that the patient has the disease or not?\nIn this medical example, we were explicitly given the likelihood of the disease. What can we do when our only information is the labelled data?\n\n\nNaive Bayes Classifier\nThe Naive Bayes Classifier is a simple yet effective classification algorithm that uses Bayes’ Rule to make predictions. The key assumption of the Naive Bayes Classifier is that the features are conditionally independent given the class label. This means that the presence or absence of a feature does not affect the presence or absence of another feature, given the class label.\nLet’s see an example with a spam classifier. Suppose we have a dataset of emails, each labelled as either spam or not spam. We want to predict whether a new email is spam or not based on its features, such as the presence of certain words. In particular, let \\(\\mathbf{X} = (X_1, X_2, \\ldots, X_d)\\) be the features of the email, where \\(X_i\\) is a binary variable indicating whether the \\(i\\)th word is present in the email. Let \\(p_i^{(1)} = \\Pr(X_i=1 | Y=0)\\) be the probability that the \\(i\\)th word is present in a spam email, while \\(p_i^{(0)} = \\Pr(X_i=1 | Y=1)\\) be the probability that the \\(i\\)th word is present in a non-spam email. We will use the independence assumption to compute the likelihood of the features given the class label. For example, \\[\n\\Pr(\\mathbf{X} = (0, 1, 0, 0, 1) | Y = 1) =\n(1-p_1^{(1)}) p_2^{(1)} (1-p_3^{(1)}) (1-p_4^{(1)}) p_5^{(1)}.\n\\]\nBeyond the likelihood, we also need the prior probability of the class label. This is even easier to compute, e.g., we can simply compute the fraction of spam and non-spam emails in the training set. Then we can use Bayes’ Rule to determine whether the posterior probability of the email being spam is greater than the posterior probability of the email being non-spam.\nMore formally, we can use the Naive Bayes Classifier by\n\nComputing \\(\\Pr(Y = 1)\\) and \\(\\Pr(Y = 0)\\) from the training data.\nComputing the observed probabilities \\(\\Pr(X_i = 1 | Y = 1)\\) and \\(\\Pr(X_i = 1 | Y = 0)\\) for each feature \\(X_i\\) from the training data.\nComputing the likelihoods \\(\\Pr(\\mathbf{X} = \\mathbf{x} | Y=1)\\) and \\(\\Pr(\\mathbf{X} = \\mathbf{x} | Y=0)\\) using the independence assumption e.g., \\[\n\\Pr(\\mathbf{X} = \\mathbf{x} | Y=1) = \\prod_{i=1}^d \\Pr(X_i = x_i | Y=1).\n\\]\nUsing Bayes’ Rule to compute the posteriors, and predicting the class label \\(y \\in \\{0,1\\}\\) with the highest posterior probability: \\[\n\\Pr(Y = y | \\mathbf{X} = \\mathbf{x}) = \\frac{\\Pr(\\mathbf{X} = \\mathbf{x} | Y=y) \\Pr(Y=y)}{\\Pr(\\mathbf{X} = \\mathbf{x})}.\n\\]"
  }
]