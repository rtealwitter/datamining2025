---
title: "**Probability and Logistic Regression**"
format:
  html:
    toc: true
    math: true
---

Instead of predicting a continuous value, there are many applications where we want to predict a discrete value. For example, we might want to predict whether an email is spam or not, whether a patient has a disease or not, or whether an image contains a cat or not. In these cases, we can use a **classification** model instead of a regression model.

## Probability

Before we can build a classification model, we need to understand the concept of probability. Probability is a measure of how likely an event is to occur.

Let $A$ and $B$ be two events. For example, $A$ could be the event that it rains tomorrow, and $B$ could be the event that I wear a raincoat tomorrow.
We will use the following notation:  
• $\Pr(A)$ represents the probability that event $A$ occurs,  
• $\Pr(A \cap B)$ represents the probability that both events $A$ and $B$ occur,  
• $\Pr(A \cup B)$ represents the probability that either event $A$ or event $B$ occurs,  
• $\Pr(A | B)$ represents the probability that event $A$ occurs given that event $B$ has occurred.

Let's see it in a Venn diagram.

[image here]

We can reason through several properties of probabilities.

* **Probability Range** The probability of an event is always between 0 and 1, inclusive: $0 \leq \Pr(A) \leq 1$. An event with probability 0 is impossible, while an event with probability 1 is certain to occur.

* **Conditional Probabilities** We can write the probability of both events $A$ and $B$ occurring in terms of conditional probabilities:
  $$\Pr(A \cap B) = \Pr(A | B) \Pr(B) = \Pr(B | A) \Pr(A).$$
  This means that the probability of both events occurring can be expressed as the product of the probability of one event given the other and the probability of the other event.

* **Complement Rule** The *complement* of an event $A$ is the event that $A$ does not occur, denoted as $\neg A$. Since either $A$ occurs or $\neg A$ occurs, we have that $Pr(A) + \Pr(\neg A) = 1$. Rearranging this gives us that $\Pr(\neg A) = 1 - \Pr(A)$.

* **Independence** Two events $A$ and $B$ are *independent* if the occurrence of one event does not affect the probability of the other event occurring. In this case, we have that $\Pr(A \cap B) = \Pr(A) \Pr(B)$. Equivalently, $\Pr(A | B) = \Pr(A)$ and $\Pr(B | A) = \Pr(B)$, do you see why this follows?

We will often model random events with *random variables*. A random variable is a function that maps outcomes to real numbers. For example, we could define a random variable $X$ that takes the outcome of a die roll and maps it to the number on the die. The probability distribution of a random variable describes the probabilities of each possible outcome. In our die example, if we roll a fair six-sided die, the probability distribution of the random variable $X$ is given by $\Pr(X = x) = \frac{1}{6}$ for $x \in \{1, 2, 3, 4, 5, 6\}$.

A particularly useful rule in machine learning is **Bayes' Rule**, which allows us to more easily compute conditional probabilities. Bayes' Rule states that
$$\Pr(A | B) = \frac{\Pr(B | A) \Pr(A)}{\Pr(B)}.$$

Based on what we have so far learned about probability, can you prove Bayes' Rule?

### Maximum A Posteriori (MAP) Estimation

### Naive Bayes Classifier

## Logistic Regression