[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "CSCI 145: Syllabus",
    "section": "",
    "text": "Course Description: Data mining is the process of discovering patterns in large data sets using techniques from mathematics, computer science and statistics with applications ranging from biology and neuroscience to history and economics. The goal of the course is to teach students fundamental data mining techniques that are commonly used in practice. Students will learn advanced data mining techniques (including linear classifiers, clustering, dimension reduction, transductive learning and topic modeling).\nPrerequisites: I expect familiarity with calculus, linear regression, probability, and Python. In particular, I expect you are comfortable with derivatives, the chain rule, gradients, matrix multiplication, and probability distributions. If this isn’t the case, please contact me as soon as possible.\nStructure: We will meet on Tuesdays and Thursdays. The first section is from 2:45 to 4pm and the second section is from 4:15 to 5:30pm. I will hold my office hours TBD. If you would like to meet outside of these times, please email me.\nResources: The primary resource for this class are the typed notes on the homepage. I highly recommend reading them before each class (it should take about 15 minutes). In addition, I will post my preparation for the slides the night before each class.\nDiscussion: Please post all your course related questions on Canvas. If your question reveals your solution to a homework problem, please email me instead.\n\nGrading\nYour grade in the class will be based on the number of points \\(p\\) that you earn. You will receive an A if \\(p \\geq 93\\), an A- if \\(93 &gt; p \\geq 90\\), a B+ if \\(90 &gt; p \\geq 87\\), and so on. You may earn points through the following assignments:\n\nParticipation (10 points): The classes at CMC are intentionally small. Unless you have a reasonable excuse (e.g. sickness, family emergency), I expect you to attend every class. Whether you are able to attend or not, I expect you to fill out the form linked from the home page to receive credit for participation (one point per lecture day that you fill it out). Of course, if you are not able to attend in person, you should read the notes before filling out the form.\nProblem Sets (10 Points): Learning requires practice. Your main opportunity to practice the concepts we cover in this class will be on the problem sets. Your grade will be based on turning in solutions to each problem and, so that you engage with the solutions, a self grade of your own work. Because I do not want to incentivize the use of LLMs, I will not grade your solutions for correctness; instead, your problem set grade is based on completion and the accuracy of your own self grade.\nQuizzes (20 Points): In lieu of grading for correctness on the problem sets, I will give short quizzes at the beginning of our Tuesday classes. These quizzes will be based on the problem sets and will test your understanding of the concepts we cover in class. The quizzes will be short (10-15 minutes) and will be graded for correctness.\nWritten Exam (20 Points): The first midterm will be a written exam. It will cover the material we have covered in class up to that point. The exam will be open book and open notes, but you will not be allowed to use any electronic devices (including your phone). The exam will be graded for correctness.\nVerbal Exam (20 Points): The second midterm will be a verbal exam. I will individually ask you questions about the concepts we have covered in class during a 30-minute meeting. The goal is to simultaneously assess your understanding of the material and give you a chance to practice explaining the concepts, as you would in a technical interview. I will provide a list of topics that I will ask about in advance.\nProject (20 Points): The final project will be a chance for you to apply the concepts we have covered in class to a real-world problem. You will select a topic we cover in class and implement an algorithm we discussed on a data set of your choosing. You will write a report describing your results and what you learned. You will also give a presentation showcasing your results to the class. Except in special circumstances, you will complete your project as an individual.\nExtra Credit: This is the first time I am teaching this class, so my typed notes are work in progress and I would love your help improving them! If you find an issue, please email me. I will give extra credit to the first person to find each typo (worth 1/4 point), ambiguous statement (worth 1/2 point), and mistake (worth 1 point)\n\nLate Policy: I expect all assignments to be turned in on time. If you are unable to turn in an assignment on time, you must email me 24 hours before the assignment is due to request an extension.\n\n\nHonor Code\nAcademic integrity is an important part of your learning experience. You are welcome to use online material and discuss problems with others but you must explicitly acknowledge the outside resources (website, person, or LLM) on the work you submit.\nLarge Language Models: LLMs are a powerful tool. However, while they are very good at producing human-like text, they have no inherent sense of ‘correctness’. You may use LLMs (as detailed below) but you are wholly responsible for the material you submit.\nYou may use LLMs for:\n\nImplementing short blocks of code that you can easily check.\nAnswering simple questions whose answers you can easily verify.\n\nDo not use LLMS for:\n\nImplementing extensive blocks of code or code that you don’t understand.\nAnswering complicated questions (like those on the problem sets) that you cannot easily verify.\n\nUltimately, the point of the assignments in this class are for you to practice the concepts. If you use an LLM in lieu of practice, then you deny yourself the chance to learn.\n\n\nAcademic Accommodations\nIf you have a Letter of Accommodation, please contact me as early in the semester as possible. If you do not have a Letter of Accommodation and you believe you are eligible, please reach out to Accessibility Services at accessibilityservices@cmc.edu."
  },
  {
    "objectID": "notes/09_Autoencoders.html",
    "href": "notes/09_Autoencoders.html",
    "title": "Autoencoders",
    "section": "",
    "text": "We have so far assumed that our data is labeled (e.g., we know an image depicts a dog). There is a wealth of unlabeled data available, and we would like to use it to improve our models. For example, think of the many images on the web, or millions of documents of text. Today, we will explore how to leverage this unlabeled data using unsupervised learning techniques.\n\nIntroduction\nThere are many simple but clever ideas that we will see in unsupervised learning. The first is an autoencoder, which allows us to learn a meaningful, latent representation of the input data. The idea of autoencoders is quite natural: when we don’t have labels for our data, let’s use the data itself as a label.\nLet \\(d\\) be the dimension of the input data. We will map the input data to a compressed representation in \\(k\\) dimensions. Let \\(f_0: \\mathbb{R}^d \\to \\mathbb{R}^k\\) be the encoder, often a neural network, that maps the input data \\(\\boldsymbol{x}\\) to the latent representation \\(\\boldsymbol{z} = f_0(\\boldsymbol{x})\\). We will then map the latent representation back to the original space with a decoder \\(f_1: \\mathbb{R}^k \\to \\mathbb{R}^d\\), also a neural network, where \\(\\tilde{\\boldsymbol{x}} = f_1(\\boldsymbol{z})\\) is the reconstruction of the original input \\(\\boldsymbol{x}\\).\nOur goal is to minimize the reconstruction error, which is typically measured as the mean squared error (MSE) between the original input \\(\\boldsymbol{x}\\) and the reconstructed input \\(\\tilde{\\boldsymbol{x}}\\):\n\\[\n\\mathcal{L}_{\\text{recon}} = \\frac1{n} \\sum_{i=1}^n \\| \\boldsymbol{x}^{(i)} - \\tilde{\\boldsymbol{x}}^{(i)} \\|^2_2.\n\\]\n[image here of autoencoder bottle neck]\nNotice that this problem is trivial when \\(k \\geq d\\), since we can simply represent \\(\\boldsymbol{z} = \\boldsymbol{x}\\) in the latent space without any loss of information. Crucially, our architecture must have a bottleneck, meaning that we need to enforce \\(k &lt; d\\) in order to learn a non-trivial representation.\nThere are many applications of autoencoders:\n\ndata compression (for example, JPEG is a form of lossy compression that can be interpreted as an autoencoder),\ndenoising (removing noise from images),\ninpainting (filling in missing parts of images),\nrepresentation learning (discovering useful features that can be used in downstream applications).\n\n[image here of inpainting]\nWhen we map from \\(\\mathbb{R}^d\\) to a smaller dimension \\(\\mathbb{R}^k\\), we are necessarily losing information. However, much of this information isn’t useful. With images, for example, most \\(d\\)-dimensional vectors are not natural images that we would recognize. Instead, there is a manifold of \\(d\\)-dimensional natural images. Learning this manifold directly is challenging, and autoencoders provide a powerful tool for mapping to a lower-dimensional space and back.\n[image here of manifold]\nOnce we represent the data in the latent space, we can use it for various tasks such as classification, clustering, and generation. Instead of working in the pixel-space, for example, we can work in a lower dimension that captures the essential features of the data. Since all of our machine learning algorithms run in time dependent on the dimensionality of the input space, reducing the dimensionality can lead to significant speedups. Such latent representations form the backbone of state-of-the-art models in various domains, including computer vision, natural language processing, and speech recognition.\n\n\nVariational Autoencoders\nThe natural manifold of our data in \\(\\mathbb{R}^d\\) is generally complicated and non-convex. For example, we may have two images of the same cat from different angles, but the linear combination of these two images does not lie on the manifold of cat images.\n[image here of stripes from two angles, and the linear combination]\nWhen we build the autoencoder, we have a chance to make the latent space more structured. One method is called Variational Autoencoders, which impose a probabilistic structure on the latent space. Instead of the latent \\(\\boldsymbol{z} = f_0(\\boldsymbol{x})\\) being a deterministic function of the input, we model it as a distribution. Because of the many elegant properties of Gaussian distributions, we often choose to model the latent space as a multivariate Gaussian distribution. Then, the latent is drawn from a Gaussian distribution i.e., \\(\\boldsymbol{z} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_\\boldsymbol{x}, \\boldsymbol{\\Sigma}_\\boldsymbol{x})\\) is a random variable, where \\(\\boldsymbol{\\mu}_\\boldsymbol{x}\\) is the input-dependent mean and \\(\\boldsymbol{\\Sigma}_\\boldsymbol{x}\\) is the input-dependent covariance.\nIn variational autoencoders, the loss consists of the normal reconstruction error and a measure of the distance between the learned distribution and a well-behaved prior distribution, typically the standard Gaussian \\(\\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{I})\\): \\[\n\\mathcal{L}_\\text{VAE} = \\alpha \\mathcal{L}_{\\text{recon}} + (1-\\alpha) D_{\\text{KL}}(\\mathcal{N}(\\boldsymbol{\\mu}_\\boldsymbol{x}, \\boldsymbol{\\Sigma}_\\boldsymbol{x}) || \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{I})),\n\\] where \\(\\alpha \\in (0,1)\\) is a hyperparameter that balances the two terms, and \\(D_{KL}\\) is the Kullback-Leibler divergence. Formally, \\[\nD_{\\text{KL}}(P, Q) =\n\\mathbb{E}_{\\boldsymbol{z} \\sim P} \\left[ \\log \\frac{P(\\boldsymbol{z})}{Q(\\boldsymbol{z})} \\right],\n\\] where \\(P\\) is the learned distribution and \\(Q\\) is the prior distribution. The two losses will push the variational autoencoder in different directions: The reconstruction loss encourages the model to generate realistic samples, while the KL divergence encourages the latent space to always resemble the standard Gaussian distribution with mean \\(\\boldsymbol{0}\\) and covariance \\(\\boldsymbol{I}\\). These two goals are naturally at odds: if the encoder always outputted the mean \\(\\boldsymbol{\\mu}_\\boldsymbol{x}=\\boldsymbol{0}\\), and the covariance \\(\\boldsymbol{\\Sigma}_\\boldsymbol{x}=\\boldsymbol{I}\\), the KL divergence would be 0 but there would be no information for the decoder to reconstruct the input. However, by simultaneously training to achieve both goals, we can learn a meaningful latent space that captures the essential features of the data while also being structured.\n[image here of autoencoder vs variational autoencoder]\nWe will use gradient descent to train the parameters of the encoder \\(f_0\\) and the decoder \\(f_1\\). However, when we use KL divergence as the loss, it’s not immediately clear how to backpropagate through a random sample \\(\\boldsymbol{z} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_\\boldsymbol{x}, \\boldsymbol{\\Sigma}_\\boldsymbol{x})\\). Instead, we will use the encoder to generate \\(\\boldsymbol{\\mu}_\\boldsymbol{x}\\) and \\(\\boldsymbol{\\Sigma}_\\boldsymbol{x}\\), and set \\(\\boldsymbol{z} = \\boldsymbol{\\mu}_\\boldsymbol{x} + \\boldsymbol{\\Sigma}_\\boldsymbol{x}^{1/2} \\boldsymbol{\\epsilon}\\), where \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{I})\\) is a standard normal variable.\n\n\nPrincipal Component Analysis\nPrincipal component analysis (PCA) is the “linear regression” of unsupervised learning, offering a simple but powerful technique for dimensional reduction. PCA is the simplest kind of autoencoder, where the encoder and decoder are each a fully connected linear layer without activation functions.\nLet \\(\\mathbf{W}_0 \\in \\mathbb{R}^{k \\times d}\\) be the encoder weight matrix, and \\(\\mathbf{W}_1 \\in \\mathbb{R}^{d \\times k}\\) be the decoder weight matrix. The latent for input \\(\\mathbf{x}\\) is \\(\\mathbf{z}^\\top = \\mathbf{x}^\\top \\mathbf{W}_0\\), and the reconstruction is \\(\\tilde{\\mathbf{x}}^\\top = \\mathbf{z}^\\top \\mathbf{W}_1  = \\mathbf{x}^\\top \\mathbf{W}_0 \\mathbf{W}_1\\).\n[image here of single input]\nWe train the encoder and decoder on \\(n\\) (unlabelled) data points \\(\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(n)}\\). Let \\(\\mathbf{X} \\in \\mathbf{R}^{n \\times d}\\) be the matrix of input data, where the \\(i\\)th row corresponds to data point \\({\\mathbf{x}^{(i)}}^\\top\\).\n[image here of all inputs]\nThe goal is for the input matrix \\(\\mathbf{X}\\) to be well approximated by the reconstruction \\(\\tilde{\\mathbf{X}} = \\mathbf{X} \\mathbf{W}_0 \\mathbf{W}_1\\). In particular, \\[\n\\mathcal{L}( \\mathbf{W}_0, \\mathbf{W}_1)\n= \\sum_{i=1}^n \\| \\mathbf{x}^{(i)} - \\tilde{\\mathbf{x}}^{(i)} \\|^2\n= \\sum_{i=1}^n \\sum_{j=1}^d (x_j^{(i)} - \\tilde{x}_j^{(i)})^2\n= \\| \\mathbf{X} - \\tilde{\\mathbf{X}} \\|_\\text{F}^2\n\\] where the Frobenius norm \\(\\| \\mathbf{A} \\|_\\text{F}\\) is the sum of squared entries of \\(\\mathbf{A}\\), and the last equality follows because \\([\\mathbf{X}]_{i,j} = x_j^{(i)}\\) is the \\(j\\)th feature of the \\(i\\)th data point.\nWe could apply gradient descent to minimize this loss with respect to the weights \\(\\mathbf{W}_0\\) and \\(\\mathbf{W}_1\\). But, like with linear regression, we can actually derive the optimal solution. Let’s revisit some linear algebra to see how we can do this derivation.\nWhile only some matrices have eigendecompositions, all matrices have singular value decompositions (SVDs). In particular, let \\(r\\) be the rank of \\(\\mathbf{X}\\), then we can write \\[\n\\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top\n\\] where \\(\\mathbf{U} \\in \\mathbb{R}^{n \\times r}\\) and \\(\\mathbf{V} \\in \\mathbb{R}^{d \\times r}\\) have orthonormal columns, and \\(\\mathbf{\\Sigma} \\in \\mathbb{R}^{r \\times r}\\) is a diagonal matrix with non-negative entries. Because the columns of \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are orthonormal, we have \\[\n\\mathbf{U}^\\top \\mathbf{U} = \\mathbf{I}_r = \\mathbf{V}^\\top \\mathbf{V}.\n\\] Using our outerproduct perspective on matrix multiplication, we can write \\[\n\\mathbf{X} = \\sum_{i=1}^r \\mathbf{u}_i \\sigma_i \\mathbf{v}_i^\\top\n\\] where \\(\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_r \\geq 0\\) are the singular values of \\(\\mathbf{X}\\), and \\(\\mathbf{u}_i \\in \\mathbb{R}^{n \\times 1}\\) and \\(\\mathbf{v}_i \\in \\mathbb{R}^{d \\times 1}\\) are the left and right singular vectors, respectively. Let \\(\\mathbf{X}_k\\) be the rank-\\(k\\) approximation \\[\n\\mathbf{X}_k = \\sum_{i=1}^k \\mathbf{u}_i \\sigma_i \\mathbf{v}_i^\\top.\n\\] With this notation, we are ready to state a useful linear algebra result.\nEckart-Young-Mirsky Theorem: The best rank-\\(k\\) approximation of a matrix \\(\\mathbf{X}\\) in the Frobenius norm is given by its top \\(k\\) singular values and corresponding singular vectors i.e., \\[\n\\mathbf{X}_k = \\arg \\min_{\\text{rank}-$k$ \\tilde{\\mathbf{X}}} \\| \\mathbf{X} - \\tilde{\\mathbf{X}} \\|_\\text{F}^2.\n\\]\n\n\n\nProof of Eckart-Young-Mirsky Theorem\n\nWe’ll prove the theorem using several properties of the trace. For a square matrix \\(\\mathbf{M}\\), the trace \\(\\text{tr}(\\mathbf{M})\\) is the sum of the diagonal entries, and it has the following three properties:\nFirst, \\(\\| \\mathbf{X} \\|_\\text{F}^2 = \\text{tr}(\\mathbf{X}^\\top \\mathbf{X})\\) for any \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\). To see why, observe that \\[\\| \\mathbf{X} \\|_\\text{F}^2 = \\sum_{i=1}^n \\sum_{j=1}^d [\\mathbf{X}]_{i,j}^2 = \\sum_{i=1}^n \\| [\\mathbf{X}]_{i,} \\|_2^2 = \\text{tr}(\\mathbf{X}^\\top \\mathbf{X}).\\]\nSecond, \\(\\text{tr}(\\mathbf{A} \\mathbf{B}) = \\text{tr}(\\mathbf{B} \\mathbf{A})\\) for any matrices \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times d}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{d \\times n}\\). To see why, observe that \\[\n\\text{tr}(\\mathbf{A} \\mathbf{B}) = \\sum_{i=1}^n [\\mathbf{A B}]_{i,i}\n= \\sum_{i=1}^n \\sum_{j=1}^d [\\mathbf{A}]_{i,j} [\\mathbf{B}]_{j,i}\n= \\sum_{j=1}^d \\sum_{i=1}^n [\\mathbf{B}]_{j,i} [\\mathbf{A}]_{i,j}\n= \\sum_{j=1}^d [\\mathbf{B A }]_{j,j}\n= \\text{tr}(\\mathbf{B} \\mathbf{A}).\n\\]\nThird, \\(\\| \\mathbf{XV} \\|_\\text{F}^2 = \\| \\mathbf{X} \\|_\\text{F}^2\\), where the columns of \\(\\mathbf{V} \\in \\mathbb{R}^{d \\times r}\\) are orthonormal. (The analogous result holds for multiplying \\(\\mathbf{X}\\) on the left by \\(\\mathbf{U}^\\top\\), where the columns of \\(\\mathbf{U} \\in \\mathbb{R}^{n \\times r}\\) are orthonormal.) To see why, observe that \\[\n\\| \\mathbf{XV} \\|_\\text{F}^2\n= \\text{tr}((\\mathbf{XV})^\\top (\\mathbf{XV}))\n= \\text{tr}(\\mathbf{V}^\\top \\mathbf{X}^\\top \\mathbf{X} \\mathbf{V})\n= \\text{tr}(\\mathbf{X}^\\top \\mathbf{X} \\mathbf{V} \\mathbf{V}^\\top)\n= \\text{tr}(\\mathbf{X}^\\top \\mathbf{X})\n= \\| \\mathbf{X} \\|_\\text{F}^2,\n\\] where we used the first property in the first and last equality, the second property in the third equality, and the orthonormality of the columns of \\(\\mathbf{V}\\) in the fourth equality.\nWith these properties, we are finally ready to prove the theorem. We have \\[\n\\begin{align}\n\\| \\mathbf{X} - \\tilde{\\mathbf{X}} \\|_\\text{F}^2\n= \\| \\mathbf{U \\Sigma V}^\\top - \\tilde{\\mathbf{X}} \\|_\\text{F}^2\n= \\| \\mathbf{U}^\\top \\mathbf{U \\Sigma V}^\\top \\mathbf{V} - \\mathbf{U}^\\top \\tilde{\\mathbf{X}} \\mathbf{V} \\|_\\text{F}^2\n= \\| \\mathbf{\\Sigma} - \\mathbf{C} \\|_\\text{F}^2,\n\\end{align}\n\\] where we use \\(\\mathbf{C} = \\mathbf{U}^\\top \\tilde{\\mathbf{X}} \\mathbf{V}\\) for notational convenience. Continuing, \\[\n\\| \\mathbf{\\Sigma} - \\mathbf{C} \\|_\\text{F}^2\n= \\sum_{i=1}^r (\\sigma_i - [\\mathbf{C}]_{i,i})^2\n+ \\sum_{i\\neq j} [\\mathbf{C}]_{i,j}^2.\n\\] When we choose \\(\\mathbf{C}\\), we can minimize this expression by setting \\([\\mathbf{C}]_{i,j} = 0\\) for all \\(i \\neq j\\). This means that the optimal \\(\\mathbf{C}\\) is diagonal, and we can choose up to \\(k\\) non-zero entries. Setting \\([\\mathbf{C}]_{i,i} = \\sigma_i\\) for \\(i \\leq k\\) and \\(0\\) otherwise gives us the best rank-\\(k\\) approximation. If we chose some \\(i' &gt; k\\) instead of \\(i \\leq k\\), we would be choosing a smaller singular value, which would increase the overall error. With the optimal choice \\(\\mathbf{C} = \\boldsymbol{\\Sigma}_k\\), we have \\(\\tilde{\\mathbf{X}} = \\mathbf{U} \\boldsymbol{\\Sigma}_k \\mathbf{V}^\\top = \\mathbf{X}_k\\), and error \\[\n\\| \\mathbf{X} - \\tilde{\\mathbf{X}} \\|_\\text{F}^2\n= \\sum_{i=k+1}^r \\sigma_i^2.\n\\]\n\n\nThe Eckart-Young-Mirsky theorem tells us that the best rank-\\(k\\) approximation of a matrix is given by its top \\(k\\) singular values and corresponding singular vectors. So we’d like to choose \\(\\mathbf{W}_0\\) and \\(\\mathbf{W}_1\\) so that \\(\\tilde{\\mathbf{X}} = \\mathbf{X} \\mathbf{W}_0 \\mathbf{W}_1^\\top\\). Setting \\(\\mathbf{W}_0 = \\mathbf{V}_k\\) and \\(\\mathbf{W}_1 = \\mathbf{V}_k^\\top\\) gives \\[\n\\tilde{\\mathbf{X}}\n= \\mathbf{X} \\mathbf{V}_k \\mathbf{V}_k^\\top\n= \\sum_{i=1}^r \\mathbf{u}_i \\sigma_i  \\mathbf{v}_i^\\top \\sum_{j=1}^k \\mathbf{v}_j \\mathbf{v}_j^\\top\n= \\sum_{i=1}^k \\mathbf{u}_i \\sigma_i \\mathbf{v}_i^\\top = \\mathbf{X}_k,\n\\] where the second equality follows because \\(\\mathbf{v}_i^\\top \\mathbf{v}_j=1\\) if \\(i=j\\) and \\(0\\) otherwise.\nWe call \\(\\mathbf{X} \\mathbf{V}_k = \\mathbf{U}_k \\boldsymbol{\\Sigma}_k\\) the latent representation, and the columns of \\(\\mathbf{V}_k\\) are the principal components. When we perform PCA, we are effectively running a singular value decomposition on \\(\\mathbf{X}\\). In practice, we generally mean-center and normalize \\(\\mathbf{X}\\) first; in particular, we subtract the mean from each column, and divide each column by its \\(\\ell_2\\)-norm.\nWe could compute the singular value decomposition directly using e.g., the library: Returning the full decomposition requires \\(O(nd^2)\\) time, while computing a rank-\\(k\\) approximation can be done in \\(O(ndk)\\) time. We could have also computed the eigendecomposition of \\(\\mathbf{X}^\\top \\mathbf{X} = \\mathbf{V} \\boldsymbol{\\Sigma}^2 \\mathbf{V}^\\top\\), and \\(\\mathbf{X} \\mathbf{X}^\\top = \\mathbf{U} \\boldsymbol{\\Sigma}^2 \\mathbf{U}^\\top\\). But, this approach is generally less efficient since it requires computing the full eigendecomposition of a \\(d \\times d\\) matrix and a \\(n\\times n\\), which is roughly \\(O(d^3 + n^3)\\).\nIn practice, PCA gives us a low-dimensional representation of the data that captures the most important variance.\n[image here of full rank vs rank 1, etc]\nAs we increase the rank of the approximation, the reconstruction error decreases. Recall from the proof of the Eckart-Young-Mirsky theorem that the error of the rank-\\(k\\) approximation is given by \\[\n\\sum_{i=k+1}^r \\sigma_i^2.\n\\] When the singular values are all similar, the reconstruction error decreases more slowly as we add more components. However, if the singular values decay quickly, we can achieve a significant reduction in error by adding just a few components.\n[image here of singular values vs i]\nThe reconstruction error is also smaller when the rank is lower. So if we have columns that are linear related, then the error tends to be lower. For example, in a housing dataset, maybe we have features like square footage, lot size, and yard size. Since these features are all related, we can capture their relationships more easily with fewer dimensions. Other kinds of data with low-rank structure include image data, where pixels in a small region are often correlated, and text data, where word usage patterns can be captured with a small number of topics.\nConsider an individual data point \\(\\mathbf{x}\\). The latent vector \\(\\mathbf{z}\\) is obtained by projecting \\(\\mathbf{x}\\) onto the principal components: \\[\n\\mathbf{z} = \\sum_{i=1}^k \\langle \\mathbf{x}, \\mathbf{v}_i \\rangle.\n\\] Then the reconstruction is given by \\[\n\\tilde{\\mathbf{x}} = \\sum_{i=1}^k z_i \\mathbf{v}_i = \\sum_{i=1}^k \\langle \\mathbf{x}, \\mathbf{v}_i \\rangle \\mathbf{v}_i.\n\\]\n[image here of directions of data, and projection into lower dimensional space]\nThe magnitude of the reconstruction is the same as the magnitude of the latent representation. To see why, observe that \\[\n\\| \\tilde{\\mathbf{x}} \\|_2^2\n= \\left \\| \\sum_{i=1}^k \\langle \\mathbf{x}, \\mathbf{v}_i \\rangle \\mathbf{v}_i \\right \\|_2^2\n= \\langle \\sum_{i=1}^k \\langle \\mathbf{x}, \\mathbf{v}_i \\rangle \\mathbf{v}_i, \\sum_{j=1}^k \\langle \\mathbf{x}, \\mathbf{v}_j \\rangle \\mathbf{v}_j \\rangle\n= \\sum_{i=1}^k \\langle \\mathbf{x}, \\mathbf{v}_i \\rangle^2\n= \\sum_{i=1}^k z_i^2 = \\| \\mathbf{z} \\|_2^2.\n\\] Similarly, the distance between reconstructed points \\(\\tilde{\\mathbf{x}}^{(i)}\\) and \\(\\tilde{\\mathbf{x}}^{(j)}\\) is the same as the distance between the latent representations \\(\\mathbf{z}^{(i)}\\) and \\(\\mathbf{z}^{(j)}\\). Do you see why?\nIf the original data is close to the reconstructed data, then \\(\\| \\tilde{\\mathbf{x}} \\|_2 \\approx \\| \\mathbf{x} \\|_2\\), and the latent representation \\(\\| \\mathbf{z} \\|_2\\) will also be similar. Put another way, the latent representations effectively capture the behavior of the original data. Next, we’ll see how PCA can effectively represent language data.\n\n\nSemantic Embeddings\nWhen we discussed transformers, we assumed we could meaningfully represent words as vectors in a continuous space. This idea is at the heart of semantic embeddings, where words with similar meanings are mapped to nearby latent representations.\nOur first attempt will be with a document-word matrix \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\). Each row of \\(\\mathbf{X}\\) corresponds to a document, and each column corresponds to a word. The entry \\(X_{ij}\\) represents whether word \\(j\\) appears in document \\(i\\). When we apply PCA to this matrix, we can obtain a low-dimensional representation of the documents, and of the words!\n[image here of document-word matrix]\nAmong other applications, the latent document representations are useful for efficient search. By representing documents in a lower-dimensional space, we can quickly find similar documents using techniques like nearest neighbor search.\nLet \\(\\mathbf{z}\\) be the latent representation of a document \\(\\mathbf{x}\\). Consider the principal components \\(\\mathbf{v}_i\\) and \\(\\mathbf{v}_j\\) associated with words \\(i\\) and \\(j\\). If the reconstruction \\(\\tilde{\\mathbf{X}}\\) is close to the original \\(\\mathbf{X}\\), then \\(\\langle \\mathbf{z}, \\mathbf{v}_i \\rangle \\approx 1\\) if word \\(i\\) appears in the document, and \\(\\langle \\mathbf{z}, \\mathbf{v}_j \\rangle \\approx 1\\) if word \\(j\\) appears. In this case, \\(\\langle \\mathbf{v}_i, \\mathbf{v}_j \\rangle\\) will likely be large. In this way, the principal components can capture the meaning of words.\n[image here of latent representations of words in various directions]\nFor our document-word matrix, we defined the document representations as \\(\\mathbf{X} \\mathbf{V}_k = \\mathbf{U}_k \\mathbf{\\Sigma}_k\\) and the word representations as \\(\\mathbf{V}_k^\\top\\). But, we could have just as easily defined the document representations as \\(\\mathbf{U}_k\\) and the word representations as \\(\\mathbf{\\Sigma}_k \\mathbf{V}_k^\\top\\). With this definition, we can write the outer product between the word representations and itself as: \\[\n\\left( \\mathbf{\\Sigma}_k \\mathbf{V}_k^\\top\\right)^\\top \\left( \\mathbf{\\Sigma}_k \\mathbf{V}_k^\\top \\right)\n= \\mathbf{V}_k \\mathbf{\\Sigma}_k^\\top \\mathbf{\\Sigma}_k \\mathbf{V}_k^\\top\n= \\tilde{\\mathbf{X}}^\\top \\tilde{\\mathbf{X}}.\n\\] When \\(\\tilde{\\mathbf{X}}\\) is close to \\(\\mathbf{X}\\), the inner product between word vectors \\(\\mathbf{y}_i\\) and \\(\\mathbf{y}_j\\) can be approximated as: \\[\n\\langle \\mathbf{y}_i, \\mathbf{y}_j \\rangle \\approx [\\mathbf{X}^\\top \\mathbf{X}]_{i,j}\n= \\text{\\# documents with both $i$ and $j$}\n\\]\nMore generally, we can compress a variety of language data beyond document and word matrices. As a general recipe, we can compute the similarity between all word pairs \\(i\\) and \\(j\\), and store the results in a matrix \\(\\mathbf{M} \\in \\mathbb{R}^{d \\times d}\\). We find a low-rank approximation $ ^ for some \\(\\mathbf{Y} \\in \\mathbb{R}^{d \\times k}\\). Then, we can define the word representations as the rows of \\(\\mathbf{Y}\\).\nFor example, one approach is to define similarity as the number of times a word appears in the same context as another word. This can be captured by counting co-occurrences in a sliding window over a corpus of text.\n[window size and matrix visualization]\nWe then could process the co-occurrence counts with non-linearities to account for Zipf’s law. In the popular word2vec algorithm, for example, the similarity is given by \\[\n[\\mathbf{M}]_{i,j} = \\log \\frac{\\text{\\# contexts with both $i$ and $j$}}{\\text{\\# contexts with $i$}{\\text{\\# contexts with $j$}}}.\n\\]\nNote: If \\(\\mathbf{M}\\) is not symmetric, then the factorization may be of the form \\(\\mathbf{M} \\approx \\mathbf{W}^\\top \\mathbf{Y}\\), where \\(\\mathbf{W} \\neq \\mathbf{Y}\\). In this case, we can take the rows of either \\(\\mathbf{W}\\) or \\(\\mathbf{Y}\\) as the word representations.\nThere are many interesting applications of latent representations.\n\nWord Math\nWhen the reconstructed data is close to the original, we saw how latent representations can effectively capture the behavior of the original data. One interesting corollary is that we can do “word math” like \\[\n\\mathbf{y}_\\text{dog} - \\mathbf{y}_\\text{old} + \\mathbf{y}_\\text{young} \\approx \\mathbf{y}_\\text{puppy}.\n\\]\n[image here of word math]\n\n\nUnsupervised Translation\nUsing our co-occurrence strategy, we can turn documents or even transcribed conversations into language data \\(\\mathbf{X}\\), and then semantic embeddings \\(\\mathbf{Y}\\) without any supervision. In fact, we can construct these embeddings for multiple languages in parallel. If the languages have similar structures (e.g., family relationships), we can leverage this to rotate one language’s embeddings into another’s space.\n[image here of aligning point clouds]\nWhile not perfect, the aligned vectors can give a translation strategy without any supervision. Some researchers are even applying these strategies to map the sounds of monkeys or whales to human language!\n\n\nGraph Representations\nThe applications of autoencoders go far beyond language. One particularly versatile data structure is a graph, where nodes represent entities and edges represent relationships. For example, we can use a graph to represent social networks (people are nodes and friendships are edges), road infrastructure (intersections are nodes and roads are edges), or even knowledge graphs (concepts are nodes and relationships are edges). Even when the underlying graph is massive, we can use a random walk through the graph and apply our co-occurrence strategy to learn embeddings for the nodes.\n[image here of random walk and resulting matrix]\n\n\nMulti-modal Contrastive Learning\nA particularly interesting feature of modern machine learning is the connection between different modalities of data e.g., generating images from text descriptions or vice versa. The method for achieving this connection is known as contrastive learning, and can be viewed as yet another application of autoencoders.\nConsider a dataset of captioned images where each image is paired with a descriptive caption. We can use neural networks (e.g., a convolutional network for the image, and a transformer for the text) to map the data to latent representations. Then we represent the similarity of the image and text embeddings in a similarity matrix \\(\\mathbf{M} = \\mathbf{I}\\), where pairs of related images and captions that match have a value of 1, and all other pairs have a value of 0. The resulting embeddings, trained with updates to the networks so that \\(\\mathbf{M} \\approx \\mathbf{W}^\\top \\mathbf{Y}\\), can then be used for various tasks such as image captioning or text-to-image generation.\n[image here of contrastive learning and multi-modal embeddings]\nWe can then use the embeddings in downstream techniques like diffusion to generate images from text descriptions."
  },
  {
    "objectID": "notes/06_NeuralNetworks.html",
    "href": "notes/06_NeuralNetworks.html",
    "title": "Neural Networks",
    "section": "",
    "text": "Previously, we have built machine learned models in two steps: First, we chose a set of features to represent the data, possibly transforming the data in the process. Second, we optimized a model on the (transformed) features. Neural networks allow us to learn both the feature transformations and the model simultaneously.\n\nNeural Network Architectures\nConsider training data that is not linearly separable, i.e., we cannot draw a straight line to separate the classes. We saw before how we could represent the data in a higher-dimensional space, where it is linearly separable. The alternative we’ll explore today is to divide the region with multiple linear boundaries.\n[image here of non linearly separable data, next to a divided region]\nWe can learn four linear boundaries, each of bounds the region in a different direction.\n[four images here of linear boundaries]\nWhen we pass the input through the learned lines, we will get real numbers representing the distance from the line e.g., \\(\\langle \\mathbf{x}, \\mathbf{w}^{(i)} \\rangle\\). Let’s apply a non-linear activation function to these distances, which will distinguish whether the distance is positive or negative e.g., \\(\\sigma(\\langle \\mathbf{x}, \\mathbf{w}^{(i)} \\rangle)\\), where \\(\\sigma(z) = \\mathbf{1}[z \\geq 0]\\). For example, the point marked by the star would represented as \\([0, 1, 1, 0]\\), because it is above the first line, below the second line, below the third line, and above the fourth line. Finally, we can combine these values to make a prediction, e.g., by taking a weighted average.\n[image here of example neural network]\nIn this way, we can learn a non-linear functions by combining multiple linear models with non-linear activations. By updating the weights of the linear models and the weights of the final combination, we can learn a complex function that fits the data well.\nNeural networks are remarkably flexible, with many different architectural options:\n\nThe activation functions which process the outputs of the linear models. Options include the ReLU function \\(\\sigma(z) = \\max(0, z)\\), the sigmoid function \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\), the hyperbolic tangent function \\(\\sigma(z) = \\tanh(z)\\), and more.\nThe number of layers in the network, i.e., how many times we apply the linear models and activation functions.\nThe number of neurons in each layer, i.e., how many linear models we apply in each layer.\nThe loss function to measure the error between the predicted and true labels. We can use the same loss functions we have seen before: cross-entropy for classification and squared error for regression.\n\nTensorflow offers an excellent visualization of common neural network architectures, which you can play around with here.\nBeyond these options, there are several types of neural network architectures that connect neurons in different ways: Convolutional layers are particularly useful for locally correlated data, such as images, where we can learn filters that capture local patterns. Transformers are particularly useful for sequential data, such as text, and form the backbone of modern foundation models. We will not cover these architectures in our course, but I highly recommend Chinmay Hegde’s convolutional network notes and transformer notes if you would like to learn more.\nWhen we train a neural network, we apply gradient descent to minimize the loss function, just like we have done for linear regression and logistic regression. But we need to be careful about how we compute the gradients, since the neural network is a composition of multiple functions, and there can be many parameters.\n\n\nBackpropagation\nThe key step in training a neural network is to compute the gradients of the loss function with respect each of the parameters in the network. Backpropagation allows us to compute these gradients efficiently. The basic idea is to modularly apply the chain rule from calculus to compute the gradients of each layer, starting from the output layer and working backwards to the input layer.\nBefore we dive into the details, let’s review the chain rule. For a scalar function \\(f: \\mathbb{R} \\to \\mathbb{R}\\), we write the derivative as \\[\n\\frac{df}{dx} = \\lim_{t \\to 0} \\frac{f(x + t) - f(x)}{t}.\n\\]\nFor a multivariate function \\(f: \\mathbb{R}^m \\to \\mathbb{R}\\), we write the partial derivative as \\[\n\\frac{\\partial f}{\\partial x_i} = \\lim_{t \\to 0} \\frac{f(x_1, \\ldots, x_i + t, \\ldots, x_d) - f(x_1, \\ldots, x_i, \\ldots, x_d)}{t}.\n\\]\nConsider two functions \\(f: \\mathbb{R} \\to \\mathbb{R}\\) be a function, and \\(y: \\mathbb{R} \\to \\mathbb{R}\\). The chain rule tells us that the derivative of the composition \\(f(y(x))\\) is given by \\(\\frac{df}{dx} = \\frac{df}{dy} \\cdot \\frac{dy}{dx}\\). To see this, we can write \\[\n\\frac{df}{dx}\n= \\lim_{t \\to 0} \\frac{f(y(x + t)) - f(y(x))}{t}\n= \\lim_{t \\to 0} \\frac{f(y(x + t)) - f(y(x))}{y(x + t) - y(x)} \\frac{y(x + t) - y(x)}{t}.\n= \\frac{df}{dy} \\cdot \\frac{dy}{dx},\n\\] where the last equality follows as long as \\(\\lim_{t \\to 0} y(x + t) - y(x) = 0\\).\nLet \\(f: \\mathbb{R}^m \\to \\mathbb{R}\\) be a function, and \\(y_i: \\mathbb{R} \\to \\mathbb{R}\\) be a sequence of \\(m\\) functions. The multivariate chain rule tells us that the derivative of the composition \\(f(y_1(x), \\ldots, y_m(x))\\) is given by \\[\n\\frac{df}{dx} = \\left(\n  \\frac{df}{dy_1} \\cdot \\frac{dy_1}{dx} + \\ldots + \\frac{df}{dy_m} \\cdot \\frac{dy_m}{dx}\n\\right).\n\\]\nBackpropagation is simply an application of the multivariate chain rule, and is the natural analog to the forward pass in a neural network.\n[image here of forward pass and image here of backward pass]\nDuring the forward pass, we successively compute the outputs, and feed them into the next layer. Consider one of the functions in the neural network: The function \\(v : \\mathbb{R}^{d_\\ell} \\to \\mathbb{R}\\) maps the input(s) parameters \\(u_1, \\ldots, u_d\\) to an output, which itself is passed to another function. (In general, \\(d\\) could be 1, e.g., if the \\(v\\) were an activation function.) When we compute \\(v(u_1, \\ldots, u_d)\\), we crucially only need to know the values of parameters \\(u_1, \\ldots, u_d\\), rather than any of the parameters that we used to compute \\(u_1, \\ldots, u_d\\).\nThe time complexity of the forward pass is linear in the number of times a parameter is used in a function, which itself is linear in the number of parameters.\nDuring the backward pass, we compute the derivative of the loss function with respect to the parameters in one layer, then feed these derivatives into the prior layer. Suppose the parameter \\(u\\) is used in \\(d\\) different functions \\(v_1, \\ldots, v_n\\). By the multivariate chain rule, the derivative of the loss function with respect to \\(u\\) is given by \\[\n\\frac{\\partial \\mathcal{L}}{\\partial u} = \\frac{\\partial \\mathcal{L}}{\\partial v_1} \\cdot \\frac{\\partial v_1}{\\partial u} + \\ldots + \\frac{\\partial \\mathcal{L}}{\\partial v_n} \\cdot \\frac{\\partial v_n}{\\partial u}.\n\\] When we compute this partial derivative, we crucially only need to know the partial derivatives \\(\\frac{\\partial \\mathcal{L}}{\\partial v_1}, \\ldots, \\frac{\\partial \\mathcal{L}}{\\partial v_n}\\), and the partial derivatives \\(\\frac{\\partial v_1}{\\partial u}, \\ldots, \\frac{\\partial v_n}{\\partial u}\\). By the time we reach \\(u\\) in the backward pass, the first set of partial derivatives are already known from the prior layer, and the second set of partial derivatives only depend on how \\(u\\) is used in the functions \\(v_1, \\ldots, v_n\\).\nLike the forward pass, the time complexity of the backward pass is linear in the number of times a parameter appears in a function, which itself is linear in the number of parameters.\n\n\nLinear Algebraic View of Backpropagation\nNeural networks have been studied since the the 1950s, and the backpropagation algorithm was first proposed in the 1980s. So why did it take so long for neural networks to become popular? The answer is that neural networks require many parameters to be effective, and for a long time, we did not have the computational resources to train them at sufficient scale. A breakthrough in the 2010s made neural networks practical: Originally intended for matrix multiplication in graphics, the development of Graphical Processing Units (GPUs) allowed us to efficiently implement backpropagation.\nConsider two adjacent layers of neurons: \\(\\mathbf{u} \\in \\mathbb{R}^d\\) and \\(\\mathbf{v} \\in \\mathbb{R}^n\\) in a fully connected neural network. Suppose the weight matrix between the two layers is \\(\\mathbf{W} \\in \\mathbb{R}^{n \\times d}\\). (We will ignore the activation function because it can easily be applied in parallel to each neuron.) During the forward pass, we have already computed the neuron values \\(\\mathbf{u}\\). With this information, we can compute the values in the second layer as \\[\n\\mathbf{v} = \\mathbf{W} \\mathbf{u}.\n\\] In particular, the \\(i\\)th value in the second layer is given by \\[\nv_i = \\sum_{j=1}^d [\\mathbf{W}]_{i,j} u_j.\n\\] Because the function is linear, observe that the derivatives of the second layer with respect to the first layer are simply given by \\[\n\\frac{\\partial v_i}{\\partial u_j} = [\\mathbf{W}]_{i,j}.\n\\]\nDuring the backward pass, we have already computed the gradients of the loss function with respect to the second layer \\(\\nabla_{\\mathbf{v}} \\mathcal{L}\\). With this information, we can compute the gradients of the loss function with respect to the first layer as \\[\n\\nabla_{\\mathbf{u}} \\mathcal{L} = \\mathbf{W}^\\top \\nabla_{\\mathbf{v}} \\mathcal{L}.\n\\] In particular, the gradient of the loss function with respect to the \\(j\\)th neuron in the first layer is given by \\[\n\\frac{\\partial \\mathcal{L}}{\\partial u_j}\n= \\sum_{i=1}^n \\frac{\\partial \\mathcal{L}}{\\partial v_i}\n\\frac{\\partial v_i}{\\partial u_j}\n= \\sum_{i=1}^n [\\mathbf{W}^\\top]_{j,i} \\frac{\\partial \\mathcal{L}}{\\partial v_i}.\n\\]\nNote: Instead of the gradients with respect to the neurons, we are actually interested in the gradients of the loss function with respect to the parameters, because we are updating the parameters during training. But, we need the neuron gradients to compute the parameter gradients: Once we have the neuron gradients, we compute the parameter gradients with another application of the chain rule.\nThe power of the linear algebraic view is that we can use hardware specialized for matrix multiplication to efficiently compute the gradients!\nNeural networks are a powerful tool for learning complex functions, but notoriously difficult to understand because of their many parameters and nonlinearities. The big surprise of deep learning is that, even though they are not convex functions, gradient descent works remarkably well to train them. We don’t have good intuition for what happens in high-dimensional space (the loss function depends on the number of parameters, which can be very large), but it seems like there’s almost always a parameter update that at least slightly improves the loss function."
  },
  {
    "objectID": "notes/05_SupportVectorMachines.html",
    "href": "notes/05_SupportVectorMachines.html",
    "title": "Support Vector Machines and Constrained Optimization",
    "section": "",
    "text": "We have so far explored the Naive Bayes classifier and logistic regression for solving classification tasks. In this section, we will learn about another tool called support vector machines (SVMs). While the final algorithm will be similar to logistic regression, SVMs approach the problem from a different perspective. As we’ll soon discover, both logistic regression and SVMs have their own strengths and weaknesses. Plus, we’ll get to see some fun math along the way!\n\nSupport Vector Machines\nConsider a binary classification problem with data points \\(\\mathbf{x}^{(i)}\\) and labels \\(y^{(i)} \\in \\{-1, 1\\}\\). (We previously used \\(y^{(i)} \\in \\{0, 1\\}\\), but this is just a relabeling for convenience.) We want to find a hyperplane that separates the two classes. For the majority of our discussion, we will assume that the data is linearly separable, meaning there exists a hyperplane that can perfectly separate the two classes.\n\n\n\nWhen there is such a separating hyperplane, there are often infinitely many that we can choose from. The question at the heart of support vector machines is: which hyperplane should we choose?\n\n\n\nAs we’ve seen in the past, it is not too difficult to find a model that perfectly fits our training data; the real challenge is to find a model that generalizes well to unseen data. In the context of classification, we may expect that a model that separates the two classes with the largest ‘margin of error’ will generalize better than one that is very close to the data points. With this in mind, Vladimir Vapnik and Alexey Chervonenkis proposed the idea of support vector machines, which aim to find the hyperplane that maximizes the margin between the two classes.\n\n\n\nLet’s consider a particular hyperplane, defined by the normal vector \\(\\mathbf{w}\\) and bias \\(b\\). The equation of the hyperplane is given by: \\[\n\\begin{align}\n\\langle \\mathbf{w}, \\mathbf{x} \\rangle - b = 0.\n\\end{align}\n\\]\nWe will define \\(\\mathbf{w}\\) and \\(b\\) so that \\(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle - b \\geq 1\\) for all points \\(\\mathbf{x}^{(i)}\\) in the positive class (\\(y^{(i)} = 1\\)) and \\(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle - b \\leq -1\\) for all points in the negative class (\\(y^{(i)} = -1\\)). As we can see in the figure above, such a hyperplane will always exist if the data is linearly separable. How can we find the hyperplane that maximizes the margin?\n\n\nComputing the Margin\nFor a given \\(\\mathbf{w}\\) and \\(b\\), we can see that the margin is the distance between the hyperplane \\(\\langle \\mathbf{w}, \\mathbf{x} \\rangle - b = 1\\) and the hyperplane \\(\\langle \\mathbf{w}, \\mathbf{x} \\rangle - b = -1\\). Let’s calculate this distance.\n\n\n\nConsider a point \\(\\mathbf{z}_1\\) on the hyperplane \\(\\langle \\mathbf{w}, \\mathbf{x} \\rangle - b = 1\\). Let \\(\\mathbf{z}_2\\) be the point on the hyperplane \\(\\langle \\mathbf{w}, \\mathbf{x} \\rangle - b = -1\\) that is closest to \\(\\mathbf{z}_1\\). Because \\(\\mathbf{z}_1\\) and \\(\\mathbf{z}_2\\) are the closest points to each other on the two hyperplanes, the line connecting them is perpendicular to both hyperplanes. (Otherwise, we could move along the hyperplane \\(\\langle \\mathbf{w}, \\mathbf{x} \\rangle - b = -1\\) to find a point \\(\\mathbf{z}_2'\\) that is closer to \\(\\mathbf{z}_1\\) than \\(\\mathbf{z}_2\\), contradicting our assumption that \\(\\mathbf{z}_2\\) is the closest point.) Formally, we can write: \\[\n\\mathbf{z}_1 - \\mathbf{z}_2 = \\lambda \\bar{\\mathbf{w}}\n\\] for some scaling \\(\\lambda \\in \\mathbb{R}\\), where \\(\\bar{\\mathbf{w}}\\) is the unit normal vector \\(\\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|_2}\\). We are interested in the length of this vector \\(\\lambda\\). By our observation above, we can write \\(\\mathbf{z}_1 =  \\lambda \\bar{\\mathbf{w}} + \\mathbf{z}_2\\). Then, plugging in \\(\\mathbf{z}_1\\) into the hyperplane equation \\(\\langle \\mathbf{w}, \\mathbf{z}_1 \\rangle - b = 1\\), we have: \\[\n\\begin{align}\n1 &= \\langle \\mathbf{w}, \\lambda \\bar{\\mathbf{w}} + \\mathbf{z}_2 \\rangle - b\n\\\\ &= \\langle \\mathbf{w}, \\lambda \\bar{\\mathbf{w}} \\rangle + \\langle \\mathbf{w}, \\mathbf{z}_2 \\rangle - b\n\\\\ &= \\lambda \\langle \\mathbf{w}, \\bar{\\mathbf{w}} \\rangle - 1\n\\\\ &= \\frac{\\lambda}{\\| \\mathbf{w} \\|_2} \\| \\mathbf{w} \\|_2^2 - 1.\n\\end{align}\n\\] Rearranging, we have that \\(\\lambda = \\frac2{\\| \\mathbf{w} \\|_2}\\). For a given \\(\\mathbf{w}\\) and \\(b\\), we now know how to compute the margin. Let’s now find the \\(\\mathbf{w}\\) and \\(b\\) with the largest margin.\n\n\nConstrained Optimization\nWe can formalize our goal as a constrained optimization problem: We want to find the hyperplane that maximizes the margin, subject to the constraints that all points in the positive class are on one side of the hyperplane and all points in the negative class are on the other side.\nObserve that maximizing the margin is equivalent to minimizing the inverse of the margin, which, by our calculation above, is equivalent to minimizing \\(\\frac12 \\| \\mathbf{w} \\|_2\\). Further, notice that we can simplify our constraints: \\(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle - b \\geq 1 \\text{ for } y^{(i)} = 1\\) and \\(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle - b \\leq -1 \\text{ for } y^{(i)} = -1\\) is equivalent to $ y^{(i)}(, ^{(i)} - b) $ for all \\(i\\). (This is why we relabelled the classes so that \\(y^{(i)} \\in \\{-1, 1\\}\\).) We can now write our optimization problem as follows: \\[\n\\begin{align}\n\\min_{\\mathbf{w}, b} \\frac12 \\| \\mathbf{w} \\|_2^2 \\textnormal{ such that }\ny^{(i)}(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle - b) \\geq 1 \\text{ } \\forall i.\n\\end{align}\n\\]\nThe points \\(\\mathbf{x}^{(i)}\\) that lie on the hyperplane i.e., those for which \\(y^{(i)}(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle - b) = 1\\) are called the support vectors. Let \\(\\mathcal{S}\\) be the set of indices of the support vectors.\n\n\n\nThe problem above is known as an example of a constrained optimization problem. More specifically, it is a quadratic programming problem: we have a quadratic objective function (the term \\(\\frac12 \\| \\mathbf{w} \\|_2^2\\) is quadratic in \\(\\mathbf{w}\\)) and linear constraints (the constraints are linear in \\(\\mathbf{w}\\) and \\(b\\)). Constrained optimization problems in general, and quadratic programming in particular, are a rich area of study in optimization, and there are many techniques to solve them. However, the time complexity of these techniques will depend on the number of variables in \\(\\mathbf{w}\\). For data in high dimensions (e.g., the transformed data after we add features to make the classes linearly separable), the time complexity can be quite high. In the next section, we will see how to find the dual of this problem, which can allow us to solve it more efficiently.\nBefore we do, let’s see why the hyperplane is called a support vector machine.\nClaim: The optimal hyperplane \\(\\mathbf{w}^\\star\\) is a linear combination of the support vectors: \\[\n\\mathbf{w}^\\star = \\sum_{i \\in \\mathcal{S}} \\beta_i  y^{(i)} \\mathbf{x}^{(i)}\n\\] for some coefficients \\(\\beta_i \\in \\mathbb{R}\\).\n\n\n\nProof\n\nSuppose for contradiction that \\(\\mathbf{w}^\\star\\) is not a linear combination of the support vectors. That is, \\(\\mathbf{w}^\\star = \\sum_{i \\in \\mathcal{S}} \\beta_i  y^{(i)} \\mathbf{x}^{(i)} + \\mathbf{v}\\) for some vector \\(\\mathbf{v}\\) that is orthogonal to all support vectors. We will construct a new hyperplane \\(\\mathbf{w}' = \\mathbf{w}^\\star - \\epsilon \\mathbf{v}\\) for some small \\(\\epsilon &gt; 0\\), so that each point is still correctly classified, and the margin is larger than that of \\(\\mathbf{w}^\\star\\).\nLet’s check that the new hyperplane \\(\\mathbf{w}'\\) still correctly classifies all points. First, for any support vector \\(\\mathbf{x}^{(i)}\\), we have \\[\n\\langle \\mathbf{w}', \\mathbf{x}^{(i)} \\rangle = \\langle \\mathbf{w}^\\star - \\epsilon \\mathbf{v}, \\mathbf{x}^{(i)} \\rangle = \\langle \\mathbf{w}^\\star, \\mathbf{x}^{(i)} \\rangle\n\\] since \\(\\mathbf{v}\\) is orthogonal to all support vectors. By assumption, \\(\\mathbf{w}^\\star\\) satisfies \\(y^{(i)}(\\langle \\mathbf{w}^\\star, \\mathbf{x}^{(i)} \\rangle - b) = 1\\). Second, for any non-support vector \\(\\mathbf{x}^{(j)}\\), we have \\[\n\\langle \\mathbf{w}', \\mathbf{x}^{(j)} \\rangle = \\langle \\mathbf{w}^\\star - \\epsilon \\mathbf{v}, \\mathbf{x}^{(j)} \\rangle = \\langle \\mathbf{w}^\\star, \\mathbf{x}^{(j)} \\rangle - \\epsilon \\langle \\mathbf{v}, \\mathbf{x}^{(j)} \\rangle.\n\\] Since \\(\\mathbf{x}^{(j)}\\) is not a support vector, we have \\(y^{(j)} (\\langle \\mathbf{v}, \\mathbf{x}^{(j)} \\rangle - b) &gt; 1\\). Because this inequality is strict, we can choose \\(\\epsilon\\) small enough so that \\(y^{(j)}(\\langle \\mathbf{w}', \\mathbf{x}^{(j)} \\rangle - b) = y^{(j)}(\\langle \\mathbf{w}^\\star, \\mathbf{x}^{(j)} \\rangle - b - \\epsilon \\langle \\mathbf{v}, \\mathbf{x}^{(j)} \\rangle) &gt; 1\\).\nNext, let’s check that the margin of \\(\\mathbf{w}'\\) is larger than that of \\(\\mathbf{w}^\\star\\). Since \\(\\mathbf{v}\\) is orthogonal to all support vectors, we can write: \\[\n\\begin{align}\n\\| \\mathbf{w}' \\|_2^2\n&= \\| \\sum_{i \\in \\mathcal{S}} \\beta_i y^{(i)} \\mathbf{x}^{(i)} - (1-\\epsilon) \\mathbf{v} \\|_2^2 \\\\\n&= \\| \\sum_{i \\in \\mathcal{S}} \\beta_i y^{(i)} \\mathbf{x}^{(i)} \\|_2^2 + (1-\\epsilon)^2 \\| \\mathbf{v} \\|_2^2 \\\\\n&&lt; \\| \\sum_{i \\in \\mathcal{S}} \\beta_i y^{(i)} \\mathbf{x}^{(i)} \\|_2^2 + \\| \\mathbf{v} \\|_2^2 = \\| \\mathbf{w}^\\star \\|_2^2.\n\\end{align}\n\\] Since the length of the normal vector \\(\\mathbf{w}\\) is inversely proportional to the margin, we have that the margin of \\(\\mathbf{w}'\\) is larger than that of \\(\\mathbf{w}^\\star\\), a contradiction!\n\n\n\n\nThe Dual Problem\nWe have derived a quadratic programming problem, which we’ll call the primal problem:\n\\[\n\\begin{align}\n\\min_{\\mathbf{w}, b} \\frac12 \\| \\mathbf{w} \\|_2^2 \\textnormal{ such that }\ny^{(i)}(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle - b) \\geq 1 \\text{ } \\forall i \\in \\mathcal{S}.\n\\end{align}\n\\]\nIn the version above, we only include the constraints for the support vectors \\(\\mathcal{S}\\). Notice that this is sufficient because the remaining points are correctly classified by the hyperplane defined by the support vectors.\nThe challenge in directly solving the primal is that the number of variables in \\(\\mathbf{w}\\) is equal to the number of features, which can be very large especially if we apply feature transformations to the data. Fortunately, we can turn the primal into a dual problem, that, roughly speaking, converts each constraint into a variable, and each variable into a constraint. Notice that this is particularly useful because we only need the constraints for the support vectors, which could be much fewer than the total number of data points \\(n\\).\nLet’s begin with the Lagrangian of the primal problem: \\[\n\\begin{align}\n\\min_{\\mathbf{w}, b} \\max_{\\boldsymbol{\\alpha}} \\frac12 \\| \\mathbf{w} \\|_2^2 - \\sum_{i \\in \\mathcal{S}} \\alpha_i \\left( y^{(i)}(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle - b) - 1 \\right)\n= \\min_{\\mathbf{w}, b} \\max_{\\boldsymbol{\\alpha}} \\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\alpha}),\n\\end{align}\n\\] where \\(\\boldsymbol{\\alpha} \\in \\mathbb{R}^{|\\mathcal{S}|}_+\\) is a vector of non-negative real numbers. Let’s see why the Lagrangian is equivalent to the primal problem: For a particular \\(\\mathbf{w}\\) and \\(b\\), the constraints in the primal problem must be satisfied by the Lagrangian, otherwise there is some \\(i\\) such that \\(y^{(i)}(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle - b) &lt; 1\\) and we can arbitrarily increase the Lagrangian objective by increasing \\(\\alpha_i\\). Among these \\(\\mathbf{w}\\) and \\(b\\) that satisfy the constraints, the Lagrangian objective is minimized when \\(\\frac12 \\|\\mathbf{w}\\|_2^2\\) is as small as possible, which is equivalent to minimizing the objective of the primal problem.\nThe Lagrangian is useful because we can more easily reason about the optimal \\(\\mathbf{w}\\) and \\(b\\). Notice that the Lagrangian is a convex function in \\(\\mathbf{w}\\) and \\(b\\), and a concave function in \\(\\boldsymbol{\\alpha}\\). Further, as long as the data is linearly separable, we can always come up with a \\(\\mathbf{w}\\) and \\(b\\) such that the constraints are satisfied, namely with \\(\\boldsymbol{\\alpha} = 0\\). Together, by Slater’s condition, we can apply the minimax theorem to exchange the order of the minimization and maximization. That is, \\[\n\\min_{\\mathbf{w}, b} \\max_{\\boldsymbol{\\alpha}} \\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\alpha})\n= \\max_{\\boldsymbol{\\alpha}} \\min_{\\mathbf{w}, b} \\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\alpha}).\n\\]\nLet’s fix a \\(\\boldsymbol{\\alpha}\\) and minimize the Lagrangian with respect to \\(\\mathbf{w}\\) and \\(b\\). Taking the gradient with respect to \\(\\mathbf{w}\\) and \\(b\\), and setting it to zero, we have: \\[\n\\begin{align}\n\\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\alpha})\n&= \\mathbf{w} - \\sum_{i \\in \\mathcal{S}} \\alpha_i y^{(i)} \\mathbf{x}^{(i)} = 0\n\\\\\\nabla_{b} \\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\alpha})\n&= \\sum_{i \\in \\mathcal{S}} \\alpha_i y^{(i)} = 0.\n\\end{align}\n\\] Then we have \\(\\mathbf{w} = \\sum_{i \\in \\mathcal{S}} \\alpha_i y^{(i)} \\mathbf{x}^{(i)}\\). Notice that this implies the claim we made earlier: the optimal hyperplane \\(\\mathbf{w}^\\star\\) is a linear combination of the support vectors, with coefficients \\(\\alpha_i y^{(i)}\\). Further, we have that \\(\\sum_{i \\in \\mathcal{S}} \\alpha_i y^{(i)} = 0\\). We can plug these two equations into the Lagrangian to obtain \\[\n\\begin{align}\n\\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\alpha})\n&=\n\\frac12 \\left( \\sum_{i \\in \\mathcal{S}} \\alpha_i y^{(i)} \\mathbf{x}^{(i)} \\right)^2\n- \\sum_{i \\in \\mathcal{S}} \\alpha_i y^{(i)} {\\mathbf{x}^{(i)}}^\\top \\sum_{j \\in \\mathcal{S}} \\alpha_j y^{(j)} \\mathbf{x}^{(j)}\n+ b \\sum_{i \\in \\mathcal{S}} \\alpha_i y^{(i)}\n+ \\sum_{i \\in \\mathcal{S}} \\alpha_i\n\\\\&=\n- \\frac12 \\sum_{i, j \\in \\mathcal{S}} \\alpha_i \\alpha_j y^{(i)} y^{(j)} \\langle \\mathbf{x}^{(i)}, \\mathbf{x}^{(j)} \\rangle\n+ \\sum_{i \\in \\mathcal{S}} \\alpha_i.\n\\end{align}\n\\] Finally, the dual problem is given by \\[\n\\begin{align}\n\\max_{\\boldsymbol{\\alpha}} \\sum_{i \\in \\mathcal{S}} \\alpha_i - \\frac12 \\sum_{i, j \\in \\mathcal{S}} \\alpha_i \\alpha_j y^{(i)} y^{(j)} \\langle \\mathbf{x}^{(i)}, \\mathbf{x}^{(j)} \\rangle\n\\text{ such that } \\alpha_i \\geq 0 \\text{ and } \\sum_{i \\in \\mathcal{S}} \\alpha_i y^{(i)} = 0.\n\\end{align}\n\\]\n\n\nSoft Margin\nWe have so far assumed that the data is linearly separable. However, in practice, we may have noisy data or outliers that make it impossible to find a hyperplane that perfectly separates the two classes. To address this, we can modify our optimization problem to allow for some misclassification. We can do this by introducing slack variables \\(\\xi_i \\geq 0\\) for each data point, which allow us to ‘relax’ the constraints. The modified optimization problem is given by:\n\\[\n\\begin{align}\n\\min_{\\mathbf{w}, b} \\| \\mathbf{w} \\|^2 + C \\sum_{i=1}^n \\xi_i\n\\text{ such that }\ny^{(i)}(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle - b) \\geq 1 - \\xi_i \\text{ and } \\xi_i \\geq 0 \\text{ for all } i.\n\\end{align}\n\\]\n\n\n\nThe optimal choice of each slack variable satisfies \\(\\xi_i = \\max(0, 1 - y^{(i)}(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle - b))\\). The parameter \\(C &gt; 0\\) controls the trade-off between maximizing the margin and minimizing the misclassification. Notice that this is equivalent to minimizing the hinge loss, with a \\(\\ell_2\\) regularization term."
  },
  {
    "objectID": "notes/01_LinearAlgebra.html",
    "href": "notes/01_LinearAlgebra.html",
    "title": "Linear Algebra Review",
    "section": "",
    "text": "When I first heard about machine learning, I imagined a computer that was rewarded every time it gave the right answer. Maybe there were electric carrots and sticks that no one had bothered to tell me about? While I now know as little as I did then about computer hardware, I have learned that machine learning is fundamentally a mathematical process.\nLuckily, we’ve been learning about the very mathematical ideas that make machine learning work for years! We’ll review the basics of these concepts here.\n\nDerivatives\nConsider a function \\({f}: \\mathbb{R} \\to \\mathbb{R}\\). The mapping notation means that \\({f}\\) takes a single real number as input and outputs a single real number. In general, mathematicians tell us to be careful about whether we can differentiate a function. But, we’re computer scientists so we’ll risk it for the biscuit.\nLet \\(x \\in \\mathbb{R}\\) be the input to \\({f}\\). The derivative of \\({f}\\) with respect to its input \\(x\\) is mathematically denoted by \\(\\frac{\\partial}{\\partial x}[{f}(x)]\\).\nFormally, the derivative is defined as \\[\n\\frac{\\partial}{\\partial x}[{f}(x)]\n= \\lim_{h \\to 0} \\frac{{f}(x + h) - {f}(x)}{h}.\n\\] If we were to plot \\({f}\\), the derivative at a point \\(x\\) would be the slope of the tangent line to the curve at that point.\nHere are several examples of functions and their derivatives that you might remember from calculus.\n\n\n\nFunction: \\({f}(x)\\) \n\n\nDerivative: \\(\\frac{\\partial}{\\partial x}[{f}(x)]\\)\n\n\n\n\n\\[x^2\\]\n\n\n\\[2x\\]\n\n\n\n\n\\[x^a\\]\n\n\n\\[a x^{a-1}\\]\n\n\n\n\n\\[ax + b\\]\n\n\n\\[a\\]\n\n\n\n\n\\[\\ln(x)\\]\n\n\n\\[\\frac{1}{x}\\]\n\n\n\n\n\\[e^x\\]\n\n\n\\[e^x\\]\n\n\n\n\n\nChain Rule and Product Rule\nWhile working with a simple basic function is easy, we’re not always so lucky. Modern machine learning chains many many complicated functions together. Fortunately, we will think of these operations modularly.\nLet \\(g: \\mathbb{R} \\to \\mathbb{R}\\) be another function. Consider the composite function \\(g({f}(x))\\).\nBy the chain rule, the derivative of \\(g({f}(x))\\) with respect to \\(x\\) is \\[\n\\frac{\\partial }{\\partial x}[g({f}(x))]\n= \\frac{\\partial g}{\\partial x}({f}(x))\n\\frac{\\partial}{\\partial x}[{f}(x)].\n\\]\nOften, we will also multiply functions together. The product rule tells us that \\[\n\\frac{\\partial }{\\partial x}[g(x) {f}(x)]\n= g(x) \\frac{\\partial}{\\partial x}[{f}(x)]\n+ {f}(x) \\frac{\\partial}{\\partial x}[g(x)].\n\\]\n\n\nGradients\nIn machine learning, we process high-dimensional data so we are interested in functions with multivariate input. Consider \\({f}: \\mathbb{R}^d \\to \\mathbb{R}\\). The output of the function is still a real number but the input consists of \\(d\\) real numbers. We will use the vector \\(\\mathbf{x} \\in \\mathbb{R}^d\\) to represent all \\(d\\) inputs \\(x_1, \\ldots, x_d\\).\nInstead of the derivative, we will talk use the partial derivative. The partial derivative with respect to \\(x_i\\) is denoted by \\(\\frac{\\partial}{\\partial x_i}[{f}(\\mathbf{x})]\\). In effect, the partial derivative tells us how \\({f}\\) changes when we change \\(x_i\\), while keeping all other inputs fixed.\nThe gradient stores all the partial derivatives in a vector. The \\(i\\)th entry of this vector is given by the partial derivative of \\({f}\\) with respect to \\(x_i\\). In mathematical notation, \\[\n\\nabla_\\mathbf{x} {f} = \\left[\\begin{smallmatrix} \\frac{\\partial}{\\partial x_1}[{f}(\\mathbf{x})] \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_d}[{f}(\\mathbf{x})] \\\\ \\end{smallmatrix}\\right]\n\\]\nJust like the derivative in one dimension, the gradient contains information about the slope of \\({f}\\) with respect to each of the \\(d\\) dimensions in its input.\n\n\nVector and Matrix Multiplication\nVector and matrix multiplication lives at the heart of machine learning. In fact, neural networks only really started to take off when researchers realized that the Graphical Processing Unit (GPU) could be used to perform gradient updates in addition to the matrix multiplication it was designed to do for gaming.\nConsider two vectors \\(\\mathbf{u} \\in \\mathbb{R}^d\\) and \\(\\mathbf{v} \\in \\mathbb{R}^d\\). We will use \\(\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^d u_i v_i\\) to denote the inner product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). We can also write the inner product as \\(\\mathbf{u}^\\top \\mathbf{v}\\), where \\(\\mathbf{u}^\\top \\in \\mathbb{R}^{1\\times d}\\) is the transpose of \\(\\mathbf{u}\\). The \\(\\mathcal{\\ell}_2\\)-norm of \\(\\mathbf{v}\\) is given by \\(\\|\\mathbf{v}\\|_2 = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}}\\).\nConsider two matrices: \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times m}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{m \\times n}\\) where \\(d \\neq n\\). We can only multiply two matrices when their inner dimension agrees. Because the number of columns in \\(\\mathbf{A}\\) is the same as the number of rows in \\(\\mathbf{B}\\), we can compute \\(\\mathbf{AB}\\). However, because the number of columns in \\(\\mathbf{B}\\) is not the same as the number of rows in \\(\\mathbf{A}\\), the product \\(\\mathbf{BA}\\) is not defined.\nWhen we can multiply two matrices, the \\((i,j)\\) entry in \\(\\mathbf{AB}\\) is given by the inner product between the \\(i\\)th row of \\(\\mathbf{A}\\) and the \\(j\\)th column of \\(\\mathbf{B}\\). In mathematical notation, we write \\[\n[\\mathbf{AB}]_{i,j} = \\sum_{k=1}^m [\\mathbf{A}]_{i,k} [\\mathbf{B}]_{k,j}.\n\\] The resulting dimensions of the matrix product will be the number of rows in \\(\\mathbf{A}\\) by the number of columns in \\(\\mathbf{B}\\).\n\n\n\nBeyond this inner product matrix multiplication that we are used to from linear algebra, there is also an outer product way to multiply two matrices. Consider the index \\(k \\in \\{1, \\ldots, m\\}\\). Let \\([\\mathbf{A}]_{,k}\\) denote the \\(k\\)th column of \\(\\mathbf{A}\\) and \\([\\mathbf{B}]_{k,}\\) denote the \\(k\\)th row of \\(\\mathbf{B}\\). We can write the matrix product as a sum of outer products for each \\(k\\): \\[\n\\mathbf{AB} = \\sum_{k=1}^m [\\mathbf{A}]_{,k} [\\mathbf{B}]_{k,}.\n\\] By checking the \\((i,j)\\) entry of the outer product, we see that the outer product definition is equivalent to the inner product matrix multiplication. We will soon see that this outer product definition is a surprisingly useful way to think about matrix multiplication.\n\n\n\n\n\nEigenvalues and Eigenvectors\nThe primary challenge when dealing with matrices is that our favorite operations on scalar real numbers do not always work on matrices. For example, it’s not obvious how to divide one matrix by another, or how to take the square root of a matrix. Eigenvalues and eigenvectors provide a way to decompose matrices into simpler components that we can work with in more familiar ways.\nAs we may remember from linear algebra, an eigenvector of a square matrix \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times d}\\) is a vector \\(\\mathbf{v} \\in \\mathbb{R}^d\\) such that \\(\\mathbf{Av} = \\lambda \\mathbf{v}\\) for some scalar \\(\\lambda\\). Let \\(r\\) be the rank, i.e., the number of eigenvectors of \\(\\mathbf{A}\\). Then, we can write the eigenvectors as \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_r\\), with corresponding eigenvalues \\(\\lambda_1 \\geq \\ldots \\geq \\lambda_r\\). The eigenvectors are orthonormal, meaning that \\(\\mathbf{v}_i \\cdot \\mathbf{v}_j = 0\\) for \\(i \\neq j\\) and \\(\\|\\mathbf{v}_i\\|_2 = 1\\) for all \\(i\\).\nGiven these properties, we can write \\[\\mathbf{A} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^\\top\\] where \\(\\mathbf{V} = [\\mathbf{v}_1, \\ldots, \\mathbf{v}_r] \\in \\mathbb{R}^{d \\times r}\\) is the matrix of eigenvectors and \\(\\mathbf{\\Lambda} = \\text{diag}(\\lambda_1, \\ldots, \\lambda_r) \\in \\mathbb{R}^{r \\times r}\\) is the diagonal matrix of eigenvalues. This decomposition is known as the eigenvalue decomposition of \\(\\mathbf{A}\\).\n\n\n\nUsing our outer product definition of matrix multiplication, we can equivalently write the eigenvalue decomposition as \\[\\mathbf{A} = \\sum_{i=1}^r \\lambda_i \\mathbf{v}_i \\mathbf{v}_i^\\top\\] where \\(\\mathbf{v}_i \\mathbf{v}_i^\\top\\) is the outer product of the eigenvector \\(\\mathbf{v}_i\\) with itself. We can check that the eigenvalue decomposition is correct by multiplying by each eigenvector. Since the eigenvectors are orthonormal, we have \\(\\mathbf{A} \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i\\) for each \\(i\\).\nLet’s see the power of the eigenvalue decomposition in the context of matrix inversion. If we have a scalar equation \\(ax = b\\), we can simply solve for \\(x\\) by dividing both sides by \\(a\\). In effect, we are applying the inverse of \\(a\\) to \\(a\\) i.e., \\(\\frac1{a} a =1\\). The same principle applies to matrices. The \\(n \\times n\\) identity matrix generalizes the scalar identity \\(1\\). This identity matrix is denoted by \\(\\mathbf{I}_{n \\times n} \\in \\mathbb{R}^{n \\times n}\\): the on-diagonal entries \\((i,i)\\) are 1 while the off-diagonal entries \\((i,j)\\) for \\(i\\neq j\\) are 0.\nFor a matrix \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\), we use \\(\\mathbf{A}^{-1}\\) to denote its inverse. (When \\(\\mathbf{A}\\) does not have full rank, i.e., \\(r &lt; d\\), the inverse is not defined, but we can still use the eigenvalue decomposition to find a pseudo-inverse.) The inverse is defined so that \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\) where \\(\\mathbf{I}_{n \\times n}\\) is the identity matrix. In terms of our eigenvalue decomposition, the inverse of \\(\\mathbf{A}\\) is given by \\[\\mathbf{A}^{-1} = \\mathbf{V} \\mathbf{\\Lambda}^{-1} \\mathbf{V}^\\top\\] where \\(\\mathbf{\\Lambda}^{-1} = \\text{diag}(\\frac{1}{\\lambda_1}, \\ldots, \\frac{1}{\\lambda_r})\\) is the diagonal matrix of the inverses of the eigenvalues. To see this, note that: \\[\n\\mathbf{A}^{-1} \\mathbf{A} = \\left(\\mathbf{V} \\mathbf{\\Lambda}^{-1} \\mathbf{V}^\\top\\right) \\left(\\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^\\top\\right)\n= \\mathbf{V} \\mathbf{\\Lambda}^{-1} \\mathbf{\\Lambda} \\mathbf{V}^\\top\n= \\mathbf{V} \\mathbf{I}_{r \\times r} \\mathbf{V}^\\top\n= \\mathbf{V} \\mathbf{V}^\\top\n= \\mathbf{I}_{n \\times n},\n\\] where we repeatedly used the fact that \\(\\mathbf{V}^\\top \\mathbf{V} = \\mathbf{I}_{r \\times r}\\) because the eigenvectors are orthonormal.\nIn our outer product form, we can write the inverse as \\[\\mathbf{A}^{-1} = \\sum_{i=1}^r \\frac{1}{\\lambda_i} \\mathbf{v}_i \\mathbf{v}_i^\\top\\] where \\(\\mathbf{v}_i\\) are the eigenvectors of \\(\\mathbf{A}\\). Can you more easily see why \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\)?\nGoing forward, we will apply the linear algebra tools we learned today to understand machine learning algorithms. The first algorithm we will study is PageRank, which, hiding behind the scenes, simply uses the eigenvalue decomposition to find the most important pages on the internet."
  },
  {
    "objectID": "notes/03_PolynomialRegression.html",
    "href": "notes/03_PolynomialRegression.html",
    "title": "Polynomial Regression",
    "section": "",
    "text": "Introduce variables that give better fit to the data, manipulate the data to create new features, e.g., polynomial regression.\nClaim: The fit of the regression model can only be improved by adding additional features.\n(Add picture here of polynomial regression with various degrees of polynomial, e.g., linear, quadratic, cubic, etc.)\nIn picture, we can tell which gives the right fit\nHow do we do this automatically?\n\n\nTraining data versus testing data\nGeneralization error\nIf we withhold data, this gives us an unbiased estimate of the generalization error: Expectation of performance on random sample, is performance on true data set.\nCan we use data more efficiently? \\(k\\)-fold cross validation\nIn practice, we often bias this process by training, testing, updating the model (hyperparameters, architecture, training method), training, testing, updating the model, etc. The repeated use of the test data means our model depends on the test data and eventually overfits to it, even though we are not using the test data to train the model.\n\n\n\nNon-linear models are incredibly powerful models that can approximate any function, given enough data and features. However, this power comes with a cost: non-linear models can be very complex models and can easily overfit the training data. That is, they can learn to memorize, but fail to generalize to unseen data.\n(Redo this figures with polynomial regression)\n\n\n\nWhen we believe our data comes from a simpler generating process, it makes sense to use a simpler model. Even when that simpler generating process is not a linear model, we can attempt to find simpler models through regularization. Regularization is a technique that adds a penalty to the loss function to discourage the model from fitting the training data too closely.\n\\[\n\\begin{align*}\n\\frac1n \\sum_{i=1}^n (f(\\mathbf{x}^{(i)}) - y^{(i)})^2 + \\lambda \\|\\mathbf{w}\\|^2_2,\n\\end{align*}\n\\] where \\(\\lambda\\) is a hyperparameter that controls the strength of the penalty and \\(\\|\\mathbf{w}\\|_2\\) is the \\(\\ell-2\\) norm of the weights, which is the square root of the sum of the squares of the weights.\nThe idea is that we can keep the model “simple” by penalizing large weights, which would otherwise allow the model to achieve large changes in the output for small changes in the input.\n\n\n\nIn the plot, we see data generated from a quadratic function. The linear model is too simple and fails to capture the underlying relationship, while the standard neural network is too complex and overfits the training data. The regularized neural network, however, is able to capture the underlying relationship while still being simple enough to generalize to unseen data.\nHow to control the strength of the regularization \\(\\lambda\\)?\nHow do we minimize the regularized loss function for regression?\nWe can also use a regularizaiton with \\(\\ell_1\\) norm, which penalizes the absolute value of the weights. This is called Lasso regression, and it has the effect of encouraging sparsity in the model, i.e., some weights will be exactly zero. Useful when we have many features, but we believe only a few of them are actually relevant to the prediction."
  },
  {
    "objectID": "notes/03_PolynomialRegression.html#polynomial-regression",
    "href": "notes/03_PolynomialRegression.html#polynomial-regression",
    "title": "Polynomial Regression",
    "section": "",
    "text": "Introduce variables that give better fit to the data, manipulate the data to create new features, e.g., polynomial regression.\nClaim: The fit of the regression model can only be improved by adding additional features.\n(Add picture here of polynomial regression with various degrees of polynomial, e.g., linear, quadratic, cubic, etc.)\nIn picture, we can tell which gives the right fit\nHow do we do this automatically?\n\n\nTraining data versus testing data\nGeneralization error\nIf we withhold data, this gives us an unbiased estimate of the generalization error: Expectation of performance on random sample, is performance on true data set.\nCan we use data more efficiently? \\(k\\)-fold cross validation\nIn practice, we often bias this process by training, testing, updating the model (hyperparameters, architecture, training method), training, testing, updating the model, etc. The repeated use of the test data means our model depends on the test data and eventually overfits to it, even though we are not using the test data to train the model.\n\n\n\nNon-linear models are incredibly powerful models that can approximate any function, given enough data and features. However, this power comes with a cost: non-linear models can be very complex models and can easily overfit the training data. That is, they can learn to memorize, but fail to generalize to unseen data.\n(Redo this figures with polynomial regression)\n\n\n\nWhen we believe our data comes from a simpler generating process, it makes sense to use a simpler model. Even when that simpler generating process is not a linear model, we can attempt to find simpler models through regularization. Regularization is a technique that adds a penalty to the loss function to discourage the model from fitting the training data too closely.\n\\[\n\\begin{align*}\n\\frac1n \\sum_{i=1}^n (f(\\mathbf{x}^{(i)}) - y^{(i)})^2 + \\lambda \\|\\mathbf{w}\\|^2_2,\n\\end{align*}\n\\] where \\(\\lambda\\) is a hyperparameter that controls the strength of the penalty and \\(\\|\\mathbf{w}\\|_2\\) is the \\(\\ell-2\\) norm of the weights, which is the square root of the sum of the squares of the weights.\nThe idea is that we can keep the model “simple” by penalizing large weights, which would otherwise allow the model to achieve large changes in the output for small changes in the input.\n\n\n\nIn the plot, we see data generated from a quadratic function. The linear model is too simple and fails to capture the underlying relationship, while the standard neural network is too complex and overfits the training data. The regularized neural network, however, is able to capture the underlying relationship while still being simple enough to generalize to unseen data.\nHow to control the strength of the regularization \\(\\lambda\\)?\nHow do we minimize the regularized loss function for regression?\nWe can also use a regularizaiton with \\(\\ell_1\\) norm, which penalizes the absolute value of the weights. This is called Lasso regression, and it has the effect of encouraging sparsity in the model, i.e., some weights will be exactly zero. Useful when we have many features, but we believe only a few of them are actually relevant to the prediction."
  },
  {
    "objectID": "notes/03_PolynomialRegression.html#going-forward",
    "href": "notes/03_PolynomialRegression.html#going-forward",
    "title": "Polynomial Regression",
    "section": "Going Forward",
    "text": "Going Forward\nToday, we got a taste of how to use gradient descent to optimize non-linear models, particularly neural networks. This is a rich area that has seen incredible recent advancements, particularly in the context of generative AI. In fact, I teach an entire course dedicated to deep learning. In this course, however, we will instead focus on the mathematical foundations of machine learning.\nWe have so far explored supervised learning in the regression setting, where the labels are real numbers. going forward, we will consider how to handle the case where the labels are categorical, e.g., we want to classify the data into two categories such as “cat” and “dog” or “spam” and “not spam”."
  },
  {
    "objectID": "notes/code.html",
    "href": "notes/code.html",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport scienceplots\n\nplt.style.use('science')\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D"
  },
  {
    "objectID": "notes/code.html#linear-regression-figures",
    "href": "notes/code.html#linear-regression-figures",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "Linear Regression Figures",
    "text": "Linear Regression Figures\n\nnp.random.seed(0) # Seed randomness\nplt.figure(figsize=(6, 3))\n\nn = 10 # Number of observations\nw = 2 # True parameter\nX = np.random.rand(n) # x-values\ny = X.dot(w).T + np.random.normal(size=n) * .2 #y-values\n\nplt.scatter(X,y, color='black', label=r'Data: $(x^{(i)}, y^{(i)})$')\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\nxaxis = np.arange(0,1,.01)\nplt.plot(xaxis, xaxis*.5, label=r'Line: $f(x) = .5x$', color='red')\nplt.plot(xaxis, xaxis*w, label=r'Line: $f(x) = 2x$', color='green')\nplt.legend()\nplt.title(r'Linear Regression in $\\mathbb{R}^1$')\nplt.savefig('images/regression_1d.svg')\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(6, 3))\nplt.xlabel(r'$z$')\nplt.ylabel(r'$\\mathcal{L}(z)$')\nxaxis = np.arange(-1.5,1.5,.001)\nplt.plot(xaxis, xaxis**2, label=r'Squared Loss: $\\mathcal{L}(z)=z^2$', color='blue')\nplt.plot(xaxis, np.abs(xaxis), label=r'Absolute Loss: $\\mathcal{L}(z)=|z|$', color='purple', linestyle='dotted')\nplt.legend()\nplt.title(r'Squared and Absolute Losses')\nplt.savefig('images/regression_losses.svg')\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\n\n# Seed randomness\nnp.random.seed(1234)\nn = 10  # Number of observations\nw = np.array([2, .5])  # True parameter\nX = np.random.rand(n, 2)  # x-values\ny = X.dot(w).T + np.random.normal(size=n) * .1  # y-values\n\n# Create figure and 3D axis\nfig = plt.figure(figsize=(5, 5))\nax = fig.add_subplot(111, projection='3d')\n\n# Scatter plot for data points\nax.scatter(X[:, 0], X[:, 1], y, color='black', label=r'Data: $(x_1^{(i)}, x_2^{(i)}, y^{(i)})$')\n\n# Hyperplane 1: Green\nx1 = np.arange(0, 1, .01)\nx2 = np.arange(0, 1, .01)\nX1, X2 = np.meshgrid(x1, x2)\nZ = w[0] * X1 + w[1] * X2\nax.plot_surface(X1, X2, Z, alpha=.5, color='green')\n\n# Hyperplane 2: Red\nax.plot_surface(X1, X2, .5 * X1 + 0 * X2, alpha=.5, color='red')\n\n# Labels and title\nax.set_xlabel(r'$x_1$')\nax.set_ylabel(r'$x_2$')\nax.set_zlabel(r'$y$')\nax.set_title(r'Linear Regression in $\\mathbb{R}^2$')\nax.grid(False)\n\n# Manually create custom legend handles for the surfaces\nhandles = [\n    Line2D([0], [0], marker='o', color='black', markerfacecolor='black', markersize=6, label=r'Data: $(x_1^{(i)}, x_2^{(i)}, y^{(i)})$'),\n    Line2D([0], [0], color='green', lw=4, label=r'Hyperplane: $f(x) = 2x_1 + .5x_2$'),\n    Line2D([0], [0], color='red', lw=4, label=r'Hyperplane: $f(x) = .5x_1 + 0x_2$')\n]\n\n# Add legend\nplt.legend(handles=handles, loc='upper left', framealpha=1)\n\n# Save the figure\nplt.savefig('images/regression_2d.svg', bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "notes/code.html#non-linear-regression-figures",
    "href": "notes/code.html#non-linear-regression-figures",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "Non-linear Regression Figures",
    "text": "Non-linear Regression Figures\n\n## Gradient descent\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom matplotlib import cm\nimport torch\n\n%matplotlib inline\n\nclass QuadFunc:\n  def __init__(self, a, b, c, d, e):\n    self.a = a\n    self.b = b\n    self.c = c\n    self.d = d\n    self.e = e\n\n  def getParams(self, x, y):\n    if y is None:\n      y = x[1]\n      x = x[0]\n    return x,y\n\n  def __call__(self, x, y=None):\n    x,y = self.getParams(x,y)\n    return 0.5 * (self.a*x**2 + self.b*y**2) + self.c * x * y + self.d * x + self.e * y\n\n  def grad(self, x, y=None):\n    #df/dx = ax + cy + d\n    #df/dy = by + cx + e\n    x,y = self.getParams(x,y)\n    return torch.tensor([self.a * x + self.c * y + self.d, self.b * y + self.c * x + self.e])\n\n  def hess(self, x, y=None):\n    #d2f/dx2 = a\n    #d2f/dy2 = b\n    #d2f/dxdy = c\n    #d2f/dydx = c\n    x, y = self.getParams(x,y)\n    return torch.tensor([[self.a, self.c], [self.c, self.b]])\n\nclass GradientDescent:\n    def __init__(self, lr=1, b1=0.9, b2=0.999):\n        # b1 -&gt; Momentum\n        # b2 -&gt; ADAM\n        # ADAM Paper -&gt; https://arxiv.org/abs/1412.6980\n        self.lr = lr # learning rate\n        self.b1 = b1 # grad aggregation param (for Momentum)\n        self.b2 = b2 # grad^2 aggregation param (for ADAM)\n\n        self.v = 0 # grad aggregation param\n        self.w = 0 # grad^2 aggregation param\n        self.t = 0\n\n        self.eps = 1e-9\n\n    def __call__(self, grad,hess):\n\n        self.t += 1\n\n\n        # aggregation\n        self.v = self.b1*self.v + (1-self.b1)*grad\n        self.w = self.b2*self.w + (1-self.b2)*grad**2\n\n        # bias correction\n        vcorr = self.v/(1-self.b1**self.t)\n        wcorr = self.w/(1-self.b2**self.t) if self.b2 != 0 else 1\n\n        return -1*self.lr*vcorr/(wcorr**0.5 + self.eps)\n\nclass Newtons:\n    # https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization\n    def __init__(self, lr=1):\n        self.lr = lr\n\n    def __call__(self,grad,hess):\n        return -1*self.lr*torch.matmul(torch.inverse(hess), grad)\n\ndef runOptim(init,optim,func,steps):\n\n    curpos = init # current position\n    path = [curpos]\n\n\n    for _ in range(steps):\n\n        grad = func.grad(curpos)\n        hess = func.hess(curpos)\n\n        dx = optim(grad,hess)\n        curpos = curpos + dx\n        path.append(curpos)\n\n    return path\n\n\ndef showPath(func,init,paths,labels,colors,levels):\n\n    x = torch.arange(-10,10,0.05)\n    y = torch.arange(-10,10,0.05)\n\n    # create meshgrid\n    xx, yy = torch.meshgrid(x,y)\n    zz = func(xx,yy)\n\n    # create contour\n    fig, ax = plt.subplots(1,1,figsize=(10,4))\n    cp = ax.contourf(xx,yy,zz,levels)\n    fig.colorbar(cp)\n\n    # mark initial point\n    ax.plot(init[0],init[1],'ro')\n    ax.text(init[0]+0.5,init[1],'Intial Point',color='white')\n\n    # Plot paths\n    for pnum in range(len(paths)):\n        for i in range(len(paths[pnum])-1):\n            curpos = paths[pnum][i]\n            d = paths[pnum][i+1] - curpos\n            ax.arrow(curpos[0],curpos[1],d[0],d[1],color=colors[pnum],head_width=0.2)\n            ax.text(curpos[0]+d[0],curpos[1]+d[1],str(i),color='white')\n\n    # Add legend\n    legends = []\n    for col in colors:\n        legends.append(mpatches.Patch(color=col))\n    # Put legend in top left corner\n    ax.legend(legends,labels, loc='upper left')\n\n\nplt.figure(figsize=(4, 2))\na = 1/torch.sqrt(torch.tensor(2.0))\ninit = torch.matmul(torch.tensor([[a,a],[-a,a]]),torch.tensor([-5.0,7.5]))\nell = QuadFunc(a,a,-0.8*a,a,a)\nsteps = 7\nlr = 1.5\nregGD = GradientDescent(lr,0,0) # Without Momentum\nmomGD = GradientDescent(lr,0.9,0) # Momentum\npath1 = runOptim(init,regGD,ell,steps)\npath2 = runOptim(init,momGD,ell,steps)\n# Set figure size\nshowPath(ell,init,[path1,path2],['Gradient Descent','Momentum'],['r','y'], 15)\n# Turn off axis ticks\nplt.xticks([])\nplt.yticks([])\nplt.xlabel(r'$w_1$', fontsize=14)\nplt.ylabel(r'$w_2$', fontsize=14)\nplt.savefig('images/regression_momentum.svg', bbox_inches='tight', dpi=300)\n\n/Users/rwitter/miniconda3/envs/class/lib/python3.12/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:4324.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n\n\n&lt;Figure size 400x200 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Step 1: Generate noisy linear data\nn = 10 # Number of observations\nnp.random.seed(0)\nX = np.linspace(0, 1, n).reshape(-1, 1)\ntrue_slope = 5\ntrue_intercept = 0\nnoise = np.random.normal(0, 1, n).reshape(-1, 1)  # Gaussian noise\ny = true_slope * X + true_intercept + noise\ny = y.ravel()\n\ny_preds = {}\n\n# Step 2: Fit linear regression model\nweights = np.linalg.lstsq(X, y, rcond=None)[0]  # Get weights from least squares solution\ny_preds['Linear'] = X.dot(weights).ravel()  # Recompute predictions using weights\n\nfor power in [5, 10]:\n    X_powers = X ** np.arange(0, power)\n    weights = np.linalg.lstsq(X_powers, y, rcond=None)[0]\n    y_preds[f'Degree {power}'] = X_powers.dot(weights).ravel()\n\n# Step 4: Plotting\nplt.figure(figsize=(5, 3))\n\nlinestyles = ['-', '--', '-.']\n\ncolors = ['#a8ddb5', '#41ab5d', '#005a32']\n\nfor idx, (label, y_pred) in enumerate(y_preds.items()):\n    plt.plot(X, y_pred, label=label, linewidth=3, alpha=0.7, zorder=1, linestyle=linestyles[idx], color=colors[idx])\n\nplt.scatter(X, y, label='Training Data', marker='o', color='#3B5998', s=60, zorder=2)  # Deep Cornflower Blue\n\nplt.legend()\nplt.title('Linear Regression vs Neural Network (Linear Data)')\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\nplt.tight_layout()\nplt.savefig('images/regression_overfitting.svg', bbox_inches='tight', dpi=300)\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Step 1: Generate noisy quadratic data\nn = 10 # Number of observations\nnp.random.seed(0)\nX = np.linspace(0, 1, n).reshape(-1, 1)\ntrue_slope = 5\nnoise = np.random.normal(0, 1, n).reshape(-1, 1)  # Gaussian noise\ny = 20 * (X-.5)**2 + noise\ny = y.ravel()\n\ny_preds = {}\n\n# Step 2: Fit linear regression model\nweights = np.linalg.lstsq(X, y, rcond=None)[0]  # Get weights from least squares solution\ny_preds['Linear'] = X.dot(weights).ravel()  # Recompute predictions using weights\n\nfor power in [5, 10]:\n    X_powers = X ** np.arange(0, power)\n    weights = np.linalg.lstsq(X_powers, y, rcond=None)[0]\n    y_preds[f'Degree {power}'] = X_powers.dot(weights).ravel()\n\n# Step 4: Plotting\nplt.figure(figsize=(5, 3))\n\nlinestyles = ['-', '--', '-.']\ncolors = ['#a8ddb5', '#41ab5d', '#005a32']\n\nfor idx, (label, y_pred) in enumerate(y_preds.items()):\n    plt.plot(X, y_pred, label=label, linewidth=3, alpha=0.7, zorder=1, linestyle=linestyles[idx], color=colors[idx])\n\nplt.scatter(X, y, label='Training Data', marker='o', color='#3B5998', s=60, zorder=2)  # Deep Cornflower Blue\n\nplt.legend()\nplt.title('Linear Regression vs Neural Network (Quadratic Data)')\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\nplt.tight_layout()\nplt.savefig('images/regression_regularization.svg', bbox_inches='tight', dpi=300)"
  },
  {
    "objectID": "notes/code.html#logistic-regression",
    "href": "notes/code.html#logistic-regression",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(5, 3))\nX = np.linspace(-5, 5, 1000).reshape(-1, 1)\nys = {}\n\nys['Step'] = np.where(X &gt; 0, 1, 0)  # Step function\nys['Logistic'] = 1 / (1 + np.exp(-X))  # Logistic function\n\ncolors = ['red', 'green']\nlinestyles = ['--', '-']\nfor idx, (label, y) in enumerate(ys.items()):\n    plt.plot(X, y, label=label, linewidth=3, alpha=0.7, zorder=1, linestyle=linestyles[idx], color=colors[idx])\n\nplt.legend()\nplt.title('Classification Outputs')\nplt.xlabel(r'$\\langle \\mathbf{x}, \\mathbf{w} \\rangle$')\nplt.ylabel(r'Output')\nplt.tight_layout()\nplt.savefig('images/classification_outputs.svg', bbox_inches='tight', dpi=300)\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(5, 3))\nX = np.linspace(-5, 5, 1000).reshape(-1, 1)\nys = {}\n\nys[r'$\\ell_1$'] = np.abs(X - 1)\nys[r'$\\ell_2$'] =  (X-1)**2 /4 # Squared loss\nys['Cross Entropy'] = -np.log(1 / (1 + np.exp(-X)))  # Logistic function\n\ncolors = ['red', 'blue', 'green']\nlinestyles = ['--', '-.', '-']\nfor idx, (label, y) in enumerate(ys.items()):\n    plt.plot(X, y, label=label, linewidth=3, alpha=0.7, zorder=1, linestyle=linestyles[idx], color=colors[idx])\n\nplt.legend()\nplt.title('Classification Loss')\nplt.xlabel(r'$\\langle \\mathbf{x}, \\mathbf{w} \\rangle$')\nplt.ylabel(r'Loss if $y=1$')\nplt.tight_layout()\nplt.savefig('images/classification_loss.svg', bbox_inches='tight', dpi=300)\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Set seed and generate balanced data\nnp.random.seed(42)\nn = 1000\n\n# Positive class: inside circle (radius &lt; 1.5)\npos = np.random.normal(0, 0.8, size=(n * 2, 2))\nr_pos = np.linalg.norm(pos, axis=1)\npos = pos[r_pos &lt; 1.4][:n // 2]\n\n# Negative class: just outside circle (radius &gt; 1.7)\nneg = np.random.normal(0, 1.2, size=(n * 2, 2))\nr_neg = np.linalg.norm(neg, axis=1)\nneg = neg[r_neg &gt; 1.6][:len(pos)]\n\n# Combine\nX = np.vstack([pos, neg])\ny = np.hstack([np.ones(len(pos)), np.zeros(len(neg))])\n\n# Colors\ncolors = np.array(['blue', 'red'])\nmarker_size = 10\n\n# Create figure\nfig = plt.figure(figsize=(9, 4))\n\n# --- 2D plot ---\nax1 = fig.add_subplot(1, 2, 1)\nax1.scatter(X[y == 0, 0], X[y == 0, 1], s=marker_size, c='blue', marker='^', edgecolors='none', label='Negative')\nax1.scatter(X[y == 1, 0], X[y == 1, 1], s=marker_size, c='green', marker='s', edgecolors='none', label='Positive')\n# Decision boundary\ncircle = plt.Circle((0, 0), 1.5, edgecolor='gray', fill=False, linewidth=2)\nax1.add_artist(circle)\n\n# Axis settings\nax1.set_xlim(-4, 4)\nax1.set_ylim(-4, 4)\nax1.set_aspect('equal')\n#ax1.set_title(\"Input Space\")\nax1.set_xlabel('$x_1$')\nax1.set_ylabel('$x_2$')\nax1.tick_params(left=True, bottom=True)\nfor spine in ax1.spines.values():\n    spine.set_visible(True)\n\n# --- 3D plot ---\nax2 = fig.add_subplot(1, 2, 2, projection='3d')\n\n# Feature transformation: z = - (x1^2 + x2^2)\nX3 = np.sum(X**2, axis=1)\nX_3d = np.hstack([X, X3[:, np.newaxis]])\n\nax2.scatter(X_3d[y == 0, 0], X_3d[y == 0, 1], X_3d[y == 0, 2],\n            c='blue', s=marker_size, marker='^', label='Negative', alpha=0.6)\nax2.scatter(X_3d[y == 1, 0], X_3d[y == 1, 1], X_3d[y == 1, 2],\n            c='green', s=marker_size, marker='s', label='Positive', alpha=0.6)\n\n\n# Separating plane: z = -2.25\nx1_range = np.linspace(-2, 2, 50)\nx2_range = np.linspace(-2, 2, 50)\nx1_grid, x2_grid = np.meshgrid(x1_range, x2_range)\nz_plane = 2.25 * np.ones_like(x1_grid)\n\nax2.plot_surface(x1_grid, x2_grid, z_plane, color='grey', alpha=0.5, edgecolor='none')\n\n# View and axis cleanup\nax2.view_init(elev=25, azim=135)\nax2.set_xlim(-4, 4)\nax2.set_ylim(-4, 4)\nax2.set_zlim(-20, 2)\n#ax2.set_title(\"Transformed Feature Space\")\nax2.set_xlabel(\"$x_1$\")\nax2.set_ylabel(\"$x_2$\")\nax2.set_zlabel(r\"$(x_1^2 + x_2^2)$\")\nax2.grid(False)\n\n# Adjust view and axis limits for a pointed cone effect\nax2.view_init(elev=10, azim=45)\nax2.set_xlim(-3, 3)\nax2.set_ylim(-3, 3)\nax2.set_zlim(-2, 10)  # Compress vertical scale to make cone sharper\n\n# Final layout adjustments\n# Set white background and black cube lines for 3D plot\nax2.set_facecolor('white')\nax2.xaxis.pane.set_edgecolor('black')\nax2.yaxis.pane.set_edgecolor('black')\nax2.zaxis.pane.set_edgecolor('black')\n\nplt.tight_layout()\nplt.savefig('images/classification_transformation.svg', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\n\n# 1. Generate overlapping data\nnp.random.seed(0)\nn = 200\nmean_pos = [1, 1]\nmean_neg = [-1, -1]\ncov = [[1.5, 0.5], [0.5, 1.5]]\n\nX_pos = np.random.multivariate_normal(mean_pos, cov, size=n//2)\nX_neg = np.random.multivariate_normal(mean_neg, cov, size=n//2)\nX = np.vstack((X_pos, X_neg))\ny = np.hstack((np.ones(n//2), np.zeros(n//2)))\n\n# 2. Fit logistic regression\nclf = LogisticRegression()\nclf.fit(X, y)\n\n# 3. Create meshgrid\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 400),\n                     np.linspace(y_min, y_max, 400))\ngrid = np.c_[xx.ravel(), yy.ravel()]\nprobs = clf.predict_proba(grid)[:, 1].reshape(xx.shape)\n\n# 4. Plot\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Scatter data points\nax.scatter(X[y == 1, 0], X[y == 1, 1], c='green', s=10, marker='s', label='Positive')\nax.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', s=10, marker='^', label='Negative')\n\n\n# Thresholds and colors\nthresholds = [0.3, 0.5, 0.7]\nimport matplotlib\noranges = matplotlib.colormaps.get_cmap('Oranges')\ncolors = [oranges(i) for i in [0.4, 0.65, 0.9]] \n\nfor tau, color in zip(thresholds, colors):\n    CS = ax.contour(xx, yy, probs, levels=[tau], colors=[color], linewidths=2)\n    \n    # If there's a contour segment, label it manually\n    if len(CS.allsegs[0]) &gt; 0:\n        seg = CS.allsegs[0][0]\n        if len(seg) &gt; 0:\n            # Pick the right-most point (max x)\n            x_text, y_text = seg[np.argmax(seg[:, 0])]\n            ax.text(x_text + 0.2, y_text, fr'$\\tau$={tau:.1f}',\n                    color=color, fontsize=9, ha='left', va='center',\n                    bbox=dict(boxstyle='round,pad=0.2', fc='white', ec='none', alpha=0.7))\n\n\n# Final formatting\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.legend()\nax.set_title(\"Decision Boundaries with Varying Thresholds\")\nplt.tight_layout()\nplt.savefig('images/classification_boundaries.svg', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\n# --- 1. Generate overlapping data ---\nnp.random.seed(0)\nn = 200\nmean_pos = [1, 1]\nmean_neg = [-1, -1]\ncov = [[1.5, 0.5], [0.5, 1.5]]\n\nX_pos = np.random.multivariate_normal(mean_pos, cov, size=n//2)\nX_neg = np.random.multivariate_normal(mean_neg, cov, size=n//2)\nX = np.vstack((X_pos, X_neg))\ny = np.hstack((np.ones(n//2), np.zeros(n//2)))\n\n# --- 2. Fit logistic regression ---\nclf = LogisticRegression()\nclf.fit(X, y)\nprobs = clf.predict_proba(X)[:, 1]\n\n# --- 3. Compute ROC curve and AUC ---\nfpr, tpr, thresholds = roc_curve(y, probs)\nauc = roc_auc_score(y, probs)\n\n# --- 4. Plot ROC curve ---\nplt.figure(figsize=(6, 4))\nplt.plot(fpr, tpr, label=f\"Linear Regression (AUC = {auc:.2f})\", color='steelblue', linewidth=2)\nplt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Guessing (AUC = 0.5)')\n\n# --- 5. Highlight specific τ values ---\ntau_values = [0.3, 0.5, 0.7]\nimport matplotlib\noranges = matplotlib.colormaps.get_cmap('Oranges')\ncolors = [oranges(i) for i in [0.4, 0.65, 0.9]] \n\nfor tau, color in zip(tau_values, colors):\n    # Find closest index in thresholds array\n    idx = np.argmin(np.abs(thresholds - tau))\n    plt.scatter(fpr[idx], tpr[idx], color=color, edgecolor='black', zorder=5)\n    plt.text(fpr[idx]+0.02, tpr[idx]-0.02, fr'$\\tau$={tau}', fontsize=9, color=color, verticalalignment='center')\n\n# --- 6. Format plot ---\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend()\nplt.tight_layout()\n\nplt.savefig('images/classification_roc.svg', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "notes/code.html#support-vector-machines",
    "href": "notes/code.html#support-vector-machines",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "Support Vector Machines",
    "text": "Support Vector Machines\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib\n\n# ==== 1. Generate *linearly separable* data ====\nseed = 42        # change seed to get different random draws\nnp.random.seed(seed)\n\nn = 30\nmean_pos = [ 1,  1]\nmean_neg = [-1, -1]\ncov = [[0.4, 0.0],   # tight, nearly spherical clusters =&gt; separable\n       [0.0, 0.4]]\n\nX_pos = np.random.multivariate_normal(mean_pos, cov, size=n//2)\nX_neg = np.random.multivariate_normal(mean_neg, cov, size=n//2)\nX = np.vstack((X_pos, X_neg))\ny = np.hstack((np.ones(n//2), np.zeros(n//2)))\n\n# Stretch x so data width : height ≈ 6 : 4  (i.e., multiply by 1.5)\nstretch = 6 / 3  # 1.5\nX[:, 0] *= stretch\n\n# ==== 2. Fit logistic regression ====\nclf = LogisticRegression(max_iter=1000)\nclf.fit(X, y)\n\n# ==== 3. Create meshgrid ====\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 400),\n                     np.linspace(y_min, y_max, 400))\ngrid = np.c_[xx.ravel(), yy.ravel()]\nprobs = clf.predict_proba(grid)[:, 1].reshape(xx.shape)\n\n# ==== 4. Plot ====\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Scatter data points — match your style\nax.scatter(X[y == 1, 0], X[y == 1, 1],\n           c='green', marker='s', label='Positive')\nax.scatter(X[y == 0, 0], X[y == 0, 1],\n           c='blue', marker='^', label='Negative')\n\n# Final formatting\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\n#ax.legend()\n#ax.set_title(\"Linearly Separable Data\")\nax.set_aspect('equal')\nplt.tight_layout()\n\n# Save & show\nplt.savefig('images/svm_data.svg', dpi=300, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\n\n# ==== 1. Generate *linearly separable* data ====\nseed = 42\nnp.random.seed(seed)\n\nn = 30\nmean_pos = [1, 1]\nmean_neg = [-1, -1]\ncov = [[0.4, 0.0], [0.0, 0.4]]\n\nX_pos = np.random.multivariate_normal(mean_pos, cov, size=n//2)\nX_neg = np.random.multivariate_normal(mean_neg, cov, size=n//2)\nX = np.vstack((X_pos, X_neg))\ny = np.hstack((np.ones(n//2), np.zeros(n//2)))\n\n# Stretch x so data width : height ≈ 6 : 4  (i.e., multiply by 1.5)\nstretch = 6 / 3  # 1.5\nX[:, 0] *= stretch\n\n# ==== 2. Fit SVM with linear kernel ====\nclf = SVC(kernel='linear', C=1e5)  # large C for hard-margin behavior\nclf.fit(X, y)\n\n# ==== 3. Create meshgrid for plotting (optional) ====\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 400),\n                     np.linspace(y_min, y_max, 400))\n\n# ==== 4. Plot ====\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Scatter data points\nax.scatter(X[y == 1, 0], X[y == 1, 1], c='green', marker='s', label='Positive')\nax.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', marker='^', label='Negative')\n\n# ==== 5. Add three different (non-parallel) separating hyperplanes ====\ndef plot_separator(ax, slope, intercept, style, color, label):\n    x_vals = np.linspace(x_min, x_max, 200)\n    y_vals = slope * x_vals + intercept\n    ax.plot(x_vals, y_vals, style, color=color, linewidth=1.8, label=label)\n\n# Line 1: SVM decision boundary (maximum margin)\nw = clf.coef_[0]\nintercept_svm = clf.intercept_[0]\nslope_svm = -w[0] / w[1]\nintercept_line = -intercept_svm / w[1]\nplot_separator(ax, slope_svm, intercept_line, '-', 'black', 'Separator 1 (SVM)')\n\n# Line 2: custom separator with small margin, different slope\nplot_separator(ax, -0.4, 0, '--', 'black', 'Separator 2 (Small Margin)')\n\n# Line 3: another custom separator with small margin, different slope\nplot_separator(ax, -1.8, 0.2, '-.', 'black', 'Separator 3 (Small Margin)')\n\n# ==== Final formatting ====\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_aspect('equal')\nplt.tight_layout()\n\n# Save and show\nplt.savefig('images/svm_possible_lines.svg', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\n\n# ==== 1. Generate *linearly separable* data ====\nseed = 42\nnp.random.seed(seed)\n\nn = 30\nmean_pos = [1, 1]\nmean_neg = [-1, -1]\ncov = [[0.4, 0.0], [0.0, 0.4]]\n\n\nX_pos = np.random.multivariate_normal(mean_pos, cov, size=n//2)\nX_neg = np.random.multivariate_normal(mean_neg, cov, size=n//2)\nX = np.vstack((X_pos, X_neg))\ny = np.hstack((np.ones(n//2), np.zeros(n//2)))\n\n# Stretch x so data width : height ≈ 6 : 4  (i.e., multiply by 1.5)\nstretch = 6 / 3  # 1.5\nX[:, 0] *= stretch\n\n# ==== 2. Fit SVM with linear kernel ====\nclf = SVC(kernel='linear', C=1e5)  # large C for hard-margin behavior\nclf.fit(X, y)\n\n# ==== 3. Create meshgrid for plotting (optional) ====\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 400),\n                     np.linspace(y_min, y_max, 400))\n\n# ==== 4. Plot ====\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Scatter data points\nax.scatter(X[y == 1, 0], X[y == 1, 1], c='green', marker='s')\nax.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', marker='^')\n\n# ==== 5. Add three different (non-parallel) separating hyperplanes ====\ndef plot_separator(ax, slope, intercept, style, color, label=\"\"):\n    x_vals = np.linspace(x_min, x_max, 200)\n    y_vals = slope * x_vals + intercept\n    ax.plot(x_vals, y_vals, style, color=color, linewidth=1.8, label=label)\n\n# Line 1: SVM decision boundary (maximum margin)\nw = clf.coef_[0]\nintercept_svm = clf.intercept_[0]\nslope_svm = -w[0] / w[1]\nintercept_line = -intercept_svm / w[1]\nmargin = 1 / np.linalg.norm(w)\nplot_separator(ax, slope_svm, intercept_line, '-', 'black')\n\nax.text(x_min + 0.5, y_max - 0.5,\n        r'$\\langle \\mathbf{x}, \\mathbf{w} \\rangle - b = 0$',\n        fontsize=12,\n        bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.2'))\n\n# Line 2 and 3: small-margin lines\n# Correct: plot margin boundaries using (w · x + b = ±1)\n# These yield: x2 = -(w0/w1) * x1 - (b ± 1)/w1\n\nintercept_plus = -(intercept_svm - 1) / w[1]  # w·x + b = 1 ⇒ b - 1\nintercept_minus = -(intercept_svm + 1) / w[1] # w·x + b = -1 ⇒ b + 1\n\nplot_separator(ax, slope_svm, intercept_plus, '--', 'gray')\nplot_separator(ax, slope_svm, intercept_minus, '--', 'gray')\n\n\n# ==== 6. Draw normal vector w ====\n# ==== Compute & draw the normal vector w ====\n# w is the normal vector to the hyperplane\nw = clf.coef_[0]\nb = clf.intercept_[0]\n\n# Unit vector in direction of w\nw_unit = w / np.linalg.norm(w)\n\n# Find a point (x0, x1) on the hyperplane\n# We'll pick x = 0 and solve for x1 using the decision boundary equation:\n# w0*x + w1*y + b = 0 → y = -(w0*x + b)/w1\nx0 = 0\ny0 = -(w[0] * x0 + b) / w[1]\nstart = np.array([x0, y0])\nend = start + 1.0 * w_unit  # extend for visibility\n\n# Draw arrow from the hyperplane in direction of w\nax.annotate('', xy=end, xytext=start,\n            arrowprops=dict(arrowstyle='-&gt;', color='black', linewidth=2))\nax.text(end[0] - 0.1, end[1], r'$\\mathbf{w}$', fontsize=12)\n\n\n# ==== Final formatting ====\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_aspect('equal') \n#ax.legend(loc='upper left', framealpha=1)\nplt.tight_layout()\n\n# Save and show\nplt.savefig('images/svm_definition.svg', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\n\n# ==== 1. Generate *linearly separable* data ====\nseed = 42\nnp.random.seed(seed)\n\nn = 30\nmean_pos = [1, 1]\nmean_neg = [-1, -1]\ncov = [[0.4, 0.0], [0.0, 0.4]]\n\n\nX_pos = np.random.multivariate_normal(mean_pos, cov, size=n//2)\nX_neg = np.random.multivariate_normal(mean_neg, cov, size=n//2)\nX = np.vstack((X_pos, X_neg))\ny = np.hstack((np.ones(n//2), np.zeros(n//2)))\n\n# Stretch x so data width : height ≈ 6 : 4  (i.e., multiply by 1.5)\nstretch = 6 / 3  # 1.5\nX[:, 0] *= stretch\n\n# ==== 2. Fit SVM with linear kernel ====\nclf = SVC(kernel='linear', C=1e5)  # large C for hard-margin behavior\nclf.fit(X, y)\n\n# ==== 3. Create meshgrid for plotting (optional) ====\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 400),\n                     np.linspace(y_min, y_max, 400))\n\n# ==== 4. Plot ====\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Scatter data points\nax.scatter(X[y == 1, 0], X[y == 1, 1], c='green', marker='s')\nax.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', marker='^')\n\n# ==== 5. Add three different (non-parallel) separating hyperplanes ====\ndef plot_separator(ax, slope, intercept, style, color, label=\"\"):\n    x_vals = np.linspace(x_min, x_max, 200)\n    y_vals = slope * x_vals + intercept\n    ax.plot(x_vals, y_vals, style, color=color, linewidth=1.8, label=label)\n\n# Line 1: SVM decision boundary (maximum margin)\nw = clf.coef_[0]\nintercept_svm = clf.intercept_[0]\nslope_svm = -w[0] / w[1]\nintercept_line = -intercept_svm / w[1]\nmargin = 1 / np.linalg.norm(w)\nplot_separator(ax, slope_svm, intercept_line, '-', 'black')\n\n# Correct: plot margin boundaries using (w · x + b = ±1)\n# These yield: x2 = -(w0/w1) * x1 - (b ± 1)/w1\n\nintercept_plus = -(intercept_svm - 1) / w[1]  # w·x + b = 1 ⇒ b - 1\nintercept_minus = -(intercept_svm + 1) / w[1] # w·x + b = -1 ⇒ b + 1\n\nplot_separator(ax, slope_svm, intercept_plus, '--', 'gray')\nplot_separator(ax, slope_svm, intercept_minus, '--', 'gray')\n\n\n# ==== 6. Add z1 and z2 on margin boundaries ====\n\n# Unit vector in direction of w (normal to boundary)\n\n# Point on decision boundary (midpoint between margins)\n# We can pick any x along the boundary; let's choose x = 0\nx0 = 0\ny0 = slope_svm * x0 + intercept_line\npoint_on_hyperplane = np.array([x0, y0])\n\n# Compute z1 and z2 by moving along ±w_unit * margin\nz1 = point_on_hyperplane - margin * w / np.linalg.norm(w)\nz2 = point_on_hyperplane + margin * w/ np.linalg.norm(w)\n\n# Plot z1 and z2\nax.plot(z1[0], z1[1], 'ko')  # black point\nax.plot(z2[0], z2[1], 'ko')\n\nax.text(z1[0] - 0.3, z1[1] - 0.2, r'$z_2$', fontsize=12)\nax.text(z2[0] + 0.1, z2[1] + 0.05, r'$z_1$', fontsize=12)\n\n\n# ==== Final formatting ====\nax.set_xlim(x_min/3, x_max/3)\nax.set_ylim(y_min/3, y_max/3)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_aspect('equal') \n#ax.legend(loc='upper left', framealpha=1)\nplt.tight_layout()\n\n# Save and show\nplt.savefig('images/svm_margin.svg', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\n\n# ==== 1. Generate *linearly separable* data ====\nseed = 42\nnp.random.seed(seed)\n\nn = 30\nmean_pos = [1, 1]\nmean_neg = [-1, -1]\ncov = [[0.4, 0.0], [0.0, 0.4]]\n\n\nX_pos = np.random.multivariate_normal(mean_pos, cov, size=n//2)\nX_neg = np.random.multivariate_normal(mean_neg, cov, size=n//2)\nX = np.vstack((X_pos, X_neg))\ny = np.hstack((np.ones(n//2), np.zeros(n//2)))\n\n# Stretch x so data width : height ≈ 6 : 4  (i.e., multiply by 1.5)\nstretch = 6 / 3  # 1.5\nX[:, 0] *= stretch\n\n# ==== 2. Fit SVM with linear kernel ====\nclf = SVC(kernel='linear', C=1e5)  # large C for hard-margin behavior\nclf.fit(X, y)\n\n# ==== 3. Create meshgrid for plotting (optional) ====\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 400),\n                     np.linspace(y_min, y_max, 400))\n\n# ==== 4. Plot ====\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Scatter data points\nax.scatter(X[y == 1, 0], X[y == 1, 1], c='green', marker='s')\nax.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', marker='^')\n\n# ==== 5. Add three different (non-parallel) separating hyperplanes ====\ndef plot_separator(ax, slope, intercept, style, color, label=\"\"):\n    x_vals = np.linspace(x_min, x_max, 200)\n    y_vals = slope * x_vals + intercept\n    ax.plot(x_vals, y_vals, style, color=color, linewidth=1.8, label=label)\n\n# Line 1: SVM decision boundary (maximum margin)\nw = clf.coef_[0]\nintercept_svm = clf.intercept_[0]\nslope_svm = -w[0] / w[1]\nintercept_line = -intercept_svm / w[1]\nmargin = 1 / np.linalg.norm(w)\nplot_separator(ax, slope_svm, intercept_line, '-', 'black')\n\n# Correct: plot margin boundaries using (w · x + b = ±1)\n# These yield: x2 = -(w0/w1) * x1 - (b ± 1)/w1\n\nintercept_plus = -(intercept_svm - 1) / w[1]  # w·x + b = 1 ⇒ b - 1\nintercept_minus = -(intercept_svm + 1) / w[1] # w·x + b = -1 ⇒ b + 1\n\nplot_separator(ax, slope_svm, intercept_plus, '--', 'gray')\nplot_separator(ax, slope_svm, intercept_minus, '--', 'gray')\n\n# ==== 6. Highlight support vectors with red dotted circles ====\nsupport_vectors = clf.support_vectors_\nfor sv in support_vectors:\n    circle = plt.Circle((sv[0], sv[1]), radius=0.2, edgecolor='red',\n                        facecolor='none', linestyle=':', linewidth=1.5)\n    ax.add_patch(circle)\n\n\n# ==== Final formatting ====\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_aspect('equal') \n#ax.legend(loc='upper left', framealpha=1)\nplt.tight_layout()\n\n# Save and show\nplt.savefig('images/svm_supports.svg', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\n\n# ==== 1. Generate *linearly separable* data ====\nseed = 42\nnp.random.seed(seed)\n\nn = 30\nmean_pos = [.75, .75]\nmean_neg = [-.75, -.75]\ncov = [[0.6, 0.0], [0.0, 0.6]]\n\n\nX_pos = np.random.multivariate_normal(mean_pos, cov, size=n//2)\nX_neg = np.random.multivariate_normal(mean_neg, cov, size=n//2)\nX = np.vstack((X_pos, X_neg))\ny = np.hstack((np.ones(n//2), np.zeros(n//2)))\n\n# Stretch x so data width : height ≈ 6 : 4  (i.e., multiply by 1.5)\nstretch = 6 / 3  # 1.5\nX[:, 0] *= stretch\n\n# ==== 2. Fit SVM with linear kernel ====\nclf = SVC(kernel='linear', C=1)  # large C for hard-margin behavior\nclf.fit(X, y)\n\n# ==== 3. Create meshgrid for plotting (optional) ====\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 400),\n                     np.linspace(y_min, y_max, 400))\n\n# ==== 4. Plot ====\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Scatter data points\nax.scatter(X[y == 1, 0], X[y == 1, 1], c='green', marker='s')\nax.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', marker='^')\n\n# ==== 5. Add three different (non-parallel) separating hyperplanes ====\ndef plot_separator(ax, slope, intercept, style, color, label=\"\"):\n    x_vals = np.linspace(x_min, x_max, 200)\n    y_vals = slope * x_vals + intercept\n    ax.plot(x_vals, y_vals, style, color=color, linewidth=1.8, label=label)\n\n# Line 1: SVM decision boundary (maximum margin)\nw = clf.coef_[0]\nintercept_svm = clf.intercept_[0]\nslope_svm = -w[0] / w[1]\nintercept_line = -intercept_svm / w[1]\nmargin = 1 / np.linalg.norm(w)\nplot_separator(ax, slope_svm, intercept_line, '-', 'black')\n\n# Correct: plot margin boundaries using (w · x + b = ±1)\n# These yield: x2 = -(w0/w1) * x1 - (b ± 1)/w1\n\nintercept_plus = -(intercept_svm - 1) / w[1]  # w·x + b = 1 ⇒ b - 1\nintercept_minus = -(intercept_svm + 1) / w[1] # w·x + b = -1 ⇒ b + 1\n\nplot_separator(ax, slope_svm, intercept_plus, '--', 'gray')\nplot_separator(ax, slope_svm, intercept_minus, '--', 'gray')\n\n# ==== 6. Draw red lines from misclassified points to correct margin ====\nfrom numpy.linalg import norm\n\n# Signed distance from the hyperplane\ndecision_values = clf.decision_function(X)\n\n# Normalized w\nw = clf.coef_[0]\nw_unit = w / norm(w)\n\n# Go through all points\nfor xi, yi, di in zip(X, y, decision_values):\n    correct_margin = 1 if yi == 1 else -1\n    if di * (2 * yi - 1) &lt; 1:  # misclassified\n        # Distance to move along w to reach the correct margin\n        delta = (correct_margin - (np.dot(w, xi) + intercept_svm)) / norm(w)**2\n        projection = xi + delta * w  # point on correct margin\n        ax.plot([xi[0], projection[0]], [xi[1], projection[1]], 'r-', linewidth=1, alpha=0.5)\n\n# Scatter data points\nax.scatter(X[y == 1, 0], X[y == 1, 1], c='green', marker='s')\nax.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', marker='^')\n\n# ==== Final formatting ====\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_aspect('equal') \n#ax.legend(loc='upper left', framealpha=1)\nplt.tight_layout()\n\n# Save and show\nplt.savefig('images/svm_soft.svg', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(5, 3))\nX = np.linspace(-5, 5, 1000).reshape(-1, 1)\nys = {}\n\n\nys['Hinge'] = np.maximum((1-X), 0)\nys['Cross Entropy'] = -np.log(1 / (1 + np.exp(-X)))  # Logistic function\n\n\ncolors = ['green', 'blue']\nlinestyles = ['-', '--']\nfor idx, (label, y) in enumerate(ys.items()):\n    plt.plot(X, y, label=label, linewidth=3, alpha=0.7, zorder=1, linestyle=linestyles[idx], color=colors[idx])\n\nplt.legend()\nplt.title('Classification Loss')\nplt.xlabel(r'$\\langle \\mathbf{x}, \\mathbf{w} \\rangle$')\nplt.ylabel(r'Loss if $y=1$')\nplt.tight_layout()\nplt.savefig('images/svm_loss.svg', bbox_inches='tight', dpi=300)"
  },
  {
    "objectID": "notes/code.html#autoencoders",
    "href": "notes/code.html#autoencoders",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "Autoencoders",
    "text": "Autoencoders\n\nfrom PIL import Image, ImageOps\n\nimg = Image.open(\"images/cmc_seal_rgb.png\").convert(\"L\")\nimg_arr = np.array(img) / 255\ncmc_red = np.array([139, 39, 52])\n\n\n# scipy linalg\nimport scipy.linalg\nU, s, Vh = scipy.linalg.svd(img_arr, full_matrices=False)\nrank = len(s)\n\ndef build_k_approx(k):\n    return (U[:, :k] * s[:k]) @ Vh[:k, :]\n\n\napprox = build_k_approx(100)\ngray_image = Image.fromarray((approx * 255).astype(np.uint8), mode=\"L\")\nred_image = ImageOps.colorize(gray_image, black=cmc_red, white=\"white\")\nred_image\n\n/var/folders/7z/sn61c8nx2yq9f35pd6932s1hzz6mr5/T/ipykernel_45799/1576587091.py:2: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n  gray_image = Image.fromarray((approx * 255).astype(np.uint8), mode=\"L\")\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib as mpl\nmpl.rcParams['text.usetex'] = False  # avoid LaTeX dependency\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageOps\n\n# If not already defined:\n# cmc_red = (152, 26, 49)\n\ndef to_red_pil(a, low=(152, 26, 49), high=\"white\"):\n    \"\"\"Map a grayscale array [0,1] to a red-toned PIL image.\"\"\"\n    a = np.asarray(a, dtype=float)\n    # Ensure it's in [0,1]\n    if a.max() &gt; 1.0 or a.min() &lt; 0.0:\n        a = (a - a.min()) / (a.max() - a.min() + 1e-12)\n    gray = Image.fromarray((a * 255).astype(np.uint8), mode=\"L\")\n    return ImageOps.colorize(gray, black=low, white=high)\n\n# Your desired ranks\ndesired = [1, 5, 10, 50, 100, 500, 1000]\n\n# Get a sample to infer full rank (min(H, W))\n# If you already have `approx` from build_k_approx(100), you can use that:\nsample = build_k_approx(1)\nfull_k = min(sample.shape)\n\n# Cap any requested ranks to full_k to avoid errors\nranks = [min(k, full_k) for k in desired]\n# Add \"full rank\" explicitly at the end\nranks_with_full = ranks + [full_k]\n\n# Make subplots\nn = len(ranks_with_full)\ncols = 4\nrows = int(np.ceil(n / cols))\nfig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows), constrained_layout=True)\naxes = np.ravel(axes)\n\nfor ax, k in zip(axes, ranks_with_full):\n    arr = build_k_approx(k)\n    red_img = to_red_pil(arr, low=cmc_red, high=\"white\")\n    ax.imshow(red_img)\n    ax.set_title(rf\"Rank ${k}$ Approximation\" if k != full_k else f\"Full Rank\")\n    ax.axis(\"off\")\n\n\n# Hide any unused axes\nfor ax in axes[n:]:\n    ax.axis(\"off\")\n\nplt.savefig(\"images/autoencoder_approximations.svg\")\n\n/var/folders/7z/sn61c8nx2yq9f35pd6932s1hzz6mr5/T/ipykernel_45799/1783938776.py:16: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n  gray = Image.fromarray((a * 255).astype(np.uint8), mode=\"L\")\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Example data: replace with your singular values array\n# s should be sorted descending from your SVD\n# U, s, Vt = np.linalg.svd(A)\nrank = len(s)\n\n# Convert cmc_red (0–255 tuple) to Matplotlib's 0–1 float\ncmc_red_norm = tuple(np.array(cmc_red) / 255)\n\n# Compute approximation error: error[k] = sum_{i=k}^rank σᵢ²\n# Note: If you want k starting from 1, adjust indexing below\nerrors = [np.sum(s[k:]**2) for k in range(rank)]\n\n# Create subplots\nfig, axes = plt.subplots(1, 2, figsize=(9, 4))\n\n# ---- Left subplot: Singular Value Spectrum ----\naxes[0].plot(range(1, rank+1), s, color=cmc_red_norm, linewidth=2)\naxes[0].set_yscale(\"log\")\naxes[0].set_xlabel(\"Singular Value Index\")\naxes[0].set_ylabel(\"Singular Values\")\naxes[0].set_title(\"Singular Value Spectrum\")\n\n# ---- Right subplot: Approximation Error ----\naxes[1].plot(range(1, rank+1), errors, color=cmc_red_norm, linewidth=2)\naxes[1].set_yscale(\"log\")\naxes[1].set_xlabel(r\"$k$\")\naxes[1].set_ylabel(r\"$\\sum_{i=k+1}^{rank} \\sigma_i^2$\")\naxes[1].set_title(\"Approximation Error\")\n\nplt.tight_layout()\nplt.savefig(\"images/autoencoder_spectrum.svg\")"
  },
  {
    "objectID": "notes/01_PageRank.html",
    "href": "notes/01_PageRank.html",
    "title": "PageRank (The Power Method)",
    "section": "",
    "text": "Behind the real world impact of the machine learning algorithms we will study this semester, there is a rich mathematical foundation. Today, we will see how the eigenvalue decomposition of a matrix (that we learned about in the previous lecture) can be used to solve a real-world problem: finding the most important pages on the internet. This algorithm is known as PageRank, and it was the key technology that powered Google in its early days.\nThere are lots of webpages on the internet, and they are all interconnected by hyperlinks. Some pages, like wikipedia.org (website A) and mayoclinic.org (website B), are clearly important and authoritative. Other pages, like tealscats.com (website C), are less important. When we search for a topic, we’d like to return the most relevant pages, sorted by their importance.\nOf course, we could manually rank the pages, but that would be tedious, and way less fun. Instead, we will use the structure of the internet to determine which pages are most important. Consider a graph representation of the internet, where each page is a node and each hyperlink is a directed edge from one node to another.\n\n\n\nWe roughly expect that important pages will have many incoming links (e.g., lots of people link to Wikipedia). Not only that, but we also expect that these links come from other important pages (e.g., even authoritative pages like Mayo Clinic link to Wikipedia).\n\nPageRank\nWith these ideas in mind, let’s define an iterative process for refining our ranks of the pages. At first, we will assign each page the same rank, say \\(1/n\\) where \\(n\\) is the number of pages. Let \\(\\mathbf{p}^{(0)} \\in \\mathbb{R}^n\\) be the initial rank vector, where \\(\\mathbf{p}^{(0)}_i = 1/n\\) for each page \\(i\\). We will then iteratively update the rank vector so that the new rank of each page depends on how many pages link to it, and their ranks. We will use the following update rule: \\[\np_i^{(t+1)} = \\sum_{j: j \\text{ links to } i } \\frac{p_j^{(t)}}{d_j},\n\\] where \\(d_j\\) is the out-degree of page \\(j\\), i.e., the number of pages that page \\(j\\) links to. Why do we divide by \\(d_j\\)? If we didn’t, then a page with many outgoing links would dominate the rank of the pages it links to.\nNotice that the new rank is a linear combination of the previous ranks. In the language of linear algebra, we are taking the dot product between the previous rank vector \\(\\mathbf{p}^{(t)}\\) and a vector that only depends on the structure of the graph. For page \\(i\\), call this vector \\([\\mathbf{A}]_{i,} \\in \\mathbb{R}^n\\) where \\[\n\\mathbf{A}_{i,j} = \\begin{cases}\n\\frac{1}{d_j} & \\text{if } j \\text{ links to } i \\\\\n0 & \\text{otherwise.}\n\\end{cases}\n\\] When we consider the entire rank vector, we can write the update rule as \\[\n\\mathbf{p}^{(t+1)} = \\mathbf{A} \\mathbf{p}^{(t)},\n\\] where \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\) is the matrix whose \\(i\\)th row is \\(\\mathbf{A}_i\\). One useful observation is that each column of \\(\\mathbf{A}\\) sums to 1, i.e., \\(\\sum_{i=1}^n \\mathbf{A}_{i,j} = \\sum_{i: j \\text{ links to i }} \\frac1{d_j} = \\frac{d_j}{d_j} = 1\\) for each \\(j\\). This means that \\(\\mathbf{p}^{(t+1)}\\) will have the same sum as \\(\\mathbf{p}^{(t)}\\) i.e., \\[\n\\sum_{i=1}^n p_i^{(t+1)} = \\sum_{i=1}^n \\left( \\sum_{j=1}^n \\mathbf{A}_{i,j} p_j^{(t)} \\right) = \\sum_{j=1}^n p_j^{(t)} \\sum_{i=1}^n \\mathbf{A}_{i,j} = \\sum_{j=1}^n p_j^{(t)}.\n\\] Since we initialize \\(\\mathbf{p}^{(0)}\\) to have sum 1, we can see that \\(\\mathbf{p}^{(t)}\\) will always have sum 1. This enables us to interpret the rank vector \\(\\mathbf{p}^{(t)}\\) as a probability distribution over the pages. In particular, \\(p_i^{(t)}\\) is the probability that a random user will land on page \\(i\\) after \\(t\\) iterations of following links, when starting from a uniform distribution over all the pages.\nBut now, we have a problem. Suppose a webpage has no outgoing links. Then, all the probability coming into that page will be stuck. To avoid this, we will add a small amount of probability to each page at every iteration, which effectively simulates a random user who jumps to a random page with some small probability. Let \\(\\alpha \\in (0,1)\\) be close to 1, and define the new rank update rule as \\[\n\\mathbf{p}^{(t+1)} = \\alpha \\mathbf{A} \\mathbf{p}^{(t)} + (1-\\alpha) \\mathbf{1} \\frac{1}{n}.\n\\] So, even if a page traps all the probability it receives, we will still add a small amount of probability to every page.\n\n\nPower Method\nOur final update rule is slightly inelegant because it has two terms: one that depends on the previous rank vector and one that does not. But, we can rewrite it as a single matrix multiplication. Let \\(\\mathbf{1} \\in \\mathbb{R}^n\\) be the vector of all ones. Recall that \\(\\sum_{i=1}^n p_i^{(t)} = 1 = \\mathbf{1}^\\top \\mathbf{p}^{(t)}\\). Then, we can rewrite the update rule as \\[\n\\mathbf{p}^{(t+1)} = \\alpha \\mathbf{A} \\mathbf{p}^{(t)} + (1-\\alpha) \\frac{1}{n} \\mathbf{1} \\mathbf{1}^\\top \\mathbf{p}^{(t)} = \\left( \\alpha \\mathbf{A} + (1-\\alpha) \\frac{1}{n} \\mathbf{1} \\mathbf{1}^\\top \\right) \\mathbf{p}^{(t)},\n\\] where we used that \\(\\mathbf{1}^\\top \\mathbf{p}^{(t)} = 1\\).\nDefine the matrix \\(\\mathbf{M} = \\alpha \\mathbf{A} + (1-\\alpha) \\frac{1}{n} \\mathbf{1} \\mathbf{1}^\\top\\). The update rule can now be written as \\[\n\\mathbf{p}^{(t+1)} = \\mathbf{M} \\mathbf{p}^{(t)} = \\mathbf{M}^2 \\mathbf{p}^{(t-1)}\n= \\mathbf{M}^t \\mathbf{p}^{(0)}.\n\\]\nLet \\(\\mathbf{M} = \\sum_{i=1}^r  \\mathbf{v}_i \\lambda_i \\mathbf{v}_i^\\top\\) be the eigenvalue decomposition of \\(\\mathbf{M}\\), where \\(\\lambda_1 \\geq \\ldots \\geq \\lambda_r\\) are the eigenvalues and \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_r\\) are the corresponding eigenvectors. When we repeatedly multiply by \\(\\mathbf{M}\\), the orthonormal property of the eigenvectors gives us a surprisingly simple result: \\[\n\\mathbf{M}^2 = \\sum_{i=1}^r \\mathbf{v}_i \\lambda_i \\mathbf{v}_i^\\top\n\\sum_{j=1}^r \\mathbf{v}_j \\lambda_j \\mathbf{v}_j^\\top\n= \\sum_{i=1}^r \\mathbf{v}_i \\lambda_i \\mathbf{v}_i^\\top \\mathbf{v}_i \\lambda_i \\mathbf{v}_i^\\top = \\sum_{i=1}^r \\lambda_i^2 \\mathbf{v}_i \\mathbf{v}_i^\\top.\n\\] Here, we used that \\(\\mathbf{v}_i^\\top \\mathbf{v}_j = 0\\) for \\(i \\neq j\\) and \\(1\\) if \\(i=j\\). In general, \\(\\mathbf{M}^t = \\sum_{i=1}^r \\lambda_i^t \\mathbf{v}_i \\mathbf{v}_i^\\top\\).\nReturning to the page rank vector, we can write \\[\n\\mathbf{p}^{(t)} = \\mathbf{M}^t \\mathbf{p}^{(0)} = \\sum_{i=1}^r \\lambda_i^t \\mathbf{v}_i \\mathbf{v}_i^\\top \\mathbf{p}^{(0)}.\n= \\lambda_1^t \\sum_{i=1}^r \\left( \\frac{\\lambda_i}{\\lambda_1} \\right)^t \\mathbf{v}_i \\mathbf{v}_i^\\top \\mathbf{p}^{(0)}.\n\\] When \\(\\lambda_1 &gt; \\lambda_2\\), the term \\(\\left( \\frac{\\lambda_i}{\\lambda_1} \\right)^t\\) will go to 0 for all \\(i \\geq 2\\) as \\(t\\) increases. As long as \\(\\mathbf{v}_1^\\top \\mathbf{p}^{(0)}\\) is a small constant, the first term will dominate the sum, and \\[\n\\lim_{t \\to \\infty} \\mathbf{p}^{(t)} = \\lambda_1 \\mathbf{v}_1 \\left(\\mathbf{v}_1^\\top \\mathbf{p}^{(0)}\\right).\n\\] In summary, the page rank that Google uses to measure the importance of a page is simply a multiple of the first eigenvector of the matrix \\(\\mathbf{M}\\). Repeatedly multiplying matrices is known as the power method, and results in the dominant eigenvector of a matrix."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSCI 145: Data Mining",
    "section": "",
    "text": "A course on the mathematical foundations of machine learning.\n\n\n\n\nInstructor: R. Teal Witter. Please call me Teal.\nClass Times: We meet Tuesdays and Thursdays; Sec. 1 is scheduled from 2:45 to 4:00pm in Kravis 164, and Sec. 2 from 4:15 to 5:30pm in Kravis 165.\nOffice Hours: Before I schedule office hours, please fill out this when2meet so we can find times that work for all of us.\nParticipation: I expect you to engage in class, ask questions, and make connections. To receive credit, please fill out this form after every lecture.\nQuizzes: There will be short quizzes at the beginning of our Tuesday classes. These quizzes will test your understanding of the problem sets and the concepts from the prior week.\n\n\nProblem Sets: Your primary opportunity to learn the material will be on problem sets. You may work with others to solve the problems, but you must write your solutions by yourself, and explicitly acknowledge any outside help (websites, people, LLMs).\nExams: The two midterm exams are designed to give you a multiple ways of demonstrating your understanding. The first is a written midterm focused on supervised learning. The second is a verbal exam, à la a technical interview.\nProject: The project offers a chance to explore an area that interests you, practice writing high quality code, and develop your ability to communicate technical ideas to an audience. In addition to your codebase, you will write a report and give a short presentation at the end of the semester.\n\n\n\n\n\nWeek\n\n\nTuesday\n\n\nThursday\n\n\nSlides\n\n\nAssignments\n\n\n\n\nWarmup\n\n\n\n\nWeek 1 (8/27 and 8/29)\n\n\nLinear Algebra\n\n\nPageRank\n\n\n\n\n\n\n\n\nSupervised Learning\n\n\n\n\nWeek 2 (9/2 and 9/4)\n\n\nLinear Regression\n\n\nOptimization\n\n\n\n\n\n\n\n\nWeek 3 (9/9 and 9/11)\n\n\nGradient Descent\n\n\nPolynomial Regression\n\n\n\n\n\n\n\n\nWeek 4 (9/16 and 9/18)\n\n\nProbability\n\n\nLogistic Regression\n\n\n\n\n\n\n\n\nWeek 5 (9/23 and 9/25)\n\n\nSupport Vector Machines\n\n\nConstrained Optimization\n\n\n\n\n\n\n\n\nWeek 6 (9/30 and 10/2)\n\n\nKernel Methods\n\n\nNeural Networks\n\n\n\n\n\n\n\n\nWeek 7 (10/7 and 10/9)\n\n\nConvolutional Networks\n\n\nTransformers\n\n\n\n\n\n\n\n\nWeek 8 (10/14 and 10/16)\n\n\nFall Break (No Class)\n\n\nDecision Trees\n\n\n\n\n\n\n\n\nWeek 9 (10/21 and 10/23)\n\n\nMidterm Exam\n\n\nBoosting\n\n\n\n\n\n\n\n\nBeyond Supervised Learning\n\n\n\n\nWeek 10 (10/28 and 10/30)\n\n\nAutoencoders\n\n\nPrincipal Component Analysis\n\n\n\n\n\n\n\n\nWeek 11 (11/4 and 11/6)\n\n\nReinforcement Learning\n\n\nQ Learning\n\n\n\n\n\n\n\n\nWeek 12 (11/11 and 11/13)\n\n\nConcentration Inequalities\n\n\nBandits\n\n\n\n\n\n\n\n\nWeek 13 (11/18 and 11/20)\n\n\nExplainable AI\n\n\nActive Regression\n\n\n\n\n\n\n\n\nWeek 14 (11/25 and 11/27)\n\n\nFinal Exam\n\n\nThanksgiving (No Class)\n\n\n\n\n\n\n\n\nWeek 15 (12/2 and 12/4)\n\n\nProject Preparation\n\n\nProject Preparation (No Class)\n\n\n\n\n\n\n\n\nWeek 16 (12/9 and 12/11)\n\n\nSec. 2 Presents 7–10pm\n\n\nSec. 1 Presents 2–5pm"
  },
  {
    "objectID": "notes/04_LogisticRegression.html",
    "href": "notes/04_LogisticRegression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "With a little probability, we saw how the Naive Bayes Classifier can be used to make predictions. However, the Naive Bayes Classifier assumes that the value of each feature is independent of the others, which is often not the case in practice (e.g., if the word “king” appears in an email, it is more likely that the word “queen” also appears). As an alternative approach to classification problems, we will see how we can generalize linear regression, with a little non-linearity.\nOur setup will be the standard supervised learning setting, where we have labelled data \\((\\mathbf{x}^{(1)}, y^{(1)}), \\ldots, (\\mathbf{x}^{(n)}, y^{(n)})\\), for \\(\\mathbf{x}^{(i)} \\in \\mathbb{R}^d\\) the feature vector for the \\(i\\)th example. However, unlike regression where we predict a continuous value \\(y^{(i)} \\in \\mathbb{R}\\), we will now predict a binary value \\(y^{(i)} \\in \\{0, 1\\}\\). We can use the same linear model as before, i.e., we will predict the output as \\(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle\\), where \\(\\mathbf{w} \\in \\mathbb{R}^d\\) is the weight vector. But, we run into an issue: the output of the linear model can take on any real value, but we want to predict a binary value.\n\nModel and Loss\nWe’ll explore several attempts to convert our linear model into a binary classifier.\nAttempt #1: We could simply apply a step function to the output of the linear model, i.e., predict \\(y^{(i)} = 1\\) if \\(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle &gt; 0\\) and \\(y^{(i)} = 0\\) otherwise. The loss could be the difference between the predicted value and the true value, i.e., \\(\\mathcal{L}(\\mathbf{w}) = \\sum_{i=1}^n |y^{(i)} - \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle|\\). However, this loss is not differentiable, so we cannot use gradient descent to optimize it.\nAttempt #2: We could use the mean squared error loss, i.e., \\(\\mathcal{L}(\\mathbf{w}) = \\sum_{i=1}^n (y^{(i)} - \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)^2\\). This loss is differentiable, but it does not work well for classification problems: if we have a large positive value for \\(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle\\), the loss will be large even if \\(y^{(i)} = 1\\).\nAttempt #3: We can apply the sigmoid function to the output of the linear model to map it to the range \\((0, 1)\\), i.e., we will predict \\(f(\\mathbf{x}^{(i)}) = \\sigma(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)\\), where \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. The sigmoid function is a smooth, non-linear function that maps any real number to the range \\((0, 1)\\). We can then interpret \\(f(\\mathbf{x}^{(i)})\\) as the probability that \\(y^{(i)} = 1\\) given the features \\(\\mathbf{x}^{(i)}\\). If we need to report a class label, we can threshold the predicted probability, e.g., predict \\(y^{(i)} = 1\\) if \\(f(\\mathbf{x}^{(i)}) &gt; \\frac12\\) and \\(y^{(i)} = 0\\) otherwise.\n\n\n\nTo train our model, we need a loss function that measures how well our predicted probabilities match the true labels. A common choice is the binary cross-entropy loss, which is defined as \\[\\mathcal{L}(\\mathbf{w}) = - \\sum_{i=1}^n \\left[y^{(i)} \\log(f(\\mathbf{x}^{(i)})) + (1 - y^{(i)}) \\log(1 - f(\\mathbf{x}^{(i)}))\\right].\\] This loss function measures the distance (in a sense we’ll explore later in the course) between the predicted probabilities and the true labels. It is differentiable, so we can use gradient descent to optimize it.\n\n\n\n\n\nOptimization\nOnce we have a model and loss function, we have seen two ways to optimize the model: Exact optimization, where we compute the gradient of the loss function with respect to the model parameters and set the gradient to zero to find the optimal parameters. Gradient descent, where we iteratively update the model parameters in the direction of the negative gradient of the loss function. Both approaches require the gradient of the loss function with respect to the model parameters, so let’s compute the gradient of \\(\\mathcal{L}(\\mathbf{w})\\) with respect to \\(\\mathbf{w}\\).\nPlugging in the sigmoid function, we have \\[\n\\begin{align*}\n\\mathcal{L}(\\mathbf{w}) &= - \\sum_{i=1}^n \\left[y^{(i)} \\log\\left(\\frac{1}{1 + e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}}\\right) + (1 - y^{(i)}) \\log\\left(1 - \\frac{1}{1 + e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}}\\right)\\right] \\\\\n\\end{align*}\n\\]\nObserve that \\(\\frac1{1 + e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}} = \\frac{e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}}{e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} + 1}\\), so \\(1-\\frac1{1 + e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}} = \\frac{1}{e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} + 1}\\).\nThen, we can rewrite the loss as \\[\n\\begin{align*}\n\\mathcal{L}(\\mathbf{w}) &= \\sum_{i=1}^n \\left[y^{(i)} \\log\\left(1+e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} \\right) + (1 - y^{(i)}) \\log\\left(e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} + 1\\right) \\right].\n\\end{align*}\n\\]\nLet’s compute the partial derivative of the loss with respect to \\(w_j\\). Applying the chain rule, we have \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial w_j} \\mathcal{L}(\\mathbf{w}) &= \\sum_{i=1}^n\n\\left[y^{(i)} \\frac1{1 + e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}} \\cdot e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} (-x_j^{(i)}) + (1 - y^{(i)}) \\frac1{e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} + 1} \\cdot e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} x_j^{(i)}\\right]\\\\\n\\end{align*}\n\\]\nObserve that \\(\\frac{e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}}{1 + e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}} = \\frac1{e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} + 1} = 1-\\sigma(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)\\), and \\(\\frac{e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}}{e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} + 1} = \\sigma(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)\\). Then, we can rewrite the partial derivative as \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial w_j} \\mathcal{L}(\\mathbf{w}) &= \\sum_{i=1}^n\n\\left[-y^{(i)} (1 - \\sigma(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)) x_j^{(i)} + (1 - y^{(i)}) \\sigma(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle) x_j^{(i)}\\right]\\\\\n&= \\sum_{i=1}^n\nx_j^{(i)} \\left[\\sigma(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle) - y^{(i)}\\right]\\\\\n&= \\mathbf{X}_j^\\top \\left(\\sigma(\\mathbf{X} \\mathbf{w}) - \\mathbf{y}\\right),\n\\end{align*}\n\\] where \\(\\sigma(\\cdot)\\) is applied element-wise to the vector \\(\\mathbf{X} \\mathbf{w}\\), and \\(\\mathbf{X}_j\\) is the \\(j\\)th column of the design matrix \\(\\mathbf{X}\\). Finally, we can write the gradient of the loss with respect to the weight vector \\(\\mathbf{w}\\) as \\[\n\\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}) =  \\mathbf{X}^\\top \\left(\\sigma(\\mathbf{X} \\mathbf{w}) - \\mathbf{y}\\right).\n\\]\nOur exact optimization approach would be to set the gradient to zero and solve for \\(\\mathbf{w}\\). Do you see why this doesn’t work with the non-linear sigmoid function?\nInstead of exact optimization, we will use gradient descent!\n\n\nNon-linear Transformations\nOften, our data is not linearly separable, i.e., we cannot draw a straight line to separate the two classes. In this case, we can use non-linear transformations to map the data to a higher-dimensional space, where it is linearly separable. One approach: As we saw for linear regression, we can add polynomial features to the data. In the image below, we add a new feature \\(x_1^2 + x_2^2\\) to the data, which allows us to separate the two classes with a linear decision boundary in the transformed feature space.\n\n\n\nIt is not a priori clear which non-linear transformation will work best for a given dataset. In several lectures, we will explore how to use kernel methods to implicitly map the data to a higher-dimensional space, which captures many of the non-linear transformations we might want to use.\n\n\nMeasuring Error in Binary Classification\nThe simplest way to measure the error of a classification model is to compute the error rate, which is the fraction of examples that are misclassified. For example, if we have \\(n\\) examples and our model misclassifies \\(k\\) of them, the error rate is \\(\\frac{k}{n}\\).\nHowever, the error rate does not take into account which points are misclassified. We will often break down the accuracy of a classification model into four categories:\n- True Positives (TP): The model correctly predicts a positive class.\n- True Negatives (TN): The model correctly predicts a negative class.\n- False Positives (FP): The model incorrectly predicts a positive class when the true class is negative.\n- False Negatives (FN): The model incorrectly predicts a negative class when the true class is positive.\n\n\n\nThe raw counts of these four categories can be summarized in a confusion matrix. The confusion matrix is a square matrix with dimensions equal to the number of classes, where the rows represent the true classes and the columns represent the predicted classes.\nBut, these raw counts themselves are not very informative.\nWe often report the True Positive Rate (TPR), also known as recall, which is the fraction of true positives out of all actual positives: \\(\\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}.\\) The TPR measures how well the model identifies positive examples (higher is better).\nWe also report the False Positive Rate (FPR), which is the fraction of false positives out of all actual negatives: \\(\\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}.\\) The FPR measures how often the model incorrectly identifies negative examples as positive (lower is better).\nFinally, we can report the Precision, which is the fraction of true positives out of all predicted positives: \\(\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}.\\) Precision measures how well the model identifies positive examples among all predicted positives (higher is better).\nIf we have a model that does not achieve the desired TPR or FPR, we have a hidden lever we can pull: the threshold for predicting a positive class. By default, we predict a positive class if the predicted probability is greater than \\(\\frac12\\). But, we can change this threshold to an arbitrary value \\(\\tau \\in [0, 1]\\). Increasing the threshold can only decrease the TPR, since we are less likely to predict a positive class; simultaneously, the FPR can only increase, since we are more likely to predict a negative class.\n[image here] 2D plots with linearly separable data, with different thresholds \\(\\tau\\).\n\n\n\nWe can visualize the trade-off between TPR and FPR by plotting the Receiver Operating Characteristic (ROC) curve. The ROC curve is a plot of the TPR against the FPR for different threshold values \\(\\tau\\). Because a higher TPR is better and a lower FPR is better, we want the ROC curve to be as close to the top-left corner as possible. The area under the ROC curve (AUC) is a single number that summarizes the performance of the model across all threshold values. A model with an AUC of 1 is perfect, while a model with an AUC of 0.5 is no better than random guessing.\n\n\n\n\n\nMultiple Classes\nIn many settings, we are interested in classifying data into more than two classes. For example, we might want to classify images into different categories, such as cats, dogs, and birds. In this case, we need to extend our binary classification model to handle multiple classes. Our supervised learning setup remains the same, where we have labelled data \\((\\mathbf{x}^{(1)}, y^{(1)}), \\ldots, (\\mathbf{x}^{(n)}, y^{(n)})\\), but now \\(y^{(i)} \\in \\{1, 2, \\ldots, k\\}\\), for \\(k\\) the number of classes.\nWe can still use the same ideas that we used for binary logistic regression, but we need to extend the output of the model to predict a probability distribution over each of the \\(k\\) classes. Instead of a single output, which we can interpret as the probability of the positive class, we will have a vector of outputs \\(\\mathbf{f}(\\mathbf{x}^{(i)}) \\in \\mathbb{R}^k\\), where \\(k\\) is the number of classes.\nTo ensure this vector is a valid probability distribution, we can use the softmax function, defined as \\[\n\\begin{align*}\n\\text{softmax}(\\mathbf{z}) &= \\begin{bmatrix}\n\\frac{e^{z_1}}{\\sum_{j=1}^k e^{z_j}} \\\\\n\\frac{e^{z_2}}{\\sum_{j=1}^k e^{z_j}} \\\\\n\\vdots \\\\\n\\frac{e^{z_k}}{\\sum_{j=1}^k e^{z_j}} \\\\\n\\end{bmatrix}\n\\end{align*}\n\\] Softmax applies the exponential function to each element of the vector, and then normalizes the resulting vector so that the sum of the resulting vector is 1. This ensures that the output is a valid probability distribution, where each probability is between 0 and 1 and the sum of all elements is 1.\nThe loss function for multi-class classification is the cross-entropy loss, which is a generalization of the binary cross-entropy loss. \\[\n\\begin{align*}\n\\mathcal{L}(\\mathbf{w}) &= - \\sum_{i=1}^n \\sum_{j=1}^k \\mathbb{1}[y^{(i)}=j] \\log\\left(f_j(\\mathbf{x}^{(i)})\\right),\n\\end{align*}\n\\] where \\(f_j(\\mathbf{x}^{(i)})\\) is the \\(j\\)th element of the softmax output vector \\(\\mathbf{f}(\\mathbf{x}^{(i)})\\), and \\(\\mathbb{1}[y^{(i)}=j]\\) is an indicator function that is 1 if \\(y^{(i)} = j\\) and 0 otherwise."
  },
  {
    "objectID": "notes/02_LinearRegression.html",
    "href": "notes/02_LinearRegression.html",
    "title": "Linear Regression and Optimization",
    "section": "",
    "text": "Machine learning is incredibly popular. Seen extensive progress in the past several decades, and especially recently with the advent of generative AI.\nThere are many problems that fall under the umbrella of machine learning:\n\nPredicting temperature based on present weather conditions,\nIdentifying whether an image contains a cat or dog, and\nGenerating the next word in a sentence.\n\nThis course is about how we go about solving these problems. The first half of the course will cover supervised learning, where we are given labeled data, and our goal is to train a function to approximately match the labels.\nConcretely, we are given \\(n\\) data points \\(\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(n)} \\in \\mathbb{R}^d\\), each with \\(d\\) dimensions, and associated labels \\(y^{(1)}, y^{(2)}, \\ldots, y^{(n)} \\in \\mathbb{R}\\). Our goal is to learn a function \\(f: \\mathbb{R}^d \\to \\mathbb{R}\\) so that \\(f(\\mathbf{x}^{(i)}) \\approx y^{(i)}\\) for all data points \\(i \\in \\{1,2,\\ldots,n\\}\\).\nOur general approach to solving supervised learning problems will be to use empirical risk minimization, which gives a flexible scaffolding that encompasses many of the topics we’ll discuss in this course. Given a function class (e.g., linear functions or neural networks), the idea is to select the function that most closely explains the data. In particular, there are three components to empirical risk minimization:\n\nFunction Class: The function class \\(\\mathcal{F}\\) from which we will select the function \\(f\\) that most closely fits the observed data.\nLoss: The loss function that measures how well a function \\(f\\) fits the observed data. (Without loss of generality, we will assume that lower is better.)\nOptimizer: The method of selecting the function from the function class.\n\nEmpirical risk minimization is an abstract idea. Luckily, we will revisit it again and again. Our first example will be linear regression, where the function class is the set of linear functions and the loss is the squared difference between the true label and our prediction. Let’s dive in!"
  },
  {
    "objectID": "notes/02_LinearRegression.html#supervised-learning",
    "href": "notes/02_LinearRegression.html#supervised-learning",
    "title": "Linear Regression and Optimization",
    "section": "",
    "text": "Machine learning is incredibly popular. Seen extensive progress in the past several decades, and especially recently with the advent of generative AI.\nThere are many problems that fall under the umbrella of machine learning:\n\nPredicting temperature based on present weather conditions,\nIdentifying whether an image contains a cat or dog, and\nGenerating the next word in a sentence.\n\nThis course is about how we go about solving these problems. The first half of the course will cover supervised learning, where we are given labeled data, and our goal is to train a function to approximately match the labels.\nConcretely, we are given \\(n\\) data points \\(\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(n)} \\in \\mathbb{R}^d\\), each with \\(d\\) dimensions, and associated labels \\(y^{(1)}, y^{(2)}, \\ldots, y^{(n)} \\in \\mathbb{R}\\). Our goal is to learn a function \\(f: \\mathbb{R}^d \\to \\mathbb{R}\\) so that \\(f(\\mathbf{x}^{(i)}) \\approx y^{(i)}\\) for all data points \\(i \\in \\{1,2,\\ldots,n\\}\\).\nOur general approach to solving supervised learning problems will be to use empirical risk minimization, which gives a flexible scaffolding that encompasses many of the topics we’ll discuss in this course. Given a function class (e.g., linear functions or neural networks), the idea is to select the function that most closely explains the data. In particular, there are three components to empirical risk minimization:\n\nFunction Class: The function class \\(\\mathcal{F}\\) from which we will select the function \\(f\\) that most closely fits the observed data.\nLoss: The loss function that measures how well a function \\(f\\) fits the observed data. (Without loss of generality, we will assume that lower is better.)\nOptimizer: The method of selecting the function from the function class.\n\nEmpirical risk minimization is an abstract idea. Luckily, we will revisit it again and again. Our first example will be linear regression, where the function class is the set of linear functions and the loss is the squared difference between the true label and our prediction. Let’s dive in!"
  },
  {
    "objectID": "notes/02_LinearRegression.html#univariate-linear-regression",
    "href": "notes/02_LinearRegression.html#univariate-linear-regression",
    "title": "Linear Regression and Optimization",
    "section": "Univariate Linear Regression",
    "text": "Univariate Linear Regression\nLinear regression is a simple but powerful tool that we will use to understand the basics of machine learning. For simplicity, we will first consider the univariate case where the inputs are all one-dimensional i.e., \\(x^{(1)}, \\ldots, x^{(n)} \\in \\mathbb{R}\\).\n\nLinear functions\nAs its name suggests, linear regression uses a linear function to process the input into an approximation of the output. Let \\(w \\in \\mathbb{R}\\) be a weight parameter. The linear function (for one-dimensional inputs) is given by \\(f(x) = wx\\).\nUnlike many machine learning functions, we can visualize the linear function since it is given by a line. In the plot, we have the \\(n=10\\) data points plotted in two dimensions. There is one linear function \\(f(x) = 2x\\) that closely approximates the data and another linear function \\(f(x)=\\frac12 x\\) that poorly approximates the data.\n\n\n\nOur goal is to learn how to find a linear function that fits the data well. Before we can do this, though, we will need to define what it means for a function to “fit the data well”.\n\n\nMean Squared Error Loss\nOur goal for the loss function is to measure how closely the data fits the prediction made by our function. Intuitively, we should take the difference between the prediction and the true outcome \\(f(x^{(i)})-y^{(i)}\\).\nThe issue with this approach is that \\(f(x^{(i)})-y^{(i)}\\) can be small (negative) even when \\(f(x^{(i)}) \\neq y^{(i)}\\). A natural fix is to take the absolute value \\(|f(x^{(i)}) - y^{(i)}|\\). The benefit of the absolute value is that the loss is \\(0\\) if and only if \\(f(x^{(i)}) = y^{(i)}\\). However, the absolute value function is not differentiable, which is a property we’ll need for optimization. Instead, we use the squared loss:\n\\(\\mathcal{L}(w) = \\frac1{n} \\sum_{i=1}^n (f(x^{(i)}) - y^{(i)})^2.\\)\nHere, we use the mean squared error loss, which is the average squared difference between the prediction and the true output over the dataset. Unlike the absolute value function, the squared function is differentiable everywhere. In addition, the squared error disproportionately penalizes predictions that are far from the true labels, a property that may be desirable when we want all of our predictions to be reasonably accurate.\n\n\n\nThe plot above compares the squared function to the absolute value function. While both are \\(0\\) if and only if their input is \\(0\\), the squared function is differentiable everywhere and penalizes large errors more.\n\n\nExact Optimization\nWe now have our function class and loss function: linear functions and mean squared error loss. The question becomes how to update the weights of the function to minimize the loss. In particular, we want to find \\(w\\) that minimizes \\(\\mathcal{L}(w)\\). While the language we’re using is new, the problem is not. We’ve actually been studying how to do this since pre-calculus!\nThe squared loss is convex (a bowl facing up versus the downward facing cave of concave); see the plot above for a ‘proof’ by example. In this case, we know there is only one minimum. Not only that but we can find the minimum by setting the derivative to \\(0\\).\nAs such, our game plan is to set \\(\\frac{\\partial \\mathcal{L}}{\\partial w}\\) to \\(0\\) and solve for \\(w\\). Recall that \\(f(x) = wx\\). We will use the linearity of the derivative, the chain rule, and the power rule to compute the derivative of \\(\\mathcal{L}\\) with respect to \\(w\\):\n\\[\n\\begin{align}\n\\frac{\\partial}{\\partial w}[\\mathcal{L}(w)]\n&= \\frac1{n} \\sum_{i=1}^n \\frac{\\partial}{\\partial w} [(f(x^{(i)}) - y^{(i)})^2]\n\\notag \\\\&= \\frac1{n} \\sum_{i=1}^n 2(f(x^{(i)}) - y^{(i)}) \\frac{\\partial}{\\partial w} [(f(x^{(i)}) - y^{(i)})]\n\\notag \\\\&= \\frac1{n} \\sum_{i=1}^n 2(w x^{(i)} - y^{(i)}) x^{(i)}.\n\\end{align}\n\\]\nSetting the derivative to \\(0\\) and solving for \\(w\\), we get \\(\\frac2{n} \\sum_{i=1}^n w \\cdot (x^{(i)})^2 = \\frac2{n} \\sum_{i=1}^n y^{(i)} x^{(i)}\\) and so \\[\nw = \\frac{\\sum_{i=1}^n y^{(i)} \\cdot x^{(i)}}{\\sum_{i=1}^n (x^{(i)})^2}.\n\\]\nThis is the exact solution to the univariate linear regression problem! We can now use this formula to find the best linear function for our univariate data. However, we’ll have to work slightly harder for the general case with multidimensional data."
  },
  {
    "objectID": "notes/02_LinearRegression.html#multivariate-linear-regression",
    "href": "notes/02_LinearRegression.html#multivariate-linear-regression",
    "title": "Linear Regression and Optimization",
    "section": "Multivariate Linear Regression",
    "text": "Multivariate Linear Regression\nConsider the more general setting where the input is \\(d\\)-dimensional. As before, we observe \\(n\\) training observations \\((\\mathbf{x}^{(1)}, y^{(1)}), \\ldots, (\\mathbf{x}^{(n)}, y^{(n)})\\) but now \\(\\mathbf{x}^{(i)} \\in \\mathbb{R}^d\\). We will generalize the ideas from univariate linear regression to the multivariate setting.\n\nLinear function\nInstead of using a single weight \\(w \\in \\mathbb{R}\\), we will use \\(d\\) weights \\(\\mathbf{w} \\in \\mathbb{R}^d\\). Then the function is given by \\(f(x) = \\langle \\mathbf{w}, \\mathbf{x} \\rangle\\).\nInstead of using a line to fit the data, we use a hyperplane. While visualizing the function is difficult in high dimensions, we can still see the function when \\(d=2\\).\n\n\n\nIn the plot above, we have \\(n=10\\) data points in 3 dimensions. There is one linear function \\(\\mathbf{w} = \\begin{bmatrix} 2 \\\\ \\frac12 \\end{bmatrix}\\) that closely approximates the data and another linear function \\(\\mathbf{w} = \\begin{bmatrix} \\frac12 \\\\ 0 \\end{bmatrix}\\) that poorly approximates the data.\n\n\nMean Squared Error\nSince the output of \\(f\\) is still a single real number, we do not have to change the loss function. However, we can use our linear algebra notation to write the mean squared error in an elegant way.\nLet \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) be the data matrix where the \\(i\\)th row is \\((\\mathbf{x}^{(i)})^\\top\\). Similarly, let \\(\\mathbf{y} \\in \\mathbb{R}^n\\) be the target vector where the \\(i\\)th entry is \\(y^{(i)}\\). We can then write the mean squared error loss as \\[\n\\mathcal{L}(\\mathbf{w}) = \\frac1{n} \\| \\mathbf{X w - y} \\|_2^2.\n\\]\n\n\nExact Optimization\nJust like computing the derivative and setting it to \\(0\\), we can compute the gradient and set it to the zero vector \\(\\mathbf{0} \\in \\mathbb{R}^d\\). In mathematical notation, we will set \\(\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}^*) = \\mathbf{0}\\) and solve for \\(\\mathbf{w}^*\\). The intuition is that such a point is a local minimum in every direction; that is, we cannot improve the loss by moving in any of the dimensions. Since the loss is convex (i.e., there can be only one minima), such a point is the unique global minimum and achieves the optimal loss.\nAs you may recall from multivariate calculus, the gradient \\(\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w})\\) is simply a vector with the same dimension as \\(\\mathbf{w}\\); the value in the \\(i\\)th dimension is \\(\\frac{\\partial \\mathcal{L}(\\mathbf{w})}{\\partial w_i}\\).\nLet’s compute this quantity \\[\n\\begin{align}\n\\frac{\\partial \\mathcal{L}(\\mathbf{w})}{\\partial w_i}\n&= \\lim_{\\Delta \\to 0} \\frac{\\mathcal{L}(\\mathbf{w}+\\Delta \\mathbf{e}_i) - \\mathcal{L}(\\mathbf{w})}{\\Delta}\n\\\\&=\\lim_{\\Delta \\to 0} \\frac{\\| \\mathbf{X} (\\mathbf{w}+\\Delta \\mathbf{e}_i) - \\mathbf{y}\\|^2 - \\| \\mathbf{X} \\mathbf{w} - \\mathbf{y}\\|^2 }{\\Delta}.\n\\end{align}\n\\] Let \\(\\mathbf{a}, \\mathbf{b}\\) be two vectors in the same dimensional space. We have \\(\\|\\mathbf{a} + \\mathbf{b} \\|^2 = \\langle \\mathbf{a} + \\mathbf{b} , \\mathbf{a} + \\mathbf{b} \\rangle = \\| \\mathbf{a} \\|^2 + 2\\langle \\mathbf{a}, \\mathbf{b} \\rangle + \\| \\mathbf{b}\\|^2\\), where we foiled to reach the final equality, and used that \\(\\langle \\mathbf{a}, \\mathbf{b} \\rangle = \\langle \\mathbf{a}, \\mathbf{b} \\rangle\\). By letting \\(\\mathbf{a} = \\mathbf{X w - b}\\) and \\(\\mathbf{b} = \\Delta \\mathbf{X} \\mathbf{e}_i\\), we reach \\[\n\\begin{align}\n\\frac{\\partial \\mathcal{L}(\\mathbf{w})}{\\partial w_i}\n&=\\lim_{\\Delta \\to 0} \\frac{\\| \\mathbf{X} \\mathbf{w} - \\mathbf{y}\\|^2\n+ 2 \\langle \\mathbf{X} \\mathbf{w} - \\mathbf{y}, \\Delta \\mathbf{X e}_i \\rangle\n+\\|\\Delta \\mathbf{X e}_i\\|^2\n- \\| \\mathbf{X} \\mathbf{w} - \\mathbf{y}\\|^2 }{\\Delta}.\n\\\\&= \\lim_{\\Delta \\to 0} \\frac{2 \\Delta \\langle \\mathbf{X} \\mathbf{w} - \\mathbf{y}, \\mathbf{X e}_i \\rangle\n+\\Delta^2 \\|\\mathbf{X e}_i\\|^2}{\\Delta}\n\\\\&= \\lim_{\\Delta \\to 0} 2 \\langle \\mathbf{X} \\mathbf{w} - \\mathbf{y}, \\mathbf{X e}_i \\rangle + \\Delta \\|\\mathbf{X e}_i\\|^2\n\\\\&=\\langle \\mathbf{X} \\mathbf{w} - \\mathbf{y}, \\mathbf{X e}_i \\rangle.\n\\end{align}\n\\] Let \\(\\mathbf{X}_i = \\mathbf{Xe}_i\\) be the \\(i\\)th row of \\(\\mathbf{X}\\). Then, the full gradient is given by \\[\n\\begin{align}\n\\nabla_\\mathbf{w}\\mathcal{L}(\\mathbf{w})\n= 2 \\begin{bmatrix}\n\\mathbf{X}_1^\\top (\\mathbf{X} \\mathbf{w} - \\mathbf{y}) \\\\ \\mathbf{X}_2^\\top (\\mathbf{X} \\mathbf{w} - \\mathbf{y}) \\\\ \\vdots\n\\end{bmatrix}\n\\end{align}\n= 2 \\mathbf{X}^\\top (\\mathbf{X} \\mathbf{w} - \\mathbf{y}).\n\\]\nBy the convexity of the loss function \\(\\mathcal{L}\\), we know that \\(\\nabla_\\mathbf{w}\\mathcal{L}(\\mathbf{w}^*)=0\\) at the optimal weights \\(\\mathbf{w}^*\\). Solving for \\(\\mathbf{w}^*\\) yields \\[\n\\begin{align}\n\\nabla_\\mathbf{w}\\mathcal{L}(\\mathbf{w}^*)\n&= 2 \\mathbf{X}^\\top (\\mathbf{X} \\mathbf{w}^* - \\mathbf{y})\n= 0\n\\\\ \\Leftrightarrow\n\\mathbf{X}^\\top \\mathbf{X} \\mathbf{w}^* &= \\mathbf{X}^\\top \\mathbf{y}\n\\\\ \\Leftrightarrow\n\\mathbf{w}^* &= (\\mathbf{X}^\\top \\mathbf{X})^+ \\mathbf{X}^\\top \\mathbf{y}\n\\end{align}\n\\] where \\((\\cdot)^+\\) is the pseudoinverse i.e., \\((\\mathbf{M})^+ \\mathbf{M} = \\mathbf{I}\\) for all symmetric matrices \\(\\mathbf{M}\\).\nQuestion: Is the pseudoinverse also defined for non-symmetric square matrices?"
  },
  {
    "objectID": "notes/02_LinearRegression.html#empirical-risk-minimization",
    "href": "notes/02_LinearRegression.html#empirical-risk-minimization",
    "title": "Linear Regression and Optimization",
    "section": "Empirical Risk Minimization",
    "text": "Empirical Risk Minimization\nWe have now seen how to fit a linear function to data using the mean squared error loss. However, we have not given a satisfying answer to the question:\n\nWhy use mean squared error as our loss function?\n\n\nSo far, our answer has been that the quadratic function is differentiable (which we use to find the optimal solution), and that it naturally penalizes predictions which are farther away more. The first point is one of convenience and, a priori, should not be particularly persuasive. The second seems somewhat arbitrary, why penalize at a quadratic rate rather than an e.g., quartic rate? We’ll now consider a more compelling answer.\nOn our way to the answer, let’s take a step back and consider another question:\n\nWhy fit the data with a linear function?\n\n\nWell, we may do so when we expect the data truly has a linear relationship with the labels. To make things interesting, we will assume that there is random noise added to the labels, but that this noise is mean-centered so that, on average, the labels come from the linear model. Concretely, we observe some point \\(\\mathbf{x}\\) with a label that comes from a linear model \\(\\mathbf{w}^*\\) but with added noise, i.e., \\[\ny= \\langle \\mathbf{w}^*, \\mathbf{x} \\rangle + \\eta.\n\\] We will model this noise as distributed from a normal distribution, i.e., \\(\\eta \\sim \\mathcal{N}(0, \\sigma^2)\\) for some unknown standard deviation \\(\\sigma\\). (To justify this choice, we imagine the noise as a sum of random variables from some other distribution(s) which, by the law of large numbers, will follow the normal distribution when the sum contains sufficiently many terms.)\nRecall that the goal of our empirical risk minimization strategy is to find the function which most closely aligns with the data, or, put differently, we want the function that most likely generated the data we observed. In order to compute this likelihood, we will use the probability density function of the normal distribution: The probability we observe a random variable \\(y\\) drawn from a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) is given by \\[\n\\frac1{\\sqrt{2\\pi \\sigma}} \\exp\\left( - \\frac{(y- \\mu)^2}{2\\sigma^2} \\right).\n\\] If the noisy linear model \\(\\mathbf{w}\\) did generate the training data \\((\\mathbf{x}^{(i)}, y^{(i)})\\), then the expectation of the generation would be \\(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle\\). Then, combined with the assumption that the training data was drawn independently, the probability of observing the training data is given by the product of the probabilities of each individual observation: \\[\n\\prod_{i=1}^n\n\\frac1{\\sqrt{2\\pi \\sigma}} \\exp\\left( - \\frac{(y^{(i)}- \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)^2}{2\\sigma^2} \\right).\n\\] Our goal is to find the function \\(\\mathbf{w}\\) that maximizes this likelihood i.e., \\[\n\\begin{align}\n&{\\arg\\max}_{\\mathbf{w} \\in \\mathbb{R}^d}\n\\prod_{i=1}^n\n\\frac{1}{\\sqrt{2\\pi \\sigma}} \\exp\\left( - \\frac{(y^{(i)}- \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)^2}{2\\sigma^2} \\right)\n\\notag \\\\\n&= {\\arg\\min}_{\\mathbf{w} \\in \\mathbb{R}^d}\n- \\log \\left(\n\\frac{1}{\\sqrt{2\\pi \\sigma}} \\exp\\left(\\sum_{i=1}^n - \\frac{(y^{(i)}- \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)^2}{2\\sigma^2} \\right)\\right)\n\\notag \\\\\n&= {\\arg\\min}_{\\mathbf{w} \\in \\mathbb{R}^d}\n- \\sum_{i=1}^n - (y^{(i)}- \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)^2.\n\\end{align}\n\\] Here, we used the following facts: maximizing an objective is equivalent to minimizing the negative of that objective, the logarithmic function is monotonically increasing so minimizing the likelihood is equivalent to minimizing the log-likelihood, the product of exponentials is the exponential of the sum, and removing a constant scalar factor or additive constant does not change the minimum.\nThe punchline is that the function \\(\\mathbf{w}\\) that maximizes the likelihood of observing the training data is the same function that minimizes the mean squared error loss. This is a powerful result that justifies our use of the mean squared error loss."
  },
  {
    "objectID": "notes/02_LinearRegression.html#looking-forward",
    "href": "notes/02_LinearRegression.html#looking-forward",
    "title": "Linear Regression and Optimization",
    "section": "Looking Forward",
    "text": "Looking Forward\nWhile we have seen the benefits of exactly optimizing linear regression functions, there are several limitations that we will address.\nComputational Complexity We saw (and you will prove) that the exact solution to linear regression is given by \\(\\mathbf{w}^* = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\\). This requires building the matrix \\(\\mathbf{X}^\\top \\mathbf{X}\\), which takes \\(O(nd^2)\\) time, and then inverting it, which takes \\(O(d^3)\\). When we have a large number of data points \\(n\\) and/or a large number of features \\(d\\), this can be prohibitively expensive.\nFunction Class Misspecification We have assumed that the data has a linear relationship (or close to linear relationship) with the labels. What happens when this is not true? That is, even the best linear function gives a poor approximation?"
  },
  {
    "objectID": "notes/03_GradientDescent.html",
    "href": "notes/03_GradientDescent.html",
    "title": "Gradient Descent and Polynomial Regression",
    "section": "",
    "text": "Today, we continue our discussion of supervised learning, where we have labeled training data and our goal is to train a model that accurately predicts the labels of unseen testing data. Recall that our general approach to supervised learning is to use empirical risk minimization: We focus on a class of models, define a loss function that quantifies how accurately a particular model explains the training data, and search for a model with low loss.\nLast week, we considered the class of linear models, i.e., the prediction is a weighted linear combination of the input features. We chose to measure loss via mean squared error, a choice both rooted in convenience and a compelling modeling assumption (if the data is generated by a linear process with Gaussian, then the mean squared error is the maximum likelihood estimator). In order to find the best parameters of the linear model, we used our knowledge of gradients to exactly compute the parameters that minimize the mean squared error loss.\nThis week, we will address two of the nagging issues with computing the best parameters of a linear model. We begin with the issue of runtime; computing the optimal parameters requires building a large matrix and inverting it, which can be computationally expensive. We will now see how we can use gradient descent to speed up this process.\n\nGradient Descent\nThe mean squared error of a linear model is particularly well-behaved because it is convex i.e., there is a single minimum. Previously, we computed the parameters where the gradient is 0, which, by convexity, immediately gave us the single optimal point. However, we could instead use a more relaxed approach; rather than jumping immediately to the best parameters, we can iterate towards better parameters by taking steps towards lower loss.\n\n\n\nGradient descent is an iterative method for moving in the direction of steepest descent. Concretely, the process produces a sequence of parameters \\(\\mathbf{w}^{(1)}, \\mathbf{w}^{(2)}, \\ldots\\). At each step, we compute the direction of steepest ascent i.e., the gradient of the loss function with respect to each parameter. The gradient quantifies how the loss function responds as we tweak each parameter. If the partial derivative is positive (increasing the parameter increases the loss), then we will want to decrease the parameter. Analogously, if the partial derivative is negative (increasing the parameter decreases the loss), then we will want to increase the parameter. In both cases, we are moving in the direction away from the gradient. Hence, we reach the next parameter vector by subtracting the gradient from the current parameter vector: \\[\n\\begin{align*}\n\\mathbf{w}^{(t+1)} \\gets \\mathbf{w}^{(t)} - \\alpha \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}^{(t)}),\n\\end{align*}\n\\] where \\(\\alpha\\) is a small positive constant called the step size or learning rate.\nNotice that, for one, this approach stops when we reach a point where the gradient is 0, i.e., we have reached a local minimum.\nBeyond the stopping condition, why does this work? Consider the one dimensional setting. The derivative of the loss function is \\[\n\\begin{align*}\n\\mathcal{L}'(w) = \\lim_{\\Delta \\to 0} \\frac{\\mathcal{L}(w + \\Delta) - \\mathcal{L}(w)}{\\Delta}.\n\\end{align*}\n\\] so, for small \\(\\Delta\\), we can approximate the loss function as \\[\n\\begin{align*}\n\\mathcal{L}(w+\\Delta) - \\mathcal{L}(w) &\\approx \\mathcal{L}'(w) \\cdot \\Delta \\\\\n\\end{align*}\n\\] We want \\(\\mathcal{L}(w+\\Delta)\\) to be smaller than \\(\\mathcal{L}(w)\\), so we want \\(\\mathcal{L}'(w) \\Delta &lt; 0\\). This can be achieved by setting \\(\\Delta = -\\eta \\mathcal{L}'(w)\\), where \\(\\eta\\) is a small positive constant. Then \\(w^{(t+1)} = w^{(t)} - \\eta \\mathcal{L}'(w^{(t)})\\) is a step in the direction of descent.\nIn the multi-dimensional setting, the partial derivative of the loss function with respect to each parameter is given by the gradient: \\[\n\\frac{\\partial \\mathcal{L}}{\\partial w_i} = \\lim_{\\Delta \\to 0} \\frac{\\mathcal{L}(\\mathbf{w} + \\Delta \\mathbf{e}_i) - \\mathcal{L}(\\mathbf{w})}{\\Delta},\n\\] where \\(\\mathbf{e}_i\\) is the \\(i\\)-th standard basis vector. Then, for small \\(\\Delta\\), we can approximate the loss function as \\[\n\\mathcal{L}(\\mathbf{w} + \\Delta \\mathbf{e}_i) - \\mathcal{L}(\\mathbf{w}) \\approx \\frac{\\partial \\mathcal{L}}{\\partial w_i} \\cdot \\Delta\n= \\langle \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}), \\Delta \\mathbf{e}_i \\rangle.\n\\] For a general vector \\(\\mathbf{v}\\), we can write \\[\n\\mathcal{L}(\\mathbf{w} + \\Delta \\mathbf{v}) - \\mathcal{L}(\\mathbf{w}) \\approx \\langle \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}), \\Delta \\mathbf{v} \\rangle.\n\\] If we want to move in the direction of steepest descent, we can set \\(\\Delta \\mathbf{v} = -\\eta \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w})\\), where \\(\\eta\\) is a small positive constant. Then, we have \\(\\mathcal{L}(\\mathbf{w} - \\eta \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w})) - \\mathcal{L}(\\mathbf{w}) \\approx -\\eta \\langle \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}), \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}) \\rangle = -\\eta \\|\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w})\\|^2\\). Why is this the right choice? Well, recall for any vectors \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\), we have \\(\\langle \\mathbf{a}, \\mathbf{b} \\rangle = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos(\\theta)\\), where \\(\\theta\\) is the angle between the two vectors. The largest value of \\(\\cos(\\theta)\\) is \\(1\\), which occurs when the two vectors are in the same direction. Notice we achieve the largest magnitude of the inner product when we take the step in the direction of the gradient, i.e., \\(- \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w})\\).\nFor linear models, we already know the gradient of the mean squared error loss: \\[\n\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}) = \\frac2{n} \\mathbf{X}^\\top (\\mathbf{X w - y}).\n\\] In contrast to the \\(O(nd^2 + d^3)\\) time required to compute the exact solution, we can now compute the gradient in \\(O(nd)\\) time, as long as we restrict ourselves to matrix-vector multiplications rather than matrix-matrix multiplications. The final time complexity of gradient descent is \\(O(T nd)\\), where \\(T\\) is the number of iterations of gradient descent.\nWhile we have achieved a significant speedup, \\(O(T nd)\\) could still be prohibitively large when we have a large number of data points \\(n\\) and/or a large number of features \\(d\\). Our solution will be a stochastic approach, where we only use a small random subset of the data to compute the gradient.\n\n\nStochastic Gradient Descent\nOur approach will be similar to gradient descent, except now we will compute the gradient using only the data in the batch. For the mean squared error loss, we can write the loss function for a random subset \\(S\\) of the data as \\[\n\\mathcal{L}_S(\\mathbf{w}) = \\frac1{|S|} \\sum_{i \\in S} (f(\\mathbf{x}^{(i)}) - y^{(i)})^2.\n\\] Then, for our linear model, the gradient of the loss function with respect to the parameters \\(\\mathbf{w}\\) is given by \\[\n\\nabla_\\mathbf{w} \\mathcal{L}_S(\\mathbf{w}) = \\frac2{|S|} \\mathbf{X}_S^\\top (\\mathbf{X}_S \\mathbf{w} - \\mathbf{y}_S),\n\\] where \\(\\mathbf{X}_S\\) is the data matrix for the subset \\(S\\) and \\(\\mathbf{y}_S\\) is the target vector for the subset \\(S\\). One iteration of stochastic gradient descent takes time \\(O(|S|d)\\), which can be much faster than the \\(O(nd)\\) time required to compute the gradient for the full dataset.\n\n\nAdaptive Step Sizes\nThe step size \\(\\alpha\\) is a crucial hyperparameter in gradient descent. If \\(\\alpha\\) is too small, then the algorithm will take a long time to converge because it will take small steps towards the minimum. If \\(\\alpha\\) is too large, then the algorithm may overshoot the minimum and diverge by repeatedly moving in the right direction but by too much. Instead, we want to choose a step size \\(\\alpha\\) that is just right, allowing us to make progress towards the minimum without overshooting.\n\n\n\nThere are several strategies for choosing the step size:\n\nWhen searching manually, we often exponentially increase and decrease the step size i.e., multiply by a factor of \\(2\\) or \\(1/2\\). If the loss consistently improves over several iterations of gradient descent, then we try increasing the step size; if the loss is unstable, then we try decreasing the step size.\nLearning rate schedules offer a more automated approach, where we start with a large step size and then decrease it over time. This is often done by multiplying the step size by a factor less than \\(1\\) after each iteration. The intuition is that we want to take large steps at the beginning to quickly find a good region of the parameter space, and then take smaller steps so as to not overshoot the minima as we get closer.\nAn even more automated approach is to use an adaptive learning rate, where we adjust the step size based on the gradient. If the gradient is large, then we can decrease the step size to avoid overshooting; if the gradient is small, then we can increase the step size to speed up convergence. One implementation of this idea is as follows: \\[\n\\alpha^{(t+1)} \\gets \\frac{\\alpha^{(t)}}{(\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}^{(t)}))^2}.\n\\] Notice that the division is element-wise, so we are adjusting the step size for each parameter individually based on the squared partial derivative of that parameter.\n\nIn addition the step size, the direction of each step is also important.\n\n\nMomentum\nThe idea of gradient descent is to converge to a local minimum of the loss function, but things can go wrong even if we have the right step size: The gradient may not point in the direction of the minima if, for example, the loss function is not symmetric. The plot illustrates this issue for a convex loss function on two parameters \\({w}_1\\) and \\({w}_2\\), where the loss function is given by level sets. In the plot, a standard gradient descent approach takes many steps but the directions cancel out, resulting in a zig-zag pattern that slows convergence.\n\n\n\nOur solution is to keep track of the direction we have been moving in and use that to inform our next step. This idea, called momentum, retains a running average of the gradients, which allows us to smooth out the direction of the steps. We can think of momentum as a ball rolling down a hill, even when the ball is pushed left or right, it will continue to roll downwards. An implementation of momentum is as follows: \\[\n\\begin{align}\n\\mathbf{m}^{(t+1)} &= \\beta \\mathbf{m}^{(t)} + (1 - \\beta) \\nabla_\\mathbf{w} \\mathcal{L}_S(\\mathbf{w}^{(t)}) \\\\\n\\mathbf{w}^{(t+1)} &= \\mathbf{w}^{(t)} - \\alpha \\mathbf{m}^{(t+1)},\n\\end{align}\n\\] where \\(\\beta\\) is a hyperparameter that controls the amount of history we keep in the momentum vector \\(\\mathbf{m}^{(t)}\\)."
  },
  {
    "objectID": "notes/06_KernelMethods.html",
    "href": "notes/06_KernelMethods.html",
    "title": "Kernel Methods",
    "section": "",
    "text": "Many of the methods we have discussed so far are linear in nature, i.e., we make predictions based on a linear combination of the input features. Next, we will explore non-linear methods, specifically kernel methods, and neural networks. Both are closely related to feature transformations, which we have already discussed in the context of linear regression and support vector machines.\n\n\\(k\\)-Nearest Neighbors\nWe’ll start with the \\(k\\)-nearest neighbors algorithm, which is a simple yet powerful non-parametric method for classification and regression. As usual in the supervised learning setting, we have \\(n\\) labeled training examples \\(\\{(\\mathbf{x}^{(i)}, y^{(i)})\\}_{i=1}^n\\), where \\(\\mathbf{x}^{(i)} \\in \\mathbb{R}^d\\) is the feature vector and \\(y^{(i)} \\in \\mathbb{R}\\) is the label. When we get a new unlabeled example \\(\\mathbf{x}\\), we find the \\(k\\) training examples that are closest to \\(\\mathbf{x}\\), then we predict the label of \\(\\mathbf{x}\\) as the average of the labels of these \\(k\\) neighbors.\nWe can visualize the \\(k\\)-nearest neighbors predictions as a kind of Voronoi diagram, where each point in the feature space is assigned to the label of its nearest neighbor(s).\n[image here]\nWe typically measure closeness via cosine similarity i.e., the cosine of the angle between two vectors. Consider two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{w}\\), the cosine similarity is: \\[\n\\cos(\\theta) = \\frac{\\langle \\mathbf{x}, \\mathbf{w} \\rangle}{\\|\\mathbf{x}\\| \\|\\mathbf{w}\\|}\n\\] where \\(\\theta\\) is the angle between the two vectors.\n\n\n\nProof of Cosine Identity\n\nConsider the triangle with vertices at the origin, \\(\\mathbf{x}\\), and \\(\\mathbf{w}\\). The lengths of the sides are \\(a = \\|\\mathbf{x}\\|_2\\), \\(b = \\|\\mathbf{w}\\|_2\\), and \\(c = \\|\\mathbf{x} - \\mathbf{w}\\|_2\\). The law of cosines tells us that: \\[\n\\cos(\\theta) = \\frac{a^2 + b^2 - c^2}{2ab}.\n\\] Then \\(a^2 + b^2 - c^2 = \\|\\mathbf{x}\\|_2^2 + \\|\\mathbf{w}\\|_2^2 - \\|\\mathbf{x} - \\mathbf{w}\\|_2^2\\). Since \\(\\|\\mathbf{x} - \\mathbf{w}\\|_2^2 = \\|\\mathbf{x}\\|_2^2 + \\|\\mathbf{w}\\|_2^2 - 2\\langle \\mathbf{x}, \\mathbf{w} \\rangle\\), we have \\[\n\\cos(\\theta) = \\frac{2 \\langle \\mathbf{x}, \\mathbf{w} \\rangle}{ 2\\|\\mathbf{x}\\|_2 \\|\\mathbf{w}\\|_2},\n\\] and the identity follows.\n\n\nThe cosine similarity identity tells us that the following are equivalent:\n- Large cosine similarity \\(\\cos(\\theta)\\).\n- Small angle \\(\\theta\\).\n- Small distance \\(\\|\\mathbf{x} - \\mathbf{w}\\|_2\\).\nLet’s apply the \\(k\\)-nearest neighbors algorithm to the MNIST dataset, which contains images of handwritten digits. Each image is a \\(28 \\times 28\\) pixel grayscale image, which we can flatten into a vector of length \\(784\\). When we compute the cosine similarity between two images, we are essentially measuring how similar the two images are based on their pixel values in each dimension. A search for the nearest neighbors will find the images that are closest in terms of pixel values, which often corresponds to similar handwriting styles.\n[image here]\nWhile quite accurate out of the box, it’s difficult to improve its accuracy because we do not have parameters to tune. Instead, we can use feature transformations to more expressively represent the data in higher dimensions.\n\n\nKernel Trick\nWe can apply feature transformations to the input data to create a new representation that captures more complex relationships. Let \\(\\phi: \\mathbb{R}^d \\to \\mathbb{R}^m\\) be a feature transformation that maps the inputs to a higher-dimensional space.\n\n\n\nFor most transformations, \\(m\\) is very large, and explicitly computing the transformation \\(\\phi(\\mathbf{x}^{(i)})\\) is infeasible. Fortunately, we can use the kernel trick to compute the inner product without explicitly computing the transformation. The kernel trick allows us to define a kernel function \\(K: \\mathbb{R^d} \\times \\mathbb{R}^d \\to \\mathbb{R}\\) such that \\[\n\\begin{align}\nK(\\mathbf{x}^{(i)}, \\mathbf{x}^{(j)}) = \\langle \\phi(\\mathbf{x}^{(i)}), \\phi(\\mathbf{x}^{(j)}) \\rangle.\n\\end{align}\n\\]\nFor many years, I was confused about why the kernel trick works. Doesn’t computing the inner product require us to apply the transformation \\(\\phi\\)?\nLet’s see two examples:\nPolynomial Kernel: Consider the polynomial kernel \\(K(\\mathbf{x}, \\mathbf{x}') = (\\langle \\mathbf{x}, \\mathbf{x}' \\rangle + 1)^p\\) for \\(p \\in \\mathbb{N}\\). The explicit transformation \\(\\phi\\) when \\(p=2\\) and \\(d=3\\) is given by: \\[\n\\phi(\\mathbf{x}) = \\begin{bmatrix}\n1 \\\\\n\\sqrt{2} x_1 \\\\\n\\sqrt{2} x_2 \\\\\n\\sqrt{2} x_3 \\\\\nx_1^2 \\\\\nx_2^2 \\\\\nx_3^2 \\\\\n\\sqrt{2} x_1 x_2 \\\\\n\\sqrt{2} x_1 x_3 \\\\\n\\sqrt{2} x_2 x_3 \\\\\n\\end{bmatrix}\n\\] However, we can efficiently compute the inner product without explicitly computing \\(\\phi\\). For example, \\[\n\\begin{align}\nK(\\mathbf{x}, \\mathbf{w}) &= (\\langle \\mathbf{x}, \\mathbf{w} \\rangle + 1)^2\n\\\\&= (1 + x_1 x_1' + x_2 x_2' + x_3 x_3')^2\n\\\\&= 1 + 2x_1 x_1' + 2x_2 x_2' + 2x_3 x_3' + x_1^2 x_1'^2 + x_2^2 x_2'^2\n\\\\& + x_3^2 x_3'^2 + 2x_1 x_2 x_1' x_2' + 2x_1 x_3 x_1' x_3' + 2x_2 x_3 x_2' x_3'\n\\\\&= \\langle \\phi(\\mathbf{x}), \\phi(\\mathbf{x}') \\rangle.\n\\end{align}\n\\]\nGaussian Kernel: Consider the Gaussian kernel \\(K(\\mathbf{x}, \\mathbf{x}') = e^{-\\frac{\\|\\mathbf{x} - \\mathbf{x}'\\|_2^2}{2\\sigma^2}}\\) for some \\(\\sigma &gt; 0\\). As shown here, the Gaussian kernel can be expressed as an infinite series. Instead of explicitly computing the transformation \\(\\phi\\), we can compute the kernel in \\(O(d)\\) time by computing the inner product \\(\\langle \\mathbf{x}, \\mathbf{x}' \\rangle\\), and then applying the exponential function with a scaling.\nWe have seen how to apply the kernel trick to the \\(k\\)-nearest neighbors algorithm, which allows us to compute the inner product in a higher-dimensional space without explicitly computing the feature transformation. Let’s see how we can apply the kernel trick to a more familiar tool: multi-class classification.\n\n\nReparameterization Trick\nLet \\(q\\) be the number of classes. We will learn \\(q\\) weight vectors \\(\\mathbf{w}^{(1)}, \\ldots, \\mathbf{w}^{(q)} \\in \\mathbb{R}^d\\). Our final prediction is the class with the highest value (we previously normalized these outputs so that we could interpret them as probabilities): \\[\n\\arg \\max_{\\ell \\in [q]} \\langle \\mathbf{w}^{(\\ell)}, \\mathbf{x} \\rangle.\n\\] Equivalently, we can view the problem as a similarity search, where we want to find the class with smallest inner product: \\[\n\\arg \\min_{\\ell \\in [q]} \\langle \\mathbf{w}^{(\\ell)}, \\mathbf{x} \\rangle.\n\\]\nIn the context of MNIST, the weight vectors look something like this:\n[image here]\nWhile vaguely resembling the digits, they are not very interpretable, and it’s unclear how much they capture the underlying structure of the data. We would really like to represent the data in a more expressive way without explicitly computing the feature transformation, similar to the kernel trick we applied to the \\(k\\)-nearest neighbors algorithm.\nFor notational simplicity, consider binary logistic regression, where we have a single weight vector \\(\\mathbf{w} \\in \\mathbb{R}^d\\). The optimization problem is to find \\[\n\\arg \\min_{\\mathbf{w} \\in \\mathbb{R}^{d}} \\mathcal{L}(\\sigma(\\mathbf{Xw}), \\mathbf{y})\n\\] \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) is the input data matrix, \\(\\mathbf{y} \\in \\{0,1\\}^n\\) is the label vector, \\(\\mathcal{L}\\) is the binary cross-entropy loss, and \\(\\sigma\\) is the entrywise softmax function. But it’s unclear how to apply the kernel trick here, since we are not computing the inner product between two feature vectors, but rather between a feature vector and a weight vector.\nReparameterization Trick: We can equivalently represent the weight vector as a linear combination of the training examples: \\[\n\\mathbf{w} = \\sum_{i=1}^n \\alpha_i \\mathbf{x}^{(i)}\n\\] for some coefficients \\(\\alpha_i \\in \\mathbb{R}\\).\n\n\n\nProof of Reparameterization Trick\n\nSuppose for contradiction that \\(\\mathbf{w}\\) is not in the span of the rows of \\(\\mathbf{X}\\). Then we can write \\(\\mathbf{w} = \\mathbf{v} + \\mathbf{u}\\), where \\(\\mathbf{v}\\) is in the span of the rows of \\(\\mathbf{X}\\) and \\(\\mathbf{u}\\) is orthogonal to the span of the rows of \\(\\mathbf{X}\\). But then \\(\\mathbf{X w} = \\mathbf{Xv} + \\mathbf{Xu} = \\mathbf{Xv}\\), since \\(\\mathbf{Xu} = 0\\), so it suffices to optimize over \\(\\mathbf{v}\\).\n\n\nThe reparameterization trick tells us that we can equivalently represent the weight vector in the span of the rows of the input data matrix \\(\\mathbf{X}\\).\n\n\n\nWith the reparameterization trick in hand, we can represent the weight vector as \\(\\mathbf{X}^\\top \\mathbf{v}\\). The optimization problem becomes: \\[\n\\arg \\min_{\\mathbf{v} \\in \\mathbb{R}^{n}} \\mathcal{L}(\\sigma(\\mathbf{X X}^\\top \\mathbf{v}), \\mathbf{y}).\n\\]\nNow, we can apply the kernel trick to compute the inner products of \\(K(\\mathbf{X X^\\top})\\). At inference time, we can similarly use the kernel trick to compute the inner product between the input vector \\(\\mathbf{x}\\) and the weight vector \\(\\mathbf{X}^\\top \\mathbf{v}\\).\nThe reparameterization trick allows us to expressively represent data without explicitly computing the feature transformation. We can also apply the trick to other linear models such as linear regression.\nWhile they allow us to expressively represent data without explicitly computing the feature transformation, the kernel and reparameterization tricks are still computationally expensive: We need to compute the \\(n \\times n\\) kernel matrix \\(K(\\mathbf{X}, \\mathbf{X})\\).\nWe will next see how neural networks allow us to automatically learn feature transformations, while basically maintaining the training and inference efficiency of linear models."
  },
  {
    "objectID": "notes/04_Probability.html",
    "href": "notes/04_Probability.html",
    "title": "Probability",
    "section": "",
    "text": "Instead of predicting a continuous value, there are many applications where we want to predict a discrete value. For example, we might want to predict whether an email is spam or not, whether a patient has a disease or not, or whether an image contains a cat or not. In these cases, we can use a classification model instead of a regression model.\nWe will review some basic concepts of probability that are useful for understanding classification models. We will also introduce Bayes’ Rule, and see how useful it can be for making predictions.\nLet \\(A\\) and \\(B\\) be two events. For example, \\(A\\) could be the event that it rains tomorrow, and \\(B\\) could be the event that I wear a raincoat tomorrow. We will use the following notation:\n- \\(\\Pr(A)\\) represents the probability that event \\(A\\) occurs,\n- \\(\\Pr(A \\cup B)\\) represents the probability that either event \\(A\\) or event \\(B\\) occurs,\n- \\(\\Pr(A \\cap B)\\) represents the probability that both events \\(A\\) and \\(B\\) occur,\n- \\(\\Pr(A | B)\\) represents the probability that event \\(A\\) occurs given that event \\(B\\) has occurred.\n\n\n\nWe can reason through several properties of probabilities:\nProbability Range The probability of an event is always between 0 and 1, i.e., \\(0 \\leq \\Pr(A) \\leq 1\\) for all events \\(A\\). An event with probability 0 never occurs, while an event with probability 1 is certain to occur.\nConditional Probabilities We can write the probability of both events \\(A\\) and \\(B\\) occurring in terms of conditional probabilities: \\[\\Pr(A \\cap B) = \\Pr(B) \\Pr(A | B)  = \\Pr(A) \\Pr(B | A).\\] This means that the probability of both events occurring is the product of the probability of one event and the conditional probability of the other event given that the first event has occurred.\nUnion of Events The probability that either event \\(A\\) or event \\(B\\) occurs is given by: \\[\\Pr(A \\cup B) = \\Pr(A) + \\Pr(B) - \\Pr(A \\cap B).\\] This means that the probability of either event occurring is the sum of the probabilities of each event minus the probability of both events occurring together (we subtract the event that both occur because it is counted twice in the sum).\nComplement Rule The complement of an event \\(A\\) is the event that \\(A\\) does not occur, denoted as \\(\\neg A\\). Since either \\(A\\) occurs or \\(\\neg A\\) occurs, we have that \\(Pr(A) + \\Pr(\\neg A) = 1\\). Rearranging this gives us that \\(\\Pr(\\neg A) = 1 - \\Pr(A)\\).\nIndependence Two events \\(A\\) and \\(B\\) are independent if the occurrence of one event does not affect the probability of the other event occurring. In this case, we have that \\(\\Pr(A \\cap B) = \\Pr(A) \\Pr(B)\\). Equivalently, \\(\\Pr(A | B) = \\Pr(A)\\) and \\(\\Pr(B | A) = \\Pr(B)\\), do you see why this follows?\nWe will often model random events with random variables. A random variable is a function that maps outcomes to real numbers. For example, we could define a random variable \\(X\\) that takes the outcome of a die roll and maps it to the number on the die. The probability distribution of a random variable describes the probabilities of each possible outcome. In our die example, if we roll a fair six-sided die, the probability distribution of the random variable \\(X\\) is given by \\(\\Pr(X = x) = \\frac{1}{6}\\) for \\(x \\in \\{1, 2, 3, 4, 5, 6\\}\\).\nBayes’ Rule is a particularly useful rule in machine learning: For any two events \\(A\\) and \\(B\\) with \\(\\Pr(B) &gt; 0\\), \\[\\Pr(A | B) = \\frac{\\Pr(B | A) \\Pr(A)}{\\Pr(B)}.\\]\nBased on what we have seen so far, can you prove Bayes’ Rule?\n\nMaximum A Posteriori (MAP) Estimation\nWhen we justified the mean squared error loss for regression, we said that it was the maximum likelihood estimate (MLE) of the parameters of a linear model. We can extend this idea to classification models as well. Suppose we have a binary classification problem, where we want to predict whether the random variable \\(Y=0\\) or \\(Y=1\\). We observe evidence \\(\\mathbf{X} = \\mathbf{x}\\), e.g., \\(\\mathbf{X}\\) is the random variable that takes on the value of the features of an email, and we want to predict whether the email is spam or not. We will compare the posteriors \\[\\Pr(Y = 1 | \\mathbf{X} = \\mathbf{x})\\] and \\[\\Pr(Y = 0 | \\mathbf{X} = \\mathbf{x})\\] to determine which class is more likely given the evidence.\nHowever, it’s not immediately clear how to compute these probabilities. Luckily, we can use Bayes’ Rule to rewrite the posteriors. Without loss of generality, consider the event that \\(Y = 1\\). Then \\[\n\\begin{align*}\n\\Pr(Y = 1 | \\mathbf{X} = \\mathbf{x})\n&= \\frac{\\Pr(\\mathbf{X} = \\mathbf{x} | Y = 1) \\Pr(Y = 1)}{\\Pr(\\mathbf{X} = \\mathbf{x})} \\\\\n&= \\frac{\\textnormal{likelihood} \\cdot \\textnormal{prior}}{\\textnormal{evidence}}\n\\end{align*}.\n\\]\nLet’s get some familiarity through a medical example. Suppose we have a medical test that can detect whether a patient has a particular disease. The disease is rare, affecting only 1% of the population. The test is unfortunately not perfect: it has a false positive rate of 5% and a false negative rate of 10%. Suppose \\(X=1\\) i.e., the test is positive. Is it more likely that the patient has the disease or not?\nIn this medical example, we were explicitly given the likelihood of the disease. What can we do when our only information is the labelled data?\n\n\nNaive Bayes Classifier\nThe Naive Bayes Classifier is a simple yet effective classification algorithm that uses Bayes’ Rule to make predictions. The key assumption of the Naive Bayes Classifier is that the features are conditionally independent given the class label. This means that the presence or absence of a feature does not affect the presence or absence of another feature, given the class label.\nLet’s see an example with a spam classifier. Suppose we have a dataset of emails, each labelled as either spam or not spam. We want to predict whether a new email is spam or not based on its features, such as the presence of certain words. In particular, let \\(\\mathbf{X} = (X_1, X_2, \\ldots, X_d)\\) be the features of the email, where \\(X_i\\) is a binary variable indicating whether the \\(i\\)th word is present in the email. Let \\(p_i^{(1)} = \\Pr(X_i=1 | Y=0)\\) be the probability that the \\(i\\)th word is present in a spam email, while \\(p_i^{(0)} = \\Pr(X_i=1 | Y=1)\\) be the probability that the \\(i\\)th word is present in a non-spam email. We will use the independence assumption to compute the likelihood of the features given the class label. For example, \\[\n\\Pr(\\mathbf{X} = (0, 1, 0, 0, 1) | Y = 1) =\n(1-p_1^{(1)}) p_2^{(1)} (1-p_3^{(1)}) (1-p_4^{(1)}) p_5^{(1)}.\n\\]\nBeyond the likelihood, we also need the prior probability of the class label. This is even easier to compute, e.g., we can simply compute the fraction of spam and non-spam emails in the training set. Then we can use Bayes’ Rule to determine whether the posterior probability of the email being spam is greater than the posterior probability of the email being non-spam.\nMore formally, we can use the Naive Bayes Classifier by\n\nComputing \\(\\Pr(Y = 1)\\) and \\(\\Pr(Y = 0)\\) from the training data.\nComputing the observed probabilities \\(\\Pr(X_i = 1 | Y = 1)\\) and \\(\\Pr(X_i = 1 | Y = 0)\\) for each feature \\(X_i\\) from the training data.\nComputing the likelihoods \\(\\Pr(\\mathbf{X} = \\mathbf{x} | Y=1)\\) and \\(\\Pr(\\mathbf{X} = \\mathbf{x} | Y=0)\\) using the independence assumption e.g., \\[\n\\Pr(\\mathbf{X} = \\mathbf{x} | Y=1) = \\prod_{i=1}^d \\Pr(X_i = x_i | Y=1).\n\\]\nUsing Bayes’ Rule to compute the posteriors, and predicting the class label \\(y \\in \\{0,1\\}\\) with the highest posterior probability: \\[\n\\Pr(Y = y | \\mathbf{X} = \\mathbf{x}) = \\frac{\\Pr(\\mathbf{X} = \\mathbf{x} | Y=y) \\Pr(Y=y)}{\\Pr(\\mathbf{X} = \\mathbf{x})}.\n\\]"
  },
  {
    "objectID": "notes/08_DecisionTrees.html",
    "href": "notes/08_DecisionTrees.html",
    "title": "Decision Trees and Boosting",
    "section": "",
    "text": "Feature transformations and neural networks are powerful tools for supervised learning, but they can be difficult to interpret. Today, we’ll consider a more intuitive model class called trees, which, until very recently, gave the best performance on many supervised learning tasks.\n\nDecision Trees\nDecision trees are a simple yet powerful model class that can be used for both classification and regression tasks. For simplicity, we’ll define them in terms of classification, but the same ideas apply to regression.\nDecision trees work by recursively partitioning the feature space into regions that are as homogeneous as possible with respect to the target variable.\n[image here 2D points to separate]\nThe tree structure is incredibly intuitive, and likely the most natural way to represent a decision-making process. At each internal node, the tree splits the data based on a feature and a threshold. The goal is to find the split that best (more on this later) separates the classes. In our example image, splitting at \\(x_1=.5\\) wouldn’t separate the circle from the square points, but splitting at \\(x_1=1.5\\) almost perfectly separates them.\n[example decision tree here]\nLike gradient descent, decision trees use a greedy approach to learning. As we build the tree, we recursively evaluate the data at each leaf, choosing the feature and threshold that best separates the classes. Let \\(\\ell\\) be a leaf after the split at the prior leaf, and \\(p^{(\\ell)}_c\\) be the proportion of points in leaf \\(\\ell\\) that are of class \\(c\\). Without loss of generality, suppose that \\(c=0\\) is the majority class. (Soon, we’ll consider weighted points, in which case we will define the weighted majority.) If we reach leaf \\(\\ell\\), we predict that the point belongs to the majority class.\nThe question is what split minimizes the loss of the resulting predictions. In particular, we’d like to minimize the expected loss of the predictions where the expectation is taken over the proportion of points that make it to the leaf \\(p^{(\\ell)}\\): \\[\n\\mathbb{E}_\\ell[\\mathcal{L}(\\ell)] = \\sum_\\ell \\mathcal{L}(\\ell) \\cdot p^{(\\ell)}.\n\\] There are several ways to define the loss of a leaf, but several common ones are:\nError Rate: The error rate is the proportion of points in the leaf that are not of the predicted majority class. That is, \\[\n\\mathcal{L}_\\text{error}(\\ell) = 1 - p^{(\\ell)}_0.\n\\]\nGini Impurity: The Gini impurity measures the probability of misclassifying a randomly chosen point from the leaf if we were to randomly assign it to a class according to the class proportions. That is, \\[\n\\mathcal{L}_\\text{Gini}(\\ell) = \\sum_{c} p^{(\\ell)}_c (1 - p^{(\\ell)}_c).\n\\] With a little algebra, we can rewrite this as: \\[\n\\mathcal{L}_\\text{Gini}(\\ell) = \\sum_{c} p^{(\\ell)}_c - \\left(p^{(\\ell)}_c\\right)^2 = 1 - \\sum_{c} \\left(p^{(\\ell)}_c\\right)^2.\n\\]\nInformation Gain: Like logistic regression, information gain uses the entropy of the leaf to measure the error. That is, \\[\n\\mathcal{L}_\\text{InfoGain}(\\ell) = H(\\ell) = -\\sum_{c} p^{(\\ell)}_c \\log(p^{(\\ell)}_c).\n\\] Since the entropy is a measure of uncertainty, we call the reduction in entropy from the prior leaf to the resulting leaves after the split the information gain.\nPreviously, we updated the parameters of our model to minimize the loss of our predictions. In decision trees, we search over all possible splits of the data to find the one that minimizes the expected loss. While this sounds expensive, in practice, there are a limited number of feasible splits.\nWe’ve discussed decision trees in terms of classification, but they can also be used for regression tasks. What prediction would a leaf make in a regression task? How could we measure the loss of this prediction?\nOn their own, decision trees are not particularly expressive. But, they can be boosted to create remarkably powerful models. A boosted model is a collection of weaker learners that together make the final prediction.\n\n\nAdaptive Boosting (AdaBoost)\nIn adaptive boosting, we iteratively train a model to improve the composite model we’ve built so far. For simplicity, we’ll explore adaptive boosting in the context of binary classification, but the method can be generalized to regression tasks.\nLet \\(f_t : \\mathbb{R}^d \\to \\{-1, 1\\}\\) be the model learned at iteration \\(t\\). The combined prediction is given by an ensemble model \\(F_t : \\mathbb{R}^d \\to \\mathbb{R}\\). The ensemble model is defined as a weighted sum of the predictions of the individual models: \\[\nF_t(x) = \\alpha_1 f_1(x) + \\alpha_2 f_2(x) + \\ldots + \\alpha_t f_t(x)\n= F_{t-1}(x) + \\alpha_t f_t(x),\n\\] where \\(\\alpha_k &gt; 0\\) is a weight that determines the importance of the model \\(f_k\\) in the ensemble.\nGiven \\(F_{t-1}\\), we are interested in learning a new model \\(f_t\\) and a new weight \\(\\alpha_t\\) that minimizes the loss of the composite model. We’ll define the loss as \\[\n\\mathcal{L}(F_t) = \\sum_{i=1}^n e^{-y^{(i)} F_t(\\mathbf{x}^{(i)})},\n\\] where \\(y^{(i)} \\in \\{-1, 1\\}\\) is the true label of the \\(i\\)-th training example. When the composite model output is the same sign as the true label, the term in the exponent is negative, and the contribution to the error is less than 1. When the composite model output is the opposite sign as the true label, the term in the exponent is positive, and the contribution to the loss is (potentially much) greater than 1.\nUsing the multiplication property of exponents, and the definition of \\(F_t\\), we can rewrite the loss as \\[\n\\mathcal{L}(F_t) = \\sum_{i=1}^n e^{-y^{(i)} (F_{t-1}(\\mathbf{x}^{(i)}) + \\alpha_t f_t(\\mathbf{x}^{(i)}))}\n= \\sum_{i=1}^n e^{-y^{(i)} F_{t-1}(\\mathbf{x}^{(i)})} e^{-y^{(i)} \\alpha_t f_t(\\mathbf{x}^{(i)})}.\n\\] Let \\(w^{(i)}_{t-1} = e^{-y^{(i)} F_{t-1}(\\mathbf{x}^{(i)})}\\) be the weight of the \\(i\\)th training example at iteration \\(t\\). Notice that these weights are independent of the new model \\(f_t\\) and the new weight \\(\\alpha_t\\). Then, partitioning the summation, we can write the error as \\[\n\\begin{align}\n\\mathcal{L}(F_t) &=\n\\sum_{i: y^{(i)} = f_t(\\mathbf{x}^{(i)})} w^{(i)}_{t-1} e^{-\\alpha_t} + \\sum_{i: y^{(i)} \\neq f_t(\\mathbf{x}^{(i)})} w^{(i)}_{t-1} e^{\\alpha_t}\n\\\\&= \\sum_{i=1}^n w^{(i)}_{t-1} e^{-\\alpha_t} + \\sum_{i: y^{(i)} \\neq f_t(\\mathbf{x}^{(i)})} w^{(i)}_{t-1} (e^{\\alpha_t} - e^{-\\alpha_t})\n\\\\&= \\sum_{i=1}^n w^{(i)}_{t-1} e^{-\\alpha_t} + (e^{\\alpha_t} - e^{-\\alpha_t}) \\sum_{i: y^{(i)} \\neq f_t(\\mathbf{x}^{(i)})} w^{(i)}_{t-1}.\n\\end{align}\n\\]\nWith \\(F_{t-1}\\) fixed, minimizing \\(\\mathcal{L}(F_t)\\) is equivalent to minimizing the weighted error of the new model \\(f_t\\) i.e., the sum of the weights for all misclassified points. Once we learn a decision tree \\(f_t\\) that minimizes the weighted error, we can find the optimal weight \\(\\alpha_t\\) by differentiating the loss with respect to \\(\\alpha_t\\) and setting it to zero. In particular, we have \\[\n\\frac{\\partial \\mathcal{L}(F_t)}{\\partial \\alpha_t}\n= - \\sum_{i : y^{(i)} = f_t(\\mathbf{x}^{(i)})} w^{(i)}_{t-1} (-e^{-\\alpha_t})\n+ \\sum_{i : y^{(i)} \\neq f_t(\\mathbf{x}^{(i)})} w^{(i)}_{t-1} e^{\\alpha_t}\n= 0\n\\] which tells us that \\[\ne^{-\\alpha_t} \\sum_{i : y^{(i)} = f_t(\\mathbf{x}^{(i)})} w^{(i)}_{t-1}\n= e^{\\alpha_t} \\sum_{i : y^{(i)} \\neq f_t(\\mathbf{x}^{(i)})} w^{(i)}_{t-1}.\n\\] Taking the logarithm of both sides, we have: \\[\n-\\alpha_t + \\log\\left(\\sum_{i : y^{(i)} = f_t(\\mathbf{x}^{(i)})} w^{(i)}_{t-1}\\right)\n= \\alpha_t + \\log\\left(\\sum_{i : y^{(i)} \\neq f_t(\\mathbf{x}^{(i)})} w^{(i)}_{t-1}\\right).\n\\] Rearranging, we have \\[\n\\alpha_t = \\frac{1}{2} \\log\\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right)\n\\] where \\(\\epsilon_t = \\frac{\\sum_{i : y^{(i)} \\neq f_t(\\mathbf{x}^{(i)})} w^{(i)}_{t-1}}{\\sum_{i=1}^n w^{(i)}_{t-1}}\\) is the weighted error rate of the model \\(f_t\\).\nAt each iteration \\(t\\), we compute the weights \\(w^{(i)}_{t-1}\\) based on which points the current ensemble model \\(F_{t-1}\\) are misclassifying. We then learn a new model \\(f_t\\) that minimizes the weighted error, adapting to correct the mistakes of the current ensemble. Finally, we compute the optimal weight \\(\\alpha_t\\) for the new model based on its weighted error rate.\n[image here of the AdaBoost algorithm]\nThe AdaBoost algorithm is agnostic to the choice of weak learner, but decision trees are efficient and interpretable: Each weak learner can be trained quickly, and the resulting decision tree can be understood as a weighted vote of the individual trees.\n\n\nGradient Boosting\nRemarkably, AdaBoost can be viewed as gradient descent where the loss function is defined over the predictions on points rather than the parameters of the model.\nLet \\(\\mathcal{L}\\) be a differentiable loss function defined over the predictions of the model on the training data. In AdaBoost, we used the exponential loss, but we could use other loss functions like squared error or logistic loss. Rather than considering weak learners that are binary classifiers, we will now consider real-valued functions \\(f_t : \\mathbb{R}^d \\to \\mathbb{R}\\).\nAs before, the combined prediction is given by an ensemble model \\(F_t : \\mathbb{R}^d \\to \\mathbb{R}\\). The ensemble model is defined as a weighted sum of the predictions of the individual models: \\[\nF_t(x) = \\alpha_1 f_1(x) + \\alpha_2 f_2(x) + \\ldots + \\alpha_t f_t(x) = F_{t-1}(x) + \\alpha_t f_t(x).\n\\]\nOur goal is to find a weak learner \\(f_t\\) that minimizes the loss \\[\n\\arg \\min_{f_t} \\sum_{i=1}^n \\mathcal{L}(y^{(i)}, F_{t-1}(\\mathbf{x}^{(i)}) + f_t(\\mathbf{x}^{(i)})).\n\\] Recall that the gradient descent update subtracts the gradient of the parameter (opposite the direction of steepest ascent) for a small learning rate \\(\\alpha\\). Instead of updating the parameters of the model, we will update the predictions of the model. In our boosting language, the gradient descent update for the \\(i\\)th prediction is \\[\nF_t(\\mathbf{x}^{(i)}) = F_{t-1}(\\mathbf{x}^{(i)}) - \\alpha \\frac{ \\partial \\mathcal{L}(y^{(i)}, F_{t-1}(\\mathbf{x}^{(i)}))}{\\partial F_{t-1}(\\mathbf{x}^{(i)})}.\n\\] In practice, finding a function that exactly matches the gradient is difficult, so we will instead find a function that approximates the gradient. That is, we train a weak learner \\(f_t\\) to fit the points \\((\\mathbf{x}^{(i)}, \\frac{ \\partial \\mathcal{L}(y^{(i)}, F_{t-1}(\\mathbf{x}^{(i)}))}{\\partial F_{t-1}(\\mathbf{x}^{(i)})})\\) for \\(i \\in \\{1, \\ldots n\\}\\). When the loss is the squared error loss, i.e., \\[\n\\mathcal{L}(y, F_{t-1}(\\mathbf{x})) = \\frac12 (F_{t-1}(\\mathbf{x}) - y)^2,\n\\] the gradient of the loss with respect to the prediction is simply the residual between the prior prediction and the label, i.e., \\[\n\\frac{ \\partial \\mathcal{L}(y, F_{t-1}(\\mathbf{x}))}{\\partial F_{t-1}(\\mathbf{x})} = F_{t-1}(\\mathbf{x}) - y.\n\\] In this, gradient boosting has a nice interpretation as an iterative process of fitting a model to the residuals of the prior model. Once we have trained the weak learner \\(f_t\\), we can select the weight \\(\\alpha_t\\) to minimize the loss of the composite model: \\[\n\\alpha_t = \\arg \\min_\\alpha \\sum_{i=1}^n \\mathcal{L}(y^{(i)}, F_{t-1}(\\mathbf{x}^{(i)}) + \\alpha f_t(\\mathbf{x}^{(i)})).\n\\] When the loss is differentiable and convex (e.g., mean squared error), we can find the optimal weight \\(\\alpha_t\\) by differentiating the loss with respect to \\(\\alpha_t\\) and setting it to zero.\nThe gradient boosting algorithm called XGBoost is a popular implementation. Since 2014, XGboost has basically been the best performing model on many supervised learning tasks. However, foundation models like TabPFN are starting to outperform XGBoost on small to medium-sized datasets. Like modern models, TabPFN uses a transformer architecture, and is trained on some 130 million synthetic datasets. While XGBoost requires relearning an ensemble for each new dataset, TabPFN can be run without any training by simply passing labeled example data, and unlabeled validation data as input to the model."
  },
  {
    "objectID": "notes/11_Concentration.html",
    "href": "notes/11_Concentration.html",
    "title": "Concentration Inequalities",
    "section": "",
    "text": "In our exploration of reinforcement learning, we saw how the outcome of actions can be random. More generally, we are surrounded by random processes, from the weather to stock prices. Today, we will discuss concentration inequalities, which provide powerful tools for understanding the behavior of random variables. In the remainder of the course, we will see applications in a simplified model of reinforcement learning, explainable AI, and active learning.\nLet \\(X\\) be a random variable drawn from a distribution. We will let \\(\\Pr(X = x)\\) denote the probability that \\(X\\) takes on the particular value \\(x\\). The expectation, or mean, of \\(X\\) is defined as \\[\n\\mathbb{E}[X] = \\sum_{x} x \\cdot \\Pr(X = x).\n\\] Note: When \\(X\\) is a continuous random variable, the expectation is defined using an integral instead of a sum. The variance measures how closely \\(X\\) concentrates around its expectation: \\[\n\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2].\n\\]\n[image here of distribution, with mean and confidence intervals]\nConcentration inequalities will tell us the probability that \\(X\\) deviates from its mean by a certain amount.\n\nMarkov’s Inequality\nThe building block of concentration inequalities is Markov’s inequality. Let \\(\\mathbf{X}\\) be a non-negative random variable. Markov’s inequality says that, for any \\(\\epsilon &gt; 0\\), \\[\n\\Pr(\\mathbf{X} \\geq \\epsilon) \\leq \\frac{\\mathbb{E}[\\mathbf{X}]}{\\epsilon}.\n\\]\n\n\n\nProof of Markov’s Inequality\n\nWe will use the definition of expectation to prove Markov’s inequality: \\[\n\\begin{align}\n\\mathbb{E}[\\mathbf{X}]\n&= \\sum_{x} x \\cdot \\Pr(\\mathbf{X} = x)\n\\\\&= \\sum_{x: x \\geq \\epsilon} x \\cdot \\Pr(\\mathbf{X} = x) + \\sum_{x: x &lt; \\epsilon} x \\cdot \\Pr(\\mathbf{X} = x)\n\\\\&\\geq \\sum_{x: x \\geq \\epsilon} \\epsilon \\cdot \\Pr(\\mathbf{X} = x) + \\sum_{x: x &lt; \\epsilon} 0 \\cdot \\Pr(\\mathbf{X} = x)\n\\\\&= \\epsilon \\sum_{x: x \\geq \\epsilon} \\Pr(\\mathbf{X} = x)\n\\\\&= \\epsilon \\cdot \\Pr(\\mathbf{X} \\geq \\epsilon),\n\\end{align}\n\\] where the second equality is by partitioning the sum, the inequality is because \\(x \\geq 0\\) by assumption in the first term and \\(x\\) is non-negative, and the last equality follows because the probability of the event \\(\\{\\mathbf{X} \\geq \\epsilon\\}\\) is the sum of the probabilities of all outcomes where \\(\\mathbf{X}\\) is at least \\(\\epsilon\\).\n\n\n[image here of number line and expectation, dependence of \\(1/\\epsilon\\)]\nAs \\(\\epsilon\\) increases, the probability that \\(X \\geq \\epsilon\\) decays at a rate of \\(1/\\epsilon\\). This is certainly in the right direction (as \\(\\epsilon\\) increases, the probability that \\(X\\) exceeds \\(\\epsilon\\) decreases), but we could hope for a stronger bound. For example, the tail bound of the Gaussian distribution decays at an exponential rate. However, there are also some distributions where Markov’s inequality is tight i.e., \\(\\Pr(X \\geq \\epsilon) = \\frac{\\mathbb{E}[X]}{\\epsilon}\\). Can you construct such a distribution?\nIn order to distinguish between distributions with different tail behaviors, we will make use of additional information. The next inequality we consider will incorporate the variance.\n\n\nChebyshev’s Inequality\nChebyshev’s inequality is a stronger concentration inequality that applies to any random variable with a finite mean and variance. Let \\(\\sigma^2 = \\text{Var}(X)\\) be the variance of \\(X\\), and \\(\\sigma = \\sqrt{\\sigma^2}\\) be the standard deviation. Chebyshev’s inequality states that, for any \\(\\epsilon &gt; 0\\), \\[\n\\Pr(|X - \\mathbb{E}[X]| \\geq \\epsilon \\sigma) \\leq \\frac{1}{\\epsilon^2}.\n\\] In words, the probability that a random variable deviates from its mean by more than \\(\\epsilon\\) standard deviations is at most \\(\\frac{1}{\\epsilon^2}\\).\n\n\n\nProof of Chebyshev’s Inequality\n\nWhile the two inequalities appear quite different, we will actually use Markov’s to prove Chebyshev’s. Define a new random variable \\(Z = (\\mathbf{X} - \\mathbb{E}[X])^2\\). Then, we can apply Markov’s inequality to \\(Z\\): \\[\n\\Pr((\\mathbf{X} - \\mathbb{E}[X])^2 \\geq \\epsilon^2) \\leq \\frac{\\mathbb{E}[(\\mathbf{X} - \\mathbb{E}[X])^2]}{\\epsilon^2}.\n\\] Notice that \\(\\mathbb{E}[(\\mathbf{X} - \\mathbb{E}[X])^2] = \\text{Var}(X) = \\sigma^2\\). Taking the squareroot of both sides of the event \\((\\mathbf{X} - \\mathbb{E}[X])^2 \\geq \\epsilon^2\\) yields \\[\n\\Pr(|\\mathbf{X} - \\mathbb{E}[X]| \\geq \\epsilon) \\leq \\frac{\\sigma^2}{\\epsilon^2}.\n\\] Setting \\(\\epsilon = \\epsilon' \\sigma\\) gives us \\[\n\\Pr(|\\mathbf{X} - \\mathbb{E}[X]| \\geq \\epsilon' \\sigma) \\leq \\frac{\\sigma^2}{(\\epsilon' \\sigma)^2} = \\frac{1}{\\epsilon'^2}.\n\\] The statement follows by relabeling \\(\\epsilon'\\) as \\(\\epsilon\\).\n\n\n[image here of number line, and probability bound]\nThe advantage of Chebyshev’s inequality is that information about the distribution’s variance yields tighter bounds. But, for fixed variance, the dependence on the the deviation is still only \\(1/\\epsilon^2\\). When we are interested in the behavior of many random variables simultaneously, we’ll need an even better dependence on \\(\\epsilon\\) to get meaningful bounds.\n\n\nHoeffding’s Inequality\nAs we add more random variables, the central limit theorem tells us that their sum will behave like a Gaussian. (Check out this excellent 3blue1brown video for an intuitive explanation.)"
  }
]