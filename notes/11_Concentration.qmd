---
title: "**Concentration Inequalities**"
format:
  html:
    toc: true
    math: mathjax
    include-after-body: readtime.html
---

In our exploration of reinforcement learning, we saw how the outcome of actions can be random.
More generally, we are surrounded by random processes, from the weather to stock prices.
Today, we will discuss concentration inequalities, which provide powerful tools for understanding the behavior of random variables.
In the remainder of the course, we will see applications in a simplified model of reinforcement learning, explainable AI, and active learning.

Let $X$ be a random variable drawn from a distribution.
We will let $\Pr(X = x)$ denote the probability that $X$ takes on the particular value $x$.
The expectation, or mean, of $X$ is defined as
$$
\mathbb{E}[X] = \sum_{x} x \cdot \Pr(X = x).
$$
Note: When $X$ is a continuous random variable, the expectation is defined using an integral instead of a sum.
The variance measures how closely $X$ concentrates around its expectation:
$$
\text{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2].
$$

[image here of distribution, with mean and confidence intervals]

Concentration inequalities will tell us the probability that $X$ deviates from its mean by a certain amount.

### Markov's Inequality

The building block of concentration inequalities is Markov's inequality.
Let $\mathbf{X}$ be a non-negative random variable.
Markov's inequality says that, for any $\epsilon > 0$,
$$
\Pr(\mathbf{X} \geq \epsilon) \leq \frac{\mathbb{E}[\mathbf{X}]}{\epsilon}.
$$

::: {.proof-block}
<details open>
<summary>Proof of Markov's Inequality</summary>

We will use the definition of expectation to prove Markov's inequality:
$$
\begin{align}
\mathbb{E}[\mathbf{X}]
&= \sum_{x} x \cdot \Pr(\mathbf{X} = x)
\\&= \sum_{x: x \geq \epsilon} x \cdot \Pr(\mathbf{X} = x) + \sum_{x: x < \epsilon} x \cdot \Pr(\mathbf{X} = x)
\\&\geq \sum_{x: x \geq \epsilon} \epsilon \cdot \Pr(\mathbf{X} = x) + \sum_{x: x < \epsilon} 0 \cdot \Pr(\mathbf{X} = x)
\\&= \epsilon \sum_{x: x \geq \epsilon} \Pr(\mathbf{X} = x)
\\&= \epsilon \cdot \Pr(\mathbf{X} \geq \epsilon),
\end{align}
$$
where the second equality is by partitioning the sum, the inequality is because $x \geq 0$ by assumption in the first term and $x$ is non-negative, and the last equality follows because the probability of the event $\{\mathbf{X} \geq \epsilon\}$ is the sum of the probabilities of all outcomes where $\mathbf{X}$ is at least $\epsilon$.

</details>
:::

[image here of number line and expectation, dependence of $1/\epsilon$]

As $\epsilon$ increases, the probability that $X \geq \epsilon$ decays at a rate of $1/\epsilon$.
This is certainly in the right direction (as $\epsilon$ increases, the probability that $X$ exceeds $\epsilon$ decreases), but we could hope for a stronger bound.
For example, the tail bound of the Gaussian distribution decays at an exponential rate.
However, there are also some distributions where Markov's inequality is tight i.e., $\Pr(X \geq \epsilon) = \frac{\mathbb{E}[X]}{\epsilon}$.
Can you construct such a distribution?

In order to distinguish between distributions with different tail behaviors, we will make use of additional information.
The next inequality we consider will incorporate the variance.

### Chebyshev's Inequality

Chebyshev's inequality is a stronger concentration inequality that applies to any random variable with a finite mean and variance.
Let $\sigma^2 = \text{Var}(X)$ be the variance of $X$, and $\sigma = \sqrt{\sigma^2}$ be the standard deviation.
Chebyshev's inequality states that, for any $\epsilon > 0$,
$$
\Pr(|X - \mathbb{E}[X]| \geq \epsilon \sigma) \leq \frac{1}{\epsilon^2}.
$$
In words, the probability that a random variable deviates from its mean by more than $\epsilon$ standard deviations is at most $\frac{1}{\epsilon^2}$.

::: {.proof-block}
<details open>
<summary>Proof of Chebyshev's Inequality</summary>

While the two inequalities appear quite different, we will actually use Markov's to prove Chebyshev's.
Define a new random variable $Z = (\mathbf{X} - \mathbb{E}[X])^2$.
Then, we can apply Markov's inequality to $Z$:
$$
\Pr((\mathbf{X} - \mathbb{E}[X])^2 \geq \epsilon^2) \leq \frac{\mathbb{E}[(\mathbf{X} - \mathbb{E}[X])^2]}{\epsilon^2}.
$$
Notice that $\mathbb{E}[(\mathbf{X} - \mathbb{E}[X])^2] = \text{Var}(X) = \sigma^2$.
Taking the squareroot of both sides of the event $(\mathbf{X} - \mathbb{E}[X])^2 \geq \epsilon^2$ yields
$$
\Pr(|\mathbf{X} - \mathbb{E}[X]| \geq \epsilon) \leq \frac{\sigma^2}{\epsilon^2}.
$$
Setting $\epsilon = \epsilon' \sigma$ gives us
$$
\Pr(|\mathbf{X} - \mathbb{E}[X]| \geq \epsilon' \sigma) \leq \frac{\sigma^2}{(\epsilon' \sigma)^2} = \frac{1}{\epsilon'^2}.
$$
The statement follows by relabeling $\epsilon'$ as $\epsilon$.

</details>
:::

[image here of number line, and probability bound]

The advantage of Chebyshev's inequality is that information about the distribution's variance yields tighter bounds.
But, for fixed variance, the dependence on the the deviation is still only $1/\epsilon^2$.
When we are interested in the behavior of many random variables simultaneously, we'll need an even better dependence on $\epsilon$ to get meaningful bounds.

### Hoeffding's Inequality

As we add more random variables, the central limit theorem tells us that their sum will behave like a Gaussian.
(Check out [this](https://www.youtube.com/watch?v=zeJD6dqJ5lo) excellent 3blue1brown video for an intuitive explanation.)
