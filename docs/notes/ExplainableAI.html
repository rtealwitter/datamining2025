<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Explainable AI and Active Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../eve.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-c5a5d5e27fcc88644031c24cff017230.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="shortcut icon" href="../eve.ico">
<script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-GZHXTPTRRE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-GZHXTPTRRE');
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Fall 2025</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <span class="nav-link">
<span class="menu-text">Canvas</span>
    </span>
  </li>  
  <li class="nav-item">
    <span class="nav-link">
<span class="menu-text">Gradescope</span>
    </span>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../syllabus.html"> 
<span class="menu-text">Syllabus</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#shapley-values" id="toc-shapley-values" class="nav-link active" data-scroll-target="#shapley-values">Shapley Values</a>
  <ul class="collapse">
  <li><a href="#monte-carlo-estimation" id="toc-monte-carlo-estimation" class="nav-link" data-scroll-target="#monte-carlo-estimation">Monte Carlo Estimation</a></li>
  <li><a href="#maximum-sample-reuse-estimation" id="toc-maximum-sample-reuse-estimation" class="nav-link" data-scroll-target="#maximum-sample-reuse-estimation">Maximum Sample Reuse Estimation</a></li>
  </ul></li>
  <li><a href="#active-linear-regression" id="toc-active-linear-regression" class="nav-link" data-scroll-target="#active-linear-regression">Active Linear Regression</a>
  <ul class="collapse">
  <li><a href="#active-learning" id="toc-active-learning" class="nav-link" data-scroll-target="#active-learning">Active Learning</a></li>
  <li><a href="#leverage-scores" id="toc-leverage-scores" class="nav-link" data-scroll-target="#leverage-scores">Leverage Scores</a></li>
  <li><a href="#regression-adjustment" id="toc-regression-adjustment" class="nav-link" data-scroll-target="#regression-adjustment">Regression Adjustment</a></li>
  <li><a href="#a-data-perspective" id="toc-a-data-perspective" class="nav-link" data-scroll-target="#a-data-perspective">A Data Perspective</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><strong>Explainable AI and Active Learning</strong></h1>
</div>



<div class="quarto-title-meta column-page-left">

    
  
    
  </div>
  


</header>


<p>In this course, we have explored how machine learning can be used to solve tasks, from supervised learning with linear models to reinforcement learning with neural networks. The approaches we’ve discussed, especially when we throw lots of compute and data at them, tend to work very well in practice. However, as these models are deployed in the real world, it becomes increasingly important to understand their behavior. Today, we’ll explore one way to gain insights into model behavior.</p>
<center>
<img src="images/xai_intro.svg" class="responsive-img">
</center>
<p>Understanding a model like linear regression is easy. The output of the model is a weighted linear combination of the inputs: <span class="math display">\[
f(\mathbf{x}) = \sum_{i=1}^{d} w_i x_i.
\]</span> So, changing feature <span class="math inline">\(i\)</span> from baseline <span class="math inline">\(b_i\)</span> to <span class="math inline">\(x_i\)</span> will simply increase the output at a rate of the weight <span class="math inline">\(w_i\)</span>. Formally, the attribution of the change in output to feature <span class="math inline">\(i\)</span> from <span class="math inline">\(x_i\)</span> to <span class="math inline">\(b_i\)</span> can be expressed as: <span class="math display">\[
\phi_i = w_i (x_i - b_i).
\]</span> With these attribution values <span class="math inline">\(\phi_i\)</span> in hand, we can perform several safety checks. For example, it would be concerning if the attribution of race in a mortgage rate model was non-zero, or if the attribution of gender in a hiring model was non-zero.</p>
<p>General models (think neural networks with many layers) are unfortunately not so easy to understand. The challenge is that there are <em>non-linear</em> interactions between features, making it difficult to attribute changes in the output to specific input features. For example, consider a model which predicts how weather will impact the growth of plants: Precipitation is beneficial for plant growth, but only when the temperature is high enough; if it’s below freezing, precipitation will instead be harmful. Teasing apart the effects of a feature requires considering not just the feature itself, but also all the other features.</p>
<p>Let <span class="math inline">\(\mathbf{x} \in \mathbb{R}^d\)</span> be the input to a model, and let <span class="math inline">\(f(\mathbf{x})\)</span> be the output that we’re trying to understand. We’ll consider the impact of feature <span class="math inline">\(x_i\)</span> relative to a baseline <span class="math inline">\(\mathbf{b} \in \mathbb{R}^d\)</span>. In order to tease apart this effect, we will consider all possible ways to set the other features. For a subset of features <span class="math inline">\(S \subseteq [d]\)</span>, define <span class="math inline">\(\mathbf{x}^S\)</span> so that <span class="math display">\[
\mathbf{x}^S_j = \begin{cases}
x_i &amp; \text{if } j \in S \\
b_j &amp; \text{if } j \notin S.
\end{cases}
\]</span> Then, one natural way to examine the effect of <span class="math inline">\(x_i\)</span> is to compare <span class="math inline">\(f(\mathbf{x}^{S \cup \{i\}}) - f(\mathbf{x}^S)\)</span> for all sets <span class="math inline">\(S \subseteq [d] \setminus \{i\}\)</span>. For notational convenience, define a function <span class="math inline">\(v: 2^{[d]} \to \mathbb{R}\)</span> so that <span class="math inline">\(v(S) = f(\mathbf{x}^{S})\)</span>.</p>
<section id="shapley-values" class="level2">
<h2 class="anchored" data-anchor-id="shapley-values">Shapley Values</h2>
<p>There are several desirable properties that we would like our attribution values to satisfy:</p>
<ul>
<li><strong>Null</strong>: If a feature does not affect the output, its attribution should be zero.</li>
<li><strong>Symmetry</strong>: If two features contribute equally to the output, they should receive equal attribution.</li>
<li><strong>Linearity</strong>: If the model is a linear combination of two models, the attribution should be a linear combination of the attributions of the individual models.</li>
<li><strong>Efficiency</strong>: The total attribution across all features should equal the change in output from the baseline.</li>
</ul>
<p>These four properties have been studied since the mid-20th century in the context of fairly distributing payouts among players in a cooperative game <span class="math inline">\(v: 2^{[d]} \to \mathbb{R}\)</span>. The Shapley value, proposed by Nobel laureate Lloyd Shapley, <em>uniquely</em> satisfies the four properties: <span class="math display">\[
\begin{align}
\phi_i &amp;= \frac1{d} \sum_{S \subseteq [d] \setminus \{i\}} \frac{v(S \cup \{i\}) - v(S)}{\binom{d-1}{|S|}}
\\&amp;=
\underbrace{
\frac1{d}
\sum_{\ell=0}^{d-1}
  \underbrace{
    \frac1{\binom{d-1}{\ell}}
    \sum_{\substack{S \subseteq [d] \setminus \{i\} \\ |S|=\ell}}
      \big( v(S \cup \{i\}) - v(S) \big)
  }_{\text{average over subsets of size }\ell}
}_{\text{average over all subset sizes}}
\end{align}
\]</span> Intuitively, the Shapley value is the <em>average</em> contribution of a feature to the model’s output, where the distribution equally weights each set size <span class="math inline">\(\ell\)</span>. While different distributions will satisfy the first three properties, only the Shapley value satisfies efficiency. Mathematically, efficiency requires that <span class="math display">\[
\sum_{i=1}^{d} \phi_i = v([d]) - v(\emptyset) = f(\mathbf{x}) - f(\mathbf{b}).
\]</span> Notably, this allows us to <em>decompose</em> the change in the prediction into contributions from each feature.</p>
<center>
<img src="images/xai_efficiency.svg" class="responsive-img">
</center>
<p>Because of these desirable properties, Shapley values are the <em>de facto</em> method of feature attribution in modern machine learning. But, because there are an exponential number of subsets, computing Shapley values exactly is computationally challenging.</p>
<section id="monte-carlo-estimation" class="level3">
<h3 class="anchored" data-anchor-id="monte-carlo-estimation">Monte Carlo Estimation</h3>
<p>Our first attempt at approximation will be with the standard Monte Carlo estimator. Any time we have a summation over many terms, we can <em>estimate</em> the entire sum using only a few of its terms. Consider a sampling distribution <span class="math inline">\(\mathcal{D}\)</span> over subsets <span class="math inline">\(S \subseteq [d] \setminus \{i\}\)</span>. We will let <span class="math inline">\(p_S\)</span> be the probability of sampling subset <span class="math inline">\(S\)</span> from <span class="math inline">\(\mathcal{D}\)</span>. Draw <span class="math inline">\(m\)</span> subsets <span class="math inline">\(S_1, \ldots, S_m\)</span> with replacement from <span class="math inline">\(\mathcal{D}\)</span>.</p>
<p>The Monte Carlo estimator is <span class="math display">\[
\tilde{\phi}_i^\text{MC} = \frac1{m} \sum_{j=1}^{m} \frac{v(S_j \cup \{i\}) - v(S_j)}{d \binom{d-1}{|S_j|} p_{S_j}}.
\]</span> Often, we set the sampling probabilities to the weights i.e., <span class="math inline">\(p_S = \frac{1}{d \binom{d-1}{|S|}}\)</span> so that the estimator can simply be written as <span class="math display">\[
\tilde{\phi}_i^\text{MC} = \frac1{m} \sum_{j=1}^{m} \left(v(S_j \cup \{i\}) - v(S_j)\right).
\]</span></p>
<p>The expected value of our estimator, with respect to the randomness of the sampling, is <span class="math display">\[
\mathbb{E}[\tilde{\phi}_i^\text{MC}]
= \mathbb{E}\left[\frac1{m} \sum_{j=1}^{m} \sum_{S \subseteq [d] \setminus \{i\}} \frac{v(S \cup \{i\}) - v(S)}{d \binom{d-1}{|S|} p_S} \mathbb{1}[S = S_j]\right]
= \frac1{m} \sum_{j=1}^{m} \sum_{S \subseteq [d] \setminus \{i\}} \frac{v(S \cup \{i\}) - v(S)}{d \binom{d-1}{|S|} p_S} \mathbb{E}[\mathbb{1}[S = S_j]]
= \phi_i,
\]</span> where the second equality follows by the linearity of expectation, and the last equality follows because the expectation of an indicator random variable is the probability that the indicator is 1. While it’s important that the estimator is right expectation, we also care about how closely it concentrates around <span class="math inline">\(\phi_i\)</span>. To this end, let’s compute the variance. We’ll use three properties of the variance:</p>
<ul>
<li><strong>Scalar Constant:</strong> If <span class="math inline">\(c\)</span> is a constant, then <span class="math inline">\(\text{Var}(cX) = \mathbb{E}[(cX)^2] - \mathbb{E}[cX]^2 = c^2 \text{Var}(X)\)</span>.</li>
<li><strong>Linearity of Variance:</strong> If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent random variables, then <span class="math inline">\(\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y)\)</span>. Can you prove why?</li>
<li><strong>Variance of an Indicator Random Variable:</strong> Let <span class="math inline">\(\mathbb{1}[A]\)</span> be an indicator random variable that is 1 if event <span class="math inline">\(A\)</span> occurs and 0 otherwise. Then, <span class="math inline">\(\text{Var}(\mathbb{1}[A]) = \mathbb{E}[\mathbb{1}[A]^2] - \mathbb{E}[\mathbb{1}[A]]^2 = \Pr(A) - \Pr(A)^2 \leq \Pr(A)\)</span>.</li>
</ul>
<p>With these in hand, the variance of our estimator is <span class="math display">\[
\text{Var}(\tilde{\phi}_i^\text{MC})
\leq \frac1{m^2} \sum_{j=1}^{m} \sum_{S \subseteq [d] \setminus \{i\}}
\left(\frac{v(S \cup \{i\}) - v(S)}{d \binom{d-1}{|S|} p_S}\right)^2
\Pr(\mathbb{1}[S_j = S])
= \frac1{m} \sum_{S \subseteq [d] \setminus \{i\}}
\left(\frac{v(S \cup \{i\}) - v(S)}{d \binom{d-1}{|S|} p_S}\right)^2
p_S
\]</span></p>
<p>With our choice of <span class="math inline">\(p_S = \frac{1}{d\binom{d-1}{|S|}}\)</span>, we get <span class="math display">\[
\text{Var}(\tilde{\phi}_i^\text{MC})
\leq \frac1{m} \sum_{S \subseteq [d] \setminus \{i\}}
\frac{\left(v(S \cup \{i\}) - v(S)\right)^2}{d\binom{d-1}{|S|}}.
\]</span></p>
<p>Using <a href="Concentration.html#chebyshev">Chebyshev’s inequality</a>, we can bound the probability that our estimator deviates from the true value: <span class="math display">\[
\Pr\left(|\tilde{\phi}_i^\text{MC} - \phi_i| \geq \epsilon\right) \leq \frac{\text{Var}(\tilde{\phi}_i^\text{MC})}{\epsilon^2}.
\]</span> Setting the failure probability to <span class="math inline">\(\delta\)</span>, we could solve for the number of samples <span class="math inline">\(m\)</span> that we need to achieve an <span class="math inline">\(\epsilon\)</span> approximation to <span class="math inline">\(\phi_i\)</span>.</p>
<p>While an excellent (and simple) estimator, the naive Monte Carlo approach has an unfortunate drawback: Each sampled pair <span class="math inline">\(v(S)\)</span> and <span class="math inline">\(v(S \cup \{i\})\)</span> can only be used for estimating the <span class="math inline">\(i\)</span>th Shapley value <span class="math inline">\(\phi_i\)</span>. In effect, we only use <span class="math inline">\(1/d\)</span> of our total sample budget for each estimate.</p>
</section>
<section id="maximum-sample-reuse-estimation" class="level3">
<h3 class="anchored" data-anchor-id="maximum-sample-reuse-estimation">Maximum Sample Reuse Estimation</h3>
<p>A more sophisticated approach to estimating Shapley values is to rewrite the Shapley value where the subsets are not paired. Observe that <span class="math display">\[
\begin{align}
\phi_i &amp;= \sum_{S \subseteq [d] \setminus \{i\}} \frac{v(S \cup \{i\}) - v(S)}{d \binom{d-1}{|S|}}
\\&amp;= \sum_{S \subseteq [d] \setminus \{i\} : i \in S} \frac{v(S)}{d \binom{d-1}{|S|-1}}
- \sum_{S \subseteq [d] \setminus \{i\} : i \notin S} \frac{v(S)}{d \binom{d-1}{|S|}}
\\&amp;= \sum_{S \subseteq [d] \setminus \{i\}} v(S) \left(\frac{\mathbb{1}[i \in S]}{d \binom{d-1}{|S|-1}}
- \frac{\mathbb{1}[i \notin S]}{d \binom{d-1}{|S|}}\right).
\end{align}
\]</span></p>
<p>A natural approach is to use each sampled set <span class="math inline">\(S\)</span> to estimate <em>every</em> Shapley value. This Maximum Sample Reuse (MSR) estimator is</p>
<p><span class="math display">\[
\begin{align}
\tilde{\phi}_i^\text{MSR}
&amp;= \frac1{m} \sum_{j=1}^m v(S_j) \left(
\frac{\mathbb{1}[i \in S_j]}{d \binom{d-1}{|S_j|-1}} - \frac{\mathbb{1}[i \notin S_j]}{d \binom{d-1}{|S_j|}}
\right)
\frac{1}{p_S}.
\end{align}
\]</span></p>
<p>A similar calculation to before shows that this estimator is also right in expectation. However, its variance depends on <span class="math inline">\([v(S)]^2\)</span> rather than <span class="math inline">\([v(S \cup \{i\}) - v(S)]^2\)</span>. In practice, we expect similar inputs <span class="math inline">\(\mathbf{x}^{S \cup \{i\}}\)</span> and <span class="math inline">\(\mathbf{x}^{S}\)</span> to yield similar outputs, so the variance of the Monte Carlo estimator is generally much smaller than the variance of the Maximum Sample Reuse estimator. Before we come up with an estimator that is both sample efficient and low-variance, let’s discuss another application of Shapley values.</p>
</section>
</section>
<section id="active-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="active-linear-regression">Active Linear Regression</h2>
<p>Shapley values have many wonderful and surprising properties. One of them is that they are the best linear approximation to the function <span class="math inline">\(v\)</span>, for a certain weighting of the values <span class="math inline">\(v(S)\)</span>.</p>
<p>For notational convenience, suppose that <span class="math inline">\(v(\emptyset) =0\)</span>. (If not, we could subtract <span class="math inline">\(v(\emptyset)\)</span> from all values to center them without change the Shapley values.) We can write the vector of Shapley values as <span class="math display">\[
\boldsymbol{\phi} =
\begin{bmatrix}
\phi_1 \\
\phi_2 \\
\vdots \\
\phi_d
\end{bmatrix}
= \arg \min_{\boldsymbol{\beta} \in \mathbb{R}^d: \langle \boldsymbol{\beta}, \mathbf{1} \rangle = v([d])}
\sum_{S \subseteq [d]: 0 &lt; |S| &lt; d}
\left( v(S) - \sum_{i \in S} \beta_i \right)^2 w_S,
\]</span> where the weights are <span class="math inline">\(w_S = \frac{1}{\binom{d}{|S|} |S| (d-|S|)}\)</span>. Notice that the weighting in the regression problem is similar to that of the Shapley values themselves. Further, the constraint that <span class="math inline">\(\boldsymbol{\beta}\)</span> sums to <span class="math inline">\(v([d])\)</span> ensures that the efficiency property is satisfied, since we assumed that <span class="math inline">\(v(\emptyset) = 0\)</span>.</p>
<p>Unfortunately, proving this equality is involved, and not particularly informative. But it will be quite useful for estimating Shapley values efficiently.</p>
<p>We’ll first need to convert the constrained regression problem to an unconstrained one. Define the input matrix <span class="math inline">\(\mathbf{A}' \in \mathbb{R}^{2^d-2 \times d}\)</span> so that each row corresponds to a subset <span class="math inline">\(S \subseteq [d]\)</span> where <span class="math inline">\(0 &lt; |S| &lt; d\)</span>, and each column corresponds to an index <span class="math inline">\(i \in [d]\)</span>. The <span class="math inline">\((S,i)\)</span> entry is given by <span class="math inline">\([\mathbf{A}']_{S,i} = \mathbb{1}[i \in S] \sqrt{w_S}\)</span>. Define the target vector <span class="math inline">\(\mathbf{b}' \in \mathbb{R}^{2^d-2}\)</span> so that the <span class="math inline">\(S\)</span>th entry is given by <span class="math inline">\([\mathbf{b}']_S = v(S) \sqrt{w_S}\)</span>. Then we can rewrite the regression problem as <span class="math display">\[
\begin{align}
\boldsymbol{\phi}
&amp;= \arg \min_{\boldsymbol{\beta} \in \mathbb{R}^d: \langle \boldsymbol{\beta}, \mathbf{1} \rangle = v([d])} \| \mathbf{A}' \boldsymbol{\beta} - \mathbf{b}' \|^2
\\&amp; = \arg \min_{\boldsymbol{\beta} \in \mathbb{R}^d: \langle \boldsymbol{\beta}, \mathbf{1} \rangle = 0} \| \mathbf{A}' \boldsymbol{\beta} + \mathbf{A}' \mathbf{1} \frac{v([d])}{d} - \mathbf{b}' \|^2 +  \mathbf{1} \frac{v([d])}{d}
\\&amp; = \arg \min_{\boldsymbol{\beta} \in \mathbb{R}^d} \| \mathbf{A} \boldsymbol{\beta} - \mathbf{b} \|^2 + \mathbf{1} \frac{v([d])}{d},
\end{align}
\]</span> where we define <span class="math inline">\(\mathbf{b} = \mathbf{b'} - \mathbf{A' 1} \frac{v([d])}{d}\)</span>, we define <span class="math inline">\(\mathbf{A}= \mathbf{A' P}\)</span>, and we define <span class="math inline">\(\mathbf{P} = \mathbf{I} - \frac1{d} \mathbf{1} \mathbf{1}^\top\)</span> as the projection matrix onto the subspace orthogonal to the constant vector <span class="math inline">\(\mathbf{1}\)</span>.</p>
<p>Now we can apply our favorite linear regression tools.</p>
<section id="active-learning" class="level3">
<h3 class="anchored" data-anchor-id="active-learning">Active Learning</h3>
<p>Let <span class="math display">\[
\boldsymbol{\beta}^* = \arg \min_{\boldsymbol{\beta} \in \mathbb{R}^d} \| \mathbf{A} \boldsymbol{\beta} - \mathbf{b} \|^2.
\]</span></p>
<p>The challenge in solving this linear regression problem is similar to the challenge of directly computing the Shapley values: the input matrix and target vector are exponentially large in <span class="math inline">\(d\)</span>. Luckily, we can sample only a fraction of the rows, and find the best linear approximation to these.</p>
<center>
<img src="images/xai_subsample.svg" class="responsive-img">
</center>
<p>Let <span class="math inline">\(\boldsymbol{\Pi} \in \mathbb{R}^{m \times (2^d-2)}\)</span> be the sampling matrix. Each row of <span class="math inline">\(\boldsymbol{\Pi}\)</span> corresponds to a sample. In the <span class="math inline">\(j\)</span>th row, we have <span class="math display">\[
[\boldsymbol{\Pi}]_{j,S} = \begin{cases}
\frac1{\sqrt{p_S}} &amp; \text{ if } S_j = S \\
0 &amp; \text{ else}.
\end{cases}
\]</span> The least squares solution on the sampled problem is given by: <span class="math display">\[
\tilde{\boldsymbol{\beta}}=\arg \min_{\boldsymbol{\beta} \in \mathbb{R}^d}
\| \boldsymbol{\Pi} \mathbf{A} \boldsymbol{\beta} - \boldsymbol{\Pi} \mathbf{b} \|_2
= \left( \mathbf{A}^\top \boldsymbol{\Pi}^\top \boldsymbol{\Pi} \mathbf{A} \right)^{+} \mathbf{A}^\top \boldsymbol{\Pi}^\top \boldsymbol{\Pi} \mathbf{b}
\]</span> Notice that we reweight the sampling matrix so that <span class="math inline">\(\mathbf{A}^\top \boldsymbol{\Pi}^\top \boldsymbol{\Pi} \mathbf{A}\)</span> and <span class="math inline">\(\mathbf{A}^\top \boldsymbol{\Pi}^\top \boldsymbol{\Pi} \mathbf{b}\)</span> are correct in expectation; that is, <span class="math inline">\(\mathbb{E}[ \boldsymbol{\Pi}^\top \boldsymbol{\Pi}] = \mathbf{I}\)</span>. Because of the correlation between samples, this doesn’t actually mean that the model is correct in expectation; that is, <span class="math inline">\(\mathbb{E}[ \tilde{\boldsymbol{\beta}}] \neq \boldsymbolhi{\beta}^*\)</span>.</p>
<p>Unlike in most of the problems we’ve seen where sampled points are given to us in a dataset, we get to choose the sampled subsets that we use to solve the regression problem.</p>
</section>
<section id="leverage-scores" class="level3">
<h3 class="anchored" data-anchor-id="leverage-scores">Leverage Scores</h3>
<p>A particularly powerful tool in active learning is the use of leverage scores. Leverage scores quantify how useful a sample is for fitting the linear regression model. For samples with high leverage, excluding them can deteriorate the quality of the learned model.</p>
<center>
<img src="images/xai_leverage.svg" class="responsive-img">
</center>
<p>The idea is to measure how “alone” a fitted sample value <em>could</em> be relative to the other samples. Mathematically, the leverage of a sample <span class="math inline">\(S\)</span> is given by <span class="math display">\[
\ell_S = \max_{\boldsymbol{\beta} \in \mathbb{R}^d}
\frac{([\boldsymbol{A \beta}]_S)^2}{\| \boldsymbol{A \beta} \|_2^2}.
\]</span> Notice that <span class="math inline">\(\ell_S\)</span> is defined independently of the target vector <span class="math inline">\(\mathbf{b}\)</span>: It measures, over all possible learned vectors <span class="math inline">\(\boldsymbol{\beta}\)</span>, what’s the largest the entry <span class="math inline">\([\boldsymbol{A \beta}]_S\)</span> can be relative to the overall norm of the fitted vector <span class="math inline">\(\boldsymbol{A \beta}\)</span>.</p>
<p>When we sample according to leverage scores, we can get strong guarantees on the performance of our learned function. Roughly, with <span class="math inline">\(m=O(d \log(d) + \frac{d}{\epsilon})\)</span> samples, the solution to the approximate regression problem <span class="math inline">\(\boldsymbol{\beta}\)</span> satisfies, with high probability, <span class="math display">\[
\| \boldsymbol{A} \tilde{\boldsymbol{\beta}} - \boldsymbol{b} \|_2^2 \leq
(1-\epsilon) \| \boldsymbol{A} \boldsymbol{\beta}^* - \boldsymbol{b} \|_2^2.
\]</span> That is, the fit of the learned model is close to the fit of the optimal model. We unfortunately won’t cover the proofs of these results here, but the key technical tool is applying <em>matrix</em> concentration inequalities.</p>
</section>
<section id="regression-adjustment" class="level3">
<h3 class="anchored" data-anchor-id="regression-adjustment">Regression Adjustment</h3>
<p>Once we have the learned model <span class="math inline">\(\tilde{\boldsymbol{\beta}}\)</span>, we can simply return our estimated Shapley values <span class="math inline">\(\tilde{\boldsymbol{\phi}} = \tilde{\boldsymbol{\beta}} + \mathbf{1} \frac{v([d])}{d}\)</span> as our best guess for the Shapley values. While this is generally quite accurate, our linear regression estimates are not correct in expectation (because of the correlated samples when we built the <span class="math inline">\(\boldsymbol{\beta}\)</span>).</p>
<p>If we wanted to get unbiased estimates with the same accuracy, we could use our estimates to reduce the variance of an unbiased estimator like Monte Carlo or Maximum Sample Reuse. Consider an approximation <span class="math inline">\(\tilde{v}: 2^{[d]} \to \mathbb{R}\)</span> defined by <span class="math display">\[
\tilde{v}(S) = \sum_{i \in S} \tilde{\phi}_i.
\]</span> Let <span class="math inline">\(\phi_i(v)\)</span> be the Shapley value of the <span class="math inline">\(i\)</span>th data point with respect to the function <span class="math inline">\(v\)</span>. Since the Shapley values are linear, we can write <span class="math display">\[
\begin{align}
\phi_i (v) &amp;=
\sum_{S \subseteq [d] \setminus \{i\}} \frac{v(S \cup \{i\}) - v(S)}{d \binom{d-1}{|S|}}
\\&amp;= \sum_{S \subseteq [d] \setminus \{i\}} \frac{v(S \cup \{i\}) - v(S) + \tilde{v}(S \cup \{i\}) - \tilde{v}(S) - \tilde{v}(S \cup \{i\}) + \tilde{v}(S)}{d \binom{d-1}{|S|}}
\\&amp;= \sum_{S \subseteq [d] \setminus \{i\}} \frac{\tilde{v}(S \cup \{i\}) - \tilde{v}(S)}{d \binom{d-1}{|S|}}
+ \sum_{S \subseteq [d] \setminus \{i\}} \frac{v(S \cup \{i\}) - v(S) - \tilde{v}(S \cup \{i\}) + \tilde{v}(S)}{d \binom{d-1}{|S|}}
\\&amp;= \phi_i(\tilde{v}) + \phi_i(v - \tilde{v}).
\end{align}
\]</span> When <span class="math inline">\(\tilde{v}\)</span> is a linear function, we can simply read off the Shapley values. It remains to estimate the Shapley values of the residual <span class="math inline">\(\phi_i(v - \tilde{v})\)</span>, for which we can use any estimator.</p>
<p>Recall that the Maximum Sample Reuse estimator is both unbiased and sample efficient. The one drawback was that the variance depended on <span class="math inline">\([v(S)]^2\)</span>. However, when we use the estimator on the residual <span class="math inline">\(v - \tilde{v}\)</span>, the variance will depend on <span class="math inline">\([v(S) - \tilde{v}(S)]^2\)</span> instead. As long as <span class="math inline">\(\tilde{v}\)</span> is a good approximation of <span class="math inline">\(v\)</span>, this can lead to much lower variance estimates.</p>
</section>
<section id="a-data-perspective" class="level3">
<h3 class="anchored" data-anchor-id="a-data-perspective">A Data Perspective</h3>
<p>We motivated Shapley values as a way to understand the predictions of models, but their definition and the estimators we discussed can be applied to any function <span class="math inline">\(v\)</span>. One particularly relevant choice for <span class="math inline">\(v(S)\)</span> is the <em>quality</em> of a model when trained only on data points in <span class="math inline">\(S\)</span>. (Typically, as you may expect, we measure quality via loss on a fixed testing set.) Just like when teasing apart the contributions of individual features, the value of a data point will change in importance depending on the context of the other data points in the training set. With this particular <span class="math inline">\(v\)</span>, the Shapley value <span class="math inline">\(\phi_i\)</span> tells us how valuable the <span class="math inline">\(i\)</span>th data point is to the model performance. As data becomes increasingly important, understanding the contributions of individual data points can help us make more informed decisions about data collection, labeling, and model training. For example, we could use the Shapley values of this data function as a mechanism for assigning value to data owners such as newspapers or artists who created the data.</p>



</section>
</section>

</main> <!-- /main -->
<script>
document.addEventListener("DOMContentLoaded", function () {
  const wordsPerMinute = 200;
  const text = document.body.innerText;
  const words = text.trim().split(/\s+/).length;
  const readingTime = Math.ceil(words / wordsPerMinute);

  const readTimeEl = document.createElement("div");
  readTimeEl.innerText = `⏱️ ${readingTime} min read`;

  // Style it to appear centered
  readTimeEl.style.fontSize = "0.9em";
  readTimeEl.style.margin = "1em auto";
  readTimeEl.style.textAlign = "left";
  readTimeEl.style.width = "100%";

  const title = document.querySelector("h1");
  if (title) {
    title.parentNode.insertBefore(readTimeEl, title.nextSibling);
  }
});
</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>