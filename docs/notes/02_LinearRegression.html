<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.47">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Linear Regression and Optimization</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../eve.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="shortcut icon" href="../eve.ico">
<script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-GZHXTPTRRE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-GZHXTPTRRE');
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Fall 2025</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <span class="nav-link">
<span class="menu-text">Canvas</span>
    </span>
  </li>  
  <li class="nav-item">
    <span class="nav-link">
<span class="menu-text">Gradescope</span>
    </span>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../syllabus.html"> 
<span class="menu-text">Syllabus</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#supervised-learning" id="toc-supervised-learning" class="nav-link active" data-scroll-target="#supervised-learning">Supervised Learning</a></li>
  <li><a href="#univariate-linear-regression" id="toc-univariate-linear-regression" class="nav-link" data-scroll-target="#univariate-linear-regression">Univariate Linear Regression</a>
  <ul class="collapse">
  <li><a href="#linear-functions" id="toc-linear-functions" class="nav-link" data-scroll-target="#linear-functions">Linear functions</a></li>
  <li><a href="#mean-squared-error-loss" id="toc-mean-squared-error-loss" class="nav-link" data-scroll-target="#mean-squared-error-loss">Mean Squared Error Loss</a></li>
  <li><a href="#exact-optimization" id="toc-exact-optimization" class="nav-link" data-scroll-target="#exact-optimization">Exact Optimization</a></li>
  </ul></li>
  <li><a href="#multivariate-linear-regression" id="toc-multivariate-linear-regression" class="nav-link" data-scroll-target="#multivariate-linear-regression">Multivariate Linear Regression</a>
  <ul class="collapse">
  <li><a href="#linear-function" id="toc-linear-function" class="nav-link" data-scroll-target="#linear-function">Linear function</a></li>
  <li><a href="#mean-squared-error" id="toc-mean-squared-error" class="nav-link" data-scroll-target="#mean-squared-error">Mean Squared Error</a></li>
  <li><a href="#exact-optimization-1" id="toc-exact-optimization-1" class="nav-link" data-scroll-target="#exact-optimization-1">Exact Optimization</a></li>
  </ul></li>
  <li><a href="#empirical-risk-minimization" id="toc-empirical-risk-minimization" class="nav-link" data-scroll-target="#empirical-risk-minimization">Empirical Risk Minimization</a></li>
  <li><a href="#looking-forward" id="toc-looking-forward" class="nav-link" data-scroll-target="#looking-forward">Looking Forward</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><strong>Linear Regression and Optimization</strong></h1>
</div>



<div class="quarto-title-meta column-page-left">

    
  
    
  </div>
  


</header>


<section id="supervised-learning" class="level2">
<h2 class="anchored" data-anchor-id="supervised-learning">Supervised Learning</h2>
<p>Machine learning is incredibly popular. Seen extensive progress in the past several decades, and especially recently with the advent of generative AI.</p>
<p>There are many problems that fall under the umbrella of machine learning:</p>
<ul>
<li><p>Predicting temperature based on present weather conditions,</p></li>
<li><p>Identifying whether an image contains a cat or dog, and</p></li>
<li><p>Generating the next word in a sentence.</p></li>
</ul>
<p>This course is about how we go about solving these problems. The first half of the course will cover <em>supervised learning</em>, where we are given labeled data, and our goal is to train a function to approximately match the labels.</p>
<p>Concretely, we are given <span class="math inline">\(n\)</span> data points <span class="math inline">\(\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)} \in \mathbb{R}^d\)</span>, each with <span class="math inline">\(d\)</span> dimensions, and associated labels <span class="math inline">\(y^{(1)}, y^{(2)}, \ldots, y^{(n)} \in \mathbb{R}\)</span>. Our goal is to learn a function <span class="math inline">\(f: \mathbb{R}^d \to \mathbb{R}\)</span> so that <span class="math inline">\(f(\mathbf{x}^{(i)}) \approx y^{(i)}\)</span> for all data points <span class="math inline">\(i \in \{1,2,\ldots,n\}\)</span>.</p>
<p>Our general approach to solving supervised learning problems will be to use <em>empirical risk minimization</em>, which gives a flexible scaffolding that encompasses many of the topics we’ll discuss in this course. Given a function class (e.g., linear functions or neural networks), the idea is to select the function that most closely explains the data. In particular, there are three components to empirical risk minimization:</p>
<ul>
<li><p><strong>Function Class:</strong> The function class <span class="math inline">\(\mathcal{F}\)</span> from which we will select the function <span class="math inline">\(f\)</span> that most closely fits the observed data.</p></li>
<li><p><strong>Loss:</strong> The loss function that measures how well a function <span class="math inline">\(f\)</span> fits the observed data. (Without loss of generality, we will assume that lower is better.)</p></li>
<li><p><strong>Optimizer:</strong> The method of selecting the function from the function class.</p></li>
</ul>
<p>Empirical risk minimization is an abstract idea. Luckily, we will revisit it again and again. Our first example will be <em>linear regression</em>, where the function class is the set of linear functions and the loss is the squared difference between the true label and our prediction. Let’s dive in!</p>
</section>
<section id="univariate-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="univariate-linear-regression">Univariate Linear Regression</h2>
<p>Linear regression is a simple but powerful tool that we will use to understand the basics of machine learning. For simplicity, we will first consider the <em>univariate</em> case where the inputs are all one-dimensional i.e., <span class="math inline">\(x^{(1)}, \ldots, x^{(n)} \in \mathbb{R}\)</span>.</p>
<section id="linear-functions" class="level3">
<h3 class="anchored" data-anchor-id="linear-functions">Linear functions</h3>
<p>As its name suggests, linear regression uses a linear function to process the input into an approximation of the output. Let <span class="math inline">\(w \in \mathbb{R}\)</span> be a weight parameter. The linear function (for one-dimensional inputs) is given by <span class="math inline">\(f(x) = wx\)</span>.</p>
<p>Unlike many machine learning functions, we can visualize the linear function since it is given by a line. In the plot, we have the <span class="math inline">\(n=10\)</span> data points plotted in two dimensions. There is one linear function <span class="math inline">\(f(x) = 2x\)</span> that closely approximates the data and another linear function <span class="math inline">\(f(x)=\frac12 x\)</span> that poorly approximates the data.</p>
<center>
<img src="images/regression_1d.pdf" class="responsive-img">
</center>
<p>Our goal is to learn how to find a linear function that fits the data well. Before we can do this, though, we will need to define what it means for a function to “fit the data well”.</p>
</section>
<section id="mean-squared-error-loss" class="level3">
<h3 class="anchored" data-anchor-id="mean-squared-error-loss">Mean Squared Error Loss</h3>
<p>Our goal for the loss function is to measure how closely the data fits the prediction made by our function. Intuitively, we should take the difference between the prediction and the true outcome <span class="math inline">\(f(x^{(i)})-y^{(i)}\)</span>.</p>
<p>The issue with this approach is that <span class="math inline">\(f(x^{(i)})-y^{(i)}\)</span> can be small (negative) even when <span class="math inline">\(f(x^{(i)}) \neq y^{(i)}\)</span>. A natural fix is to take the absolute value <span class="math inline">\(|f(x^{(i)}) - y^{(i)}|\)</span>. The benefit of the absolute value is that the loss is <span class="math inline">\(0\)</span> if and only if <span class="math inline">\(f(x^{(i)}) = y^{(i)}\)</span>. However, the absolute value function is not differentiable, which is a property we’ll need for optimization. Instead, we use the squared loss:</p>
<p><span class="math inline">\(\mathcal{L}(w) = \frac1{n} \sum_{i=1}^n (f(x^{(i)}) - y^{(i)})^2.\)</span></p>
<p>Here, we use the mean squared error loss, which is the average squared difference between the prediction and the true output over the dataset. Unlike the absolute value function, the squared function is differentiable everywhere. In addition, the squared error disproportionately penalizes predictions that are far from the true labels, a property that may be desirable when we want <em>all</em> of our predictions to be reasonably accurate.</p>
<center>
<img src="images/regression_losses.pdf" class="responsive-img">
</center>
<p>The plot above compares the squared function to the absolute value function. While both are <span class="math inline">\(0\)</span> if and only if their input is <span class="math inline">\(0\)</span>, the squared function is differentiable everywhere and penalizes large errors more.</p>
</section>
<section id="exact-optimization" class="level3">
<h3 class="anchored" data-anchor-id="exact-optimization">Exact Optimization</h3>
<p>We now have our function class and loss function: linear functions and mean squared error loss. The question becomes how to update the weights of the function to minimize the loss. In particular, we want to find <span class="math inline">\(w\)</span> that minimizes <span class="math inline">\(\mathcal{L}(w)\)</span>. While the language we’re using is new, the problem is not. We’ve actually been studying how to do this since pre-calculus!</p>
<p>The squared loss is convex (a bowl facing up versus the downward facing <em>cave</em> of con<em>cave</em>); see the plot above for a ‘proof’ by example. In this case, we know there is only one minimum. Not only that but we can find the minimum by setting the derivative to <span class="math inline">\(0\)</span>.</p>
<p>As such, our game plan is to set <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial w}\)</span> to <span class="math inline">\(0\)</span> and solve for <span class="math inline">\(w\)</span>. Recall that <span class="math inline">\(f(x) = wx\)</span>. We will use the linearity of the derivative, the chain rule, and the power rule to compute the derivative of <span class="math inline">\(\mathcal{L}\)</span> with respect to <span class="math inline">\(w\)</span>:</p>
<p><span class="math display">\[
\begin{align}
\frac{\partial}{\partial w}[\mathcal{L}(w)]
&amp;= \frac1{n} \sum_{i=1}^n \frac{\partial}{\partial w} [(f(x^{(i)}) - y^{(i)})^2]
\notag \\&amp;= \frac1{n} \sum_{i=1}^n 2(f(x^{(i)}) - y^{(i)}) \frac{\partial}{\partial w} [(f(x^{(i)}) - y^{(i)})]
\notag \\&amp;= \frac1{n} \sum_{i=1}^n 2(w x^{(i)} - y^{(i)}) x^{(i)}.
\end{align}
\]</span></p>
<p>Setting the derivative to <span class="math inline">\(0\)</span> and solving for <span class="math inline">\(w\)</span>, we get <span class="math inline">\(\frac2{n} \sum_{i=1}^n w \cdot (x^{(i)})^2 = \frac2{n} \sum_{i=1}^n y^{(i)} x^{(i)}\)</span> and so <span class="math display">\[
w = \frac{\sum_{i=1}^n y^{(i)} \cdot x^{(i)}}{\sum_{i=1}^n (x^{(i)})^2}.
\]</span></p>
<p>This is the exact solution to the univariate linear regression problem! We can now use this formula to find the best linear function for our univariate data. However, we’ll have to work slightly harder for the general case with multidimensional data.</p>
</section>
</section>
<section id="multivariate-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="multivariate-linear-regression">Multivariate Linear Regression</h2>
<p>Consider the more general setting where the input is <span class="math inline">\(d\)</span>-dimensional. As before, we observe <span class="math inline">\(n\)</span> training observations <span class="math inline">\((\mathbf{x}^{(1)}, y^{(1)}), \ldots, (\mathbf{x}^{(n)}, y^{(n)})\)</span> but now <span class="math inline">\(\mathbf{x}^{(i)} \in \mathbb{R}^d\)</span>. We will generalize the ideas from univariate linear regression to the multivariate setting.</p>
<section id="linear-function" class="level3">
<h3 class="anchored" data-anchor-id="linear-function">Linear function</h3>
<p>Instead of using a single weight <span class="math inline">\(w \in \mathbb{R}\)</span>, we will use <span class="math inline">\(d\)</span> weights <span class="math inline">\(\mathbf{w} \in \mathbb{R}^d\)</span>. Then the function is given by <span class="math inline">\(f(x) = \langle \mathbf{w}, \mathbf{x} \rangle\)</span>.</p>
<p>Instead of using a <em>line</em> to fit the data, we use a <em>hyperplane</em>. While visualizing the function is difficult in high dimensions, we can still see the function when <span class="math inline">\(d=2\)</span>.</p>
<center>
<img src="images/regression_2d.pdf" width="600">
</center>
<p>In the plot above, we have <span class="math inline">\(n=10\)</span> data points in 3 dimensions. There is one linear function <span class="math inline">\(\mathbf{w} = \begin{bmatrix} 2 \\ \frac12 \end{bmatrix}\)</span> that closely approximates the data and another linear function <span class="math inline">\(\mathbf{w} = \begin{bmatrix} \frac12 \\ 0 \end{bmatrix}\)</span> that poorly approximates the data.</p>
</section>
<section id="mean-squared-error" class="level3">
<h3 class="anchored" data-anchor-id="mean-squared-error">Mean Squared Error</h3>
<p>Since the output of <span class="math inline">\(f\)</span> is still a single real number, we do not have to change the loss function. However, we can use our linear algebra notation to write the mean squared error in an elegant way.</p>
<p>Let <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times d}\)</span> be the data matrix where the <span class="math inline">\(i\)</span>th row is <span class="math inline">\((\mathbf{x}^{(i)})^\top\)</span>. Similarly, let <span class="math inline">\(\mathbf{y} \in \mathbb{R}^n\)</span> be the target vector where the <span class="math inline">\(i\)</span>th entry is <span class="math inline">\(y^{(i)}\)</span>. We can then write the mean squared error loss as <span class="math display">\[
\mathcal{L}(\mathbf{w}) = \frac1{n} \| \mathbf{X w - y} \|_2^2.
\]</span></p>
</section>
<section id="exact-optimization-1" class="level3">
<h3 class="anchored" data-anchor-id="exact-optimization-1">Exact Optimization</h3>
<p>Just like computing the derivative and setting it to <span class="math inline">\(0\)</span>, we can compute the gradient and set it to the zero vector <span class="math inline">\(\mathbf{0} \in \mathbb{R}^d\)</span>. In mathematical notation, we will set <span class="math inline">\(\nabla_\mathbf{w} \mathcal{L}(\mathbf{w}^*) = \mathbf{0}\)</span> and solve for <span class="math inline">\(\mathbf{w}^*\)</span>. The intuition is that such a point is a local minimum in every direction; that is, we cannot improve the loss by moving in any of the dimensions. Since the loss is convex (i.e., there can be only one minima), such a point is the unique global minimum and achieves the optimal loss.</p>
<p>As you may recall from multivariate calculus, the gradient <span class="math inline">\(\nabla_\mathbf{w} \mathcal{L}(\mathbf{w})\)</span> is simply a vector with the same dimension as <span class="math inline">\(\mathbf{w}\)</span>; the value in the <span class="math inline">\(i\)</span>th dimension is <span class="math inline">\(\frac{\partial \mathcal{L}(\mathbf{w})}{\partial w_i}\)</span>.</p>
<p>Let’s compute this quantity <span class="math display">\[
\begin{align}
\frac{\partial \mathcal{L}(\mathbf{w})}{\partial w_i}
&amp;= \lim_{\Delta \to 0} \frac{\mathcal{L}(\mathbf{w}+\Delta \mathbf{e}_i) - \mathcal{L}(\mathbf{w})}{\Delta}
\\&amp;=\lim_{\Delta \to 0} \frac{\| \mathbf{X} (\mathbf{w}+\Delta \mathbf{e}_i) - \mathbf{y}\|^2 - \| \mathbf{X} \mathbf{w} - \mathbf{y}\|^2 }{\Delta}.
\end{align}
\]</span> Let <span class="math inline">\(\mathbf{a}, \mathbf{b}\)</span> be two vectors in the same dimensional space. We have <span class="math inline">\(\|\mathbf{a} + \mathbf{b} \|^2 = \langle \mathbf{a} + \mathbf{b} , \mathbf{a} + \mathbf{b} \rangle = \| \mathbf{a} \|^2 + 2\langle \mathbf{a}, \mathbf{b} \rangle + \| \mathbf{b}\|^2\)</span>, where we foiled to reach the final equality, and used that <span class="math inline">\(\langle \mathbf{a}, \mathbf{b} \rangle = \langle \mathbf{a}, \mathbf{b} \rangle\)</span>. By letting <span class="math inline">\(\mathbf{a} = \mathbf{X w - b}\)</span> and <span class="math inline">\(\mathbf{b} = \Delta \mathbf{X} \mathbf{e}_i\)</span>, we reach <span class="math display">\[
\begin{align}
\frac{\partial \mathcal{L}(\mathbf{w})}{\partial w_i}
&amp;=\lim_{\Delta \to 0} \frac{\| \mathbf{X} \mathbf{w} - \mathbf{y}\|^2
+ 2 \langle \mathbf{X} \mathbf{w} - \mathbf{y}, \Delta \mathbf{X e}_i \rangle
+\|\Delta \mathbf{X e}_i\|^2
- \| \mathbf{X} \mathbf{w} - \mathbf{y}\|^2 }{\Delta}.
\\&amp;= \lim_{\Delta \to 0} \frac{2 \Delta \langle \mathbf{X} \mathbf{w} - \mathbf{y}, \mathbf{X e}_i \rangle
+\Delta^2 \|\mathbf{X e}_i\|^2}{\Delta}
\\&amp;= \lim_{\Delta \to 0} 2 \langle \mathbf{X} \mathbf{w} - \mathbf{y}, \mathbf{X e}_i \rangle + \Delta \|\mathbf{X e}_i\|^2
\\&amp;=\langle \mathbf{X} \mathbf{w} - \mathbf{y}, \mathbf{X e}_i \rangle.
\end{align}
\]</span> Let <span class="math inline">\(\mathbf{X}_i = \mathbf{Xe}_i\)</span> be the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mathbf{X}\)</span>. Then, the full gradient is given by <span class="math display">\[
\begin{align}
\nabla_\mathbf{w}\mathcal{L}(\mathbf{w})
= 2 \begin{bmatrix}
\mathbf{X}_1^\top (\mathbf{X} \mathbf{w} - \mathbf{y}) \\ \mathbf{X}_2^\top (\mathbf{X} \mathbf{w} - \mathbf{y}) \\ \vdots
\end{bmatrix}
\end{align}
= 2 \mathbf{X}^\top (\mathbf{X} \mathbf{w} - \mathbf{y}).
\]</span></p>
<p>By the convexity of the loss function <span class="math inline">\(\mathcal{L}\)</span>, we know that <span class="math inline">\(\nabla_\mathbf{w}\mathcal{L}(\mathbf{w}^*)=0\)</span> at the optimal weights <span class="math inline">\(\mathbf{w}^*\)</span>. Solving for <span class="math inline">\(\mathbf{w}^*\)</span> yields <span class="math display">\[
\begin{align}
\nabla_\mathbf{w}\mathcal{L}(\mathbf{w}^*)
&amp;= 2 \mathbf{X}^\top (\mathbf{X} \mathbf{w}^* - \mathbf{y})
= 0
\\ \Leftrightarrow
\mathbf{X}^\top \mathbf{X} \mathbf{w}^* &amp;= \mathbf{X}^\top \mathbf{y}
\\ \Leftrightarrow
\mathbf{w}^* &amp;= (\mathbf{X}^\top \mathbf{X})^+ \mathbf{X}^\top \mathbf{y}
\end{align}
\]</span> where <span class="math inline">\((\cdot)^+\)</span> is the pseudoinverse i.e., <span class="math inline">\((\mathbf{M})^+ \mathbf{M} = \mathbf{I}\)</span> for all symmetric matrices <span class="math inline">\(\mathbf{M}\)</span>.</p>
<p>Question: Is the pseudoinverse also defined for non-symmetric square matrices?</p>
</section>
</section>
<section id="empirical-risk-minimization" class="level2">
<h2 class="anchored" data-anchor-id="empirical-risk-minimization">Empirical Risk Minimization</h2>
<p>We have now seen how to fit a linear function to data using the mean squared error loss. However, we have not given a satisfying answer to the question:</p>
<center>
<em>Why use mean squared error as our loss function?</em>
</center>
<p><br></p>
<p>So far, our answer has been that the quadratic function is differentiable (which we use to find the optimal solution), and that it naturally penalizes predictions which are farther away more. The first point is one of convenience and, a priori, should not be particularly persuasive. The second seems somewhat arbitrary, why penalize at a quadratic rate rather than an e.g., quartic rate? We’ll now consider a more compelling answer.</p>
<p>On our way to the answer, let’s take a step back and consider another question:</p>
<center>
<em>Why fit the data with a linear function?</em>
</center>
<p><br></p>
<p>Well, we may do so when we expect the data truly has a linear relationship with the labels. To make things interesting, we will assume that there is random noise added to the labels, but that this noise is mean-centered so that, on average, the labels come from the linear model. Concretely, we observe some point <span class="math inline">\(\mathbf{x}\)</span> with a label that comes from a linear model <span class="math inline">\(\mathbf{w}^*\)</span> but with added noise, i.e., <span class="math display">\[
y= \langle \mathbf{w}^*, \mathbf{x} \rangle + \eta.
\]</span> We will model this noise as distributed from a normal distribution, i.e., <span class="math inline">\(\eta \sim \mathcal{N}(0, \sigma^2)\)</span> for some unknown standard deviation <span class="math inline">\(\sigma\)</span>. (To justify this choice, we imagine the noise as a sum of random variables from some other distribution(s) which, by the law of large numbers, will follow the normal distribution when the sum contains sufficiently many terms.)</p>
<p>Recall that the goal of our empirical risk minimization strategy is to find the function which most closely aligns with the data, or, put differently, <em>we want the function that most likely generated the data we observed</em>. In order to compute this likelihood, we will use the probability density function of the normal distribution: The probability we observe a random variable <span class="math inline">\(y\)</span> drawn from a normal distribution with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span> is given by <span class="math display">\[
\frac1{\sqrt{2\pi \sigma}} \exp\left( - \frac{(y- \mu)^2}{2\sigma^2} \right).
\]</span> If the noisy linear model <span class="math inline">\(\mathbf{w}\)</span> <em>did</em> generate the training data <span class="math inline">\((\mathbf{x}^{(i)}, y^{(i)})\)</span>, then the expectation of the generation would be <span class="math inline">\(\langle \mathbf{w}, \mathbf{x}^{(i)} \rangle\)</span>. Then, combined with the assumption that the training data was drawn independently, the probability of observing the training data is given by the product of the probabilities of each individual observation: <span class="math display">\[
\prod_{i=1}^n
\frac1{\sqrt{2\pi \sigma}} \exp\left( - \frac{(y^{(i)}- \langle \mathbf{w}, \mathbf{x}^{(i)} \rangle)^2}{2\sigma^2} \right).
\]</span> Our goal is to find the function <span class="math inline">\(\mathbf{w}\)</span> that maximizes this likelihood i.e., <span class="math display">\[
\begin{align}
&amp;{\arg\max}_{\mathbf{w} \in \mathbb{R}^d}
\prod_{i=1}^n
\frac{1}{\sqrt{2\pi \sigma}} \exp\left( - \frac{(y^{(i)}- \langle \mathbf{w}, \mathbf{x}^{(i)} \rangle)^2}{2\sigma^2} \right)
\notag \\
&amp;= {\arg\min}_{\mathbf{w} \in \mathbb{R}^d}
- \log \left(
\frac{1}{\sqrt{2\pi \sigma}} \exp\left(\sum_{i=1}^n - \frac{(y^{(i)}- \langle \mathbf{w}, \mathbf{x}^{(i)} \rangle)^2}{2\sigma^2} \right)\right)
\notag \\
&amp;= {\arg\min}_{\mathbf{w} \in \mathbb{R}^d}
- \sum_{i=1}^n - (y^{(i)}- \langle \mathbf{w}, \mathbf{x}^{(i)} \rangle)^2.
\end{align}
\]</span> Here, we used the following facts: maximizing an objective is equivalent to minimizing the negative of that objective, the logarithmic function is monotonically increasing so minimizing the likelihood is equivalent to minimizing the log-likelihood, the product of exponentials is the exponential of the sum, and removing a constant scalar factor or additive constant does not change the minimum.</p>
<p>The punchline is that the function <span class="math inline">\(\mathbf{w}\)</span> that maximizes the likelihood of observing the training data is the same function that minimizes the mean squared error loss. This is a powerful result that justifies our use of the mean squared error loss.</p>
</section>
<section id="looking-forward" class="level2">
<h2 class="anchored" data-anchor-id="looking-forward">Looking Forward</h2>
<p>While we have seen the benefits of exactly optimizing linear regression functions, there are several limitations that we will address.</p>
<p><strong>Computational Complexity</strong> We saw (and you will prove) that the exact solution to linear regression is given by <span class="math inline">\(\mathbf{w}^* = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}\)</span>. This requires building the matrix <span class="math inline">\(\mathbf{X}^\top \mathbf{X}\)</span>, which takes <span class="math inline">\(O(nd^2)\)</span> time, and then inverting it, which takes <span class="math inline">\(O(d^3)\)</span>. When we have a large number of data points <span class="math inline">\(n\)</span> and/or a large number of features <span class="math inline">\(d\)</span>, this can be prohibitively expensive.</p>
<p><strong>Function Class Misspecification</strong> We have assumed that the data has a linear relationship (or close to linear relationship) with the labels. What happens when this is not true? That is, even the best linear function gives a poor approximation?</p>



</section>

</main> <!-- /main -->
<script>
document.addEventListener("DOMContentLoaded", function () {
  const wordsPerMinute = 200;
  const text = document.body.innerText;
  const words = text.trim().split(/\s+/).length;
  const readingTime = Math.ceil(words / wordsPerMinute);

  const readTimeEl = document.createElement("div");
  readTimeEl.innerText = `⏱️ ${readingTime} min read`;

  // Style it to appear centered
  readTimeEl.style.fontSize = "0.9em";
  readTimeEl.style.margin = "1em auto";
  readTimeEl.style.textAlign = "left";
  readTimeEl.style.width = "100%";

  const title = document.querySelector("h1");
  if (title) {
    title.parentNode.insertBefore(readTimeEl, title.nextSibling);
  }
});
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>