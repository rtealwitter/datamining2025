[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "MATH 166: Syllabus",
    "section": "",
    "text": "Course Description: Data mining is the process of discovering patterns in large data sets using techniques from mathematics, computer science and statistics with applications ranging from biology and neuroscience to history and economics. The goal of the course is to teach students fundamental data mining techniques that are commonly used in practice. Students will learn advanced data mining techniques (including linear classifiers, clustering, dimension reduction, transductive learning and topic modeling).\nPrerequisites: I expect familiarity with calculus, linear regression, probability, and Python. In particular, I expect you are comfortable with derivatives, the chain rule, gradients, matrix multiplication, and probability distributions. If this isn’t the case, please contact me as soon as possible.\nStructure: We will meet on Tuesdays and Thursdays. The first section is from 2:45 to 4pm and the second section is from 4:15 to 5:30pm. I will hold my office hours TBD. If you would like to meet outside of these times, please email me.\nResources: The primary resource for this class are the typed notes on the homepage. I highly recommend reading them before each class (it should take about 15 minutes). In addition, I will post my preparation for the slides the night before each class.\nDiscussion: Please post all your course related questions on Canvas. If your question reveals your solution to a homework problem, please email me instead.\n\nGrading\nYour grade in the class will be based on the number of points \\(p\\) that you earn. You will receive an A if \\(p \\geq 93\\), an A- if \\(93 &gt; p \\geq 90\\), a B+ if \\(90 &gt; p \\geq 87\\), and so on. You may earn points through the following assignments:\n\nParticipation (10 points): The classes at CMC are intentionally small. Unless you have a reasonable excuse (e.g. sickness, family emergency), I expect you to attend every class. Whether you are able to attend or not, I expect you to fill out the form linked from the home page to receive credit for participation (one point per lecture day that you fill it out). Of course, if you are not able to attend in person, you should read the notes before filling out the form.\nProblem Sets (10 Points): Learning requires practice. Your main opportunity to practice the concepts we cover in this class will be on the problem sets. Your grade will be based on turning in solutions to each problem and, so that you engage with the solutions, a self grade of your own work. Because I do not want to incentivize the use of LLMs, I will not grade your solutions for correctness; instead, your problem set grade is based on completion and the accuracy of your own self grade.\nQuizzes (20 Points): In lieu of grading for correctness on the problem sets, I will give short quizzes at the beginning of our Tuesday classes. These quizzes will be based on the problem sets and will test your understanding of the concepts we cover in class. The quizzes will be short (10-15 minutes) and will be graded for correctness.\nWritten Exam (20 Points): The first midterm will be a written exam. It will cover the material we have covered in class up to that point. The exam will be open book and open notes, but you will not be allowed to use any electronic devices (including your phone). The exam will be graded for correctness.\nOral Exam (20 Points): The second midterm will be an oral exam. I will individually ask you questions about the concepts we have covered in class during a 30-minute meeting. The goal is to simultaneously assess your understanding of the material and give you a chance to practice explaining the concepts, as you would in a technical interview. I will provide a list of topics that I will ask about in advance.\nProject (20 Points): The final project will be a chance for you to apply the concepts we have covered in class to a real-world problem. You will select a topic we cover in class and implement an algorithm we discussed on a data set of your choosing. You will write a report describing your results and what you learned. You will also give a presentation showcasing your results to the class. Except in special circumstances, you will complete your project as an individual.\nExtra Credit: This is the first time I am teaching this class, so my typed notes are work in progress and I would love your help improving them! If you find an issue, please email me. I will give extra credit to the first person to find each typo (worth 1/4 point), ambiguous statement (worth 1/2 point), and mistake (worth 1 point)\n\nLate Policy: I expect all assignments to be turned in on time. If you are unable to turn in an assignment on time, you must email me 24 hours before the assignment is due to request an extension.\n\n\nHonor Code\nAcademic integrity is an important part of your learning experience. You are welcome to use online material and discuss problems with others but you must explicitly acknowledge the outside resources (website, person, or LLM) on the work you submit.\nLarge Language Models: LLMs are a powerful tool. However, while they are very good at producing human-like text, they have no inherent sense of ‘correctness’. You may use LLMs (as detailed below) but you are wholly responsible for the material you submit.\nYou may use LLMs for:\n\nImplementing short blocks of code that you can easily check.\nAnswering simple questions whose answers you can easily verify.\n\nDo not use LLMS for:\n\nImplementing extensive blocks of code or code that you don’t understand.\nAnswering complicated questions (like those on the problem sets) that you cannot easily verify.\n\nUltimately, the point of the assignments in this class are for you to practice the concepts. If you use an LLM in lieu of practice, then you deny yourself the chance to learn.\n\n\nAcademic Accommodations\nIf you have a Letter of Accommodation, please contact me as early in the semester as possible. If you do not have a Letter of Accommodation and you believe you are eligible, please reach out to Accessibility Services at accessibilityservices@cmc.edu."
  },
  {
    "objectID": "notes/code.html",
    "href": "notes/code.html",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\nLinear Regression Figures\n\nnp.random.seed(1234) # Seed randomness\n\nn = 10 # Number of observations\nw = 2 # True parameter\nX = np.random.rand(n) # x-values\ny = X.dot(w).T + np.random.normal(size=n) * .2 #y-values\n\nplt.scatter(X,y, color='black', label=r'Data: $(x^{(i)}, y^{(i)})$')\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\nxaxis = np.arange(0,1,.01)\nplt.plot(xaxis, xaxis*.5, label=r'Line: $f(x) = .5x$', color='red')\nplt.plot(xaxis, xaxis*w, label=r'Line: $f(x) = 2x$', color='green')\nplt.legend()\nplt.title(r'Linear Regression in $\\mathbb{R}^1$')\nplt.savefig('images/regression_1d.pdf')\n\n\n\n\n\n\n\n\n\nplt.xlabel(r'$z$')\nplt.ylabel(r'$\\mathcal{L}(z)$')\nxaxis = np.arange(-1.5,1.5,.001)\nplt.plot(xaxis, xaxis**2, label=r'Squared Loss: $\\mathcal{L}(z)=z^2$', color='blue')\nplt.plot(xaxis, np.abs(xaxis), label=r'Absolute Loss: $\\mathcal{L}(z)=|z|$', color='purple', linestyle='dotted')\nplt.legend()\nplt.title(r'Squared and Absolute Losses')\nplt.savefig('images/regression_losses.pdf')\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\n\n# Seed randomness\nnp.random.seed(1234)\nn = 10  # Number of observations\nw = np.array([2, .5])  # True parameter\nX = np.random.rand(n, 2)  # x-values\ny = X.dot(w).T + np.random.normal(size=n) * .1  # y-values\n\n# Create figure and 3D axis\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Scatter plot for data points\nax.scatter(X[:, 0], X[:, 1], y, color='black', label=r'Data: $(x_1^{(i)}, x_2^{(i)}, y^{(i)})$')\n\n# Hyperplane 1: Green\nx1 = np.arange(0, 1, .01)\nx2 = np.arange(0, 1, .01)\nX1, X2 = np.meshgrid(x1, x2)\nZ = w[0] * X1 + w[1] * X2\nax.plot_surface(X1, X2, Z, alpha=.5, color='green')\n\n# Hyperplane 2: Red\nax.plot_surface(X1, X2, .5 * X1 + 0 * X2, alpha=.5, color='red')\n\n# Labels and title\nax.set_xlabel(r'$x_1$')\nax.set_ylabel(r'$x_2$')\nax.set_zlabel(r'$y$')\nax.set_title(r'Linear Regression in $\\mathbb{R}^2$')\n\n# Manually create custom legend handles for the surfaces\nhandles = [\n    Line2D([0], [0], marker='o', color='black', markerfacecolor='black', markersize=6, label=r'Data: $(x_1^{(i)}, x_2^{(i)}, y^{(i)})$'),\n    Line2D([0], [0], color='green', lw=4, label=r'Hyperplane: $f(x) = 2x_1 + .5x_2$'),\n    Line2D([0], [0], color='red', lw=4, label=r'Hyperplane: $f(x) = .5x_1 + 0x_2$')\n]\n\n# Add legend\nplt.legend(handles=handles, loc='upper left', framealpha=1)\n\n# Save the figure\nplt.savefig('images/regression_2d.pdf', bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLogistic Regression Figures\n\nplt.xlabel(r'$z$')\n#plt.ylabel(r'$\\sigma(z)$')\nxaxis = np.arange(-10,10,.001)\nsigma = lambda z : 1 / (1+np.exp(-z))\nplt.plot(xaxis, sigma(xaxis), label=r'$\\sigma(z)$', color='blue')\nplt.legend()\nplt.title(r'Sigmoid Function')\nplt.savefig('images/logistic_sigmoid.pdf')\n\n\n\n\n\n\n\n\n\nplt.xlabel(r'$z$')\n#plt.ylabel(r'$\\sigma(z)$')\nxaxis = np.arange(-10,10,.001)\nsigma = lambda z : 1 / (1+np.exp(-z))\nplt.plot(xaxis, sigma(xaxis), label=r'$\\sigma(z)$', color='blue')\nplt.legend()\nplt.title(r'Sigmoid Function')\nplt.savefig('images/logistic_sigmoid.pdf')\n\n\n\n\n\n\n\n\n\n# Create data points\nz = np.linspace(0.1, 5, 1000)  # Avoid z=0 since ln(0) is undefined\ny = -np.log(z)\n\n# Create the figure and axis\nplt.figure(figsize=(10, 6))\nplt.plot(z, y, 'b-', linewidth=2, label='-ln(z)')\n\n# Add grid\nplt.grid(True, linestyle='--', alpha=0.7)\n\n# Add title and labels\nplt.title('Graph of -ln(z)', fontsize=14)\nplt.xlabel('z', fontsize=12)\nplt.ylabel('-ln(z)', fontsize=12)\n\n# Add legend\nplt.legend(fontsize=12)\n\n# Add horizontal and vertical axes\nplt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nplt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n\n# Set reasonable axis limits\nplt.xlim(0, 5)\nplt.ylim(-2, 3)\n\n# Adjust layout\nplt.tight_layout()\n\nplt.savefig('images/NegLog.pdf')\n# Show plot\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH 166: Data Mining",
    "section": "",
    "text": "A course on the mathematical foundations of machine learning.\n\n\n\n\nInstructor: R. Teal Witter. Please call me Teal.\nClass Times: We meet Tuesdays and Thursdays. The first section is from 2:45 to 4:00pm and the second section is from 4:15 to 5:30pm.\nOffice Hours: I will hold office hours TBD.\nParticipation: I expect you to engage in class, ask questions, and make connections. To receive credit, please fill out this form after every lecture.\nQuizzes: There will be short quizzes at the beginning of our Tues classes. These quizzes will test your understanding of the problem sets and the concepts from the prior week.\n\n\nProblem Sets: Your primary opportunity to learn the material will be on problem sets. You may work with others to solve the problems, but you must write your solutions by yourself, and explicitly acknowledge any outside help (websites, people, LLMs).\nExams: There will be two midterms: The first is a written exam that covers most of the supervised learning topics. The second is a cumulative oral exam.\nProject: You will complete a project on a topic of your choice. You will write a report and give a presentation to the class. The project is due at the end of the semester.\n\n\n\n\n\n\nWeek\n\n\nTopic\n\n\nReading\n\n\nSlides\n\n\nAssignments\n\n\n\n\nLinear Algebra and Probability Review\n\n\n\n\nTues 8/27 + Thurs 8/29\n\n\nPageRank\n\n\nNotes\n\n\n\n\n\n\n\n\nSupervised Learning\n\n\n\n\nTues 9/2 + Thurs 9/4\n\n\nLinear Regression and Optimization\n\n\nNotes\n\n\n\n\n\n\n\n\nTues 9/9 + Thurs 9/11\n\n\nLinear Regression and Gradient Descent\n\n\n\n\n\n\n\n\n\n\nTues 9/16 + Thurs 9/18\n\n\nSupport Vector Machines and Optimization\n\n\n\n\n\n\n\n\n\n\nTues 9/23 + Thurs 9/25\n\n\nLogistic Regression\n\n\n\n\n\n\n\n\n\n\nTues 9/30 + Thurs 10/2\n\n\nNeural Networks\n\n\n\n\n\n\n\n\n\n\nTues 10/7 + Thurs 10/9\n\n\nDecision Trees and Boosting\n\n\n\n\n\n\n\n\n\n\nTues 10/14 + Thurs 10/16\n\n\nWritten Exam\n\n\n\n\n\n\n\n\n\n\nTues 10/21 + Thurs 10/23\n\n\nK Nearest Neighbors and Kernel Methods\n\n\n\n\n\n\n\n\n\n\nUnsupervised Learning\n\n\n\n\nTues 10/28 + Thurs 10/30\n\n\nAutoencoders\n\n\n\n\n\n\n\n\n\n\nTues 11/4 + Thurs 11/6\n\n\nPrincipal Component Analysis\n\n\n\n\n\n\n\n\n\n\nTues 11/11 + Thurs 11/13\n\n\nSemantic Embeddings\n\n\n\n\n\n\n\n\n\n\nTues 11/18 + Thurs 11/20\n\n\n\n\n\n\n\n\n\n\n\n\nTues 11/25 + Thurs 11/27\n\n\nOral Exam\n\n\n\n\n\n\n\n\n\n\nTues 12/2 + Thurs 12/4\n\n\nInterpretability and Active Regression"
  },
  {
    "objectID": "notes/00_PageRank.html",
    "href": "notes/00_PageRank.html",
    "title": "Review via PageRank",
    "section": "",
    "text": "When I first heard about machine learning, I imagined a computer that was rewarded every time it gave the right answer. Maybe there were electric carrots and sticks that no one had bothered to tell me about? While I now know as little as I did then about computer hardware, I have learned that machine learning is fundamentally a mathematical process.\nLuckily, we’ve been learning about the very mathematical ideas that make machine learning work for years! We’ll review the basics of these concepts and then jump in to linear regression, arguably the foundation of neural networks.\n\n\nImagine a function \\(\\mathcal{L}: \\mathbb{R} \\to \\mathbb{R}\\). (Instead of the usual \\(f\\), we’ll use \\(\\mathcal{L}\\) for reasons that will soon become clear.) The mapping notation means that \\(\\mathcal{L}\\) takes a single real number as input and outputs a single real number. In general, mathematicians tell us to be careful about whether we can differentiate a function. But, we’re computer scientists so we’ll risk it for the biscuit.\nLet \\(z \\in \\mathbb{R}\\) be the input to \\(\\mathcal{L}\\). The derivative of \\(\\mathcal{L}\\) with respect to its input \\(z\\) is mathematically denoted by \\(\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\\).\nFormally, the derivative is defined as \\[\n\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\n= \\lim_{h \\to 0} \\frac{\\mathcal{L}(z + h) - \\mathcal{L}(z)}{h}.\n\\] If we were to plot \\(\\mathcal{L}\\), the derivative at a point \\(z\\) would be the slope of the tangent line to the curve at that point.\nHere are several examples of functions and their derivatives that you might remember from calculus.\n\n\n\nFunction: \\(\\mathcal{L}(z)\\) \n\n\nDerivative: \\(\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\\)\n\n\n\n\n\\[z^2\\]\n\n\n\\[2z\\]\n\n\n\n\n\\[z^a\\]\n\n\n\\[a z^{a-1}\\]\n\n\n\n\n\\[az + b\\]\n\n\n\\[a\\]\n\n\n\n\n\\[\\ln(z)\\]\n\n\n\\[\\frac{1}{z}\\]\n\n\n\n\n\\[e^z\\]\n\n\n\\[e^z\\]\n\n\n\n\n\n\nWhile working with a simple basic function is easy, we’re not always so lucky. Modern machine learning chains many many complicated functions together. Fortunately, we will think of these operations modularly.\nLet \\(g: \\mathbb{R} \\to \\mathbb{R}\\) be another function. Consider the composite function \\(g(\\mathcal{L}(z))\\).\nBy the chain rule, the derivative of \\(g(\\mathcal{L}(z))\\) with respect to \\(z\\) is \\[\n\\frac{\\partial }{\\partial z}[g(\\mathcal{L}(z))]\n= \\frac{\\partial g}{\\partial z}(\\mathcal{L}(z))\n\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)].\n\\]\nOften, we will also multiply functions together. The product rule tells us that \\[\n\\frac{\\partial }{\\partial z}[g(z) \\mathcal{L}(z)]\n= g(z) \\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\n+ \\mathcal{L}(z) \\frac{\\partial}{\\partial z}[g(z)].\n\\]\n\n\n\nIn machine learning, we process high-dimensional data so we are interested in functions with multivariate input. Consider \\(\\mathcal{L}: \\mathbb{R}^d \\to \\mathbb{R}\\). The output of the function is still a real number but the input consists of \\(d\\) real numbers. We will use the vector \\(\\mathbf{z} \\in \\mathbb{R}^d\\) to represent all \\(d\\) inputs \\(z_1, \\ldots, z_d\\).\nInstead of the derivative, we will talk use the partial derivative. The partial derivative with respect to \\(z_i\\) is denoted by \\(\\frac{\\partial}{\\partial z_i}[\\mathcal{L}(\\mathbf{z})]\\). In effect, the partial derivative tells us how \\(\\mathcal{L}\\) changes when we change \\(z_i\\), while keeping all other inputs fixed.\nThe gradient stores all the partial derivatives in a vector. The \\(i\\)th entry of this vector is given by the partial derivative of \\(\\mathcal{L}\\) with respect to \\(z_i\\). In mathematical notation, \\[\n\\nabla_\\mathbf{z} \\mathcal{L} = \\left[\\begin{smallmatrix} \\frac{\\partial}{\\partial z_1}[\\mathcal{L}(\\mathbf{z})] \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial z_d}[\\mathcal{L}(\\mathbf{z})] \\\\ \\end{smallmatrix}\\right]\n\\]\nJust like the derivative in one dimension, the gradient contains information about the slope of \\(\\mathcal{L}\\) with respect to each of the \\(d\\) dimensions in its input.\n\n\n\nVector and matrix multiplication lives at the heart of deep learning. In fact, deep learning really started to take off when researchers realized that the Graphical Processing Unit (GPU) could be used to perform gradient updates in addition to the matrix multiplication it was designed to do for gaming.\nConsider two vectors \\(\\mathbf{u} \\in \\mathbb{R}^d\\) and \\(\\mathbf{v} \\in \\mathbb{R}^d\\). We will use \\(\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^d u_i v_i\\) to denote the inner product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). The \\(\\mathcal{\\ell}_2\\)-norm of \\(v\\) is given by \\(\\|\\mathbf{v}\\|_2 = \\sqrt{\\mathbf{u} \\cdot \\mathbf{u}}\\).\nConsider two matrices: \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times m}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{m \\times n}\\) where \\(d \\neq n\\). We can only multiply two matrices when their inner dimension agrees. Because the number of columns in \\(\\mathbf{A}\\) is the same as the number of rows in \\(\\mathbf{B}\\), we can compute \\(\\mathbf{AB}\\). However, because the number of columns in \\(\\mathbf{B}\\) is not the same as the number of rows in \\(\\mathbf{A}\\), the product \\(\\mathbf{BA}\\) is not defined.\nWhen we can multiply two matrices, the \\((i,j)\\) entry in \\(\\mathbf{AB}\\) is given by the inner product between the \\(i\\)th row of \\(\\mathbf{A}\\) and the \\(j\\)th column of \\(\\mathbf{B}\\). The resulting dimensions of the matrix product will be the number of rows in \\(\\mathbf{A}\\) by the number of columns in \\(\\mathbf{B}\\).\n\n\n\nIf we have a scalar equation \\(ax = b\\), we can simply solve for \\(x\\) by dividing both sides by \\(a\\). In effect, we are applying the inverse of \\(a\\) to \\(a\\) i.e., \\(\\frac1{a} a =1\\). The same principle applies to matrices. The \\(n \\times n\\) identity matrix generalizes the scalar identity \\(1\\). This identity matrix is denoted by \\(\\mathbf{I}_{n \\times n} \\in \\mathbb{R}^{n \\times n}\\): the on-diagonal entries \\((i,i)\\) are 1 while the off-diagonal entries \\((i,j)\\) for \\(i\\neq j\\) are 0.\nConsider the matrix equation \\(\\mathbf{Ax} = \\mathbf{b}\\) where \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times d}\\), \\(\\mathbf{x} \\in \\mathbb{R}^d\\), and \\(\\mathbf{b} \\in \\mathbb{R}^d\\). (Notice that the inner dimensions of \\(\\mathbf{A}\\) and \\(\\mathbf{x}\\) agree so their multiplication is well-defined, and the resulting vector is the same dimension as \\(\\mathbf{b}\\).)\nIf we want to solve for \\(\\mathbf{x}\\), we can use the matrix inverse. For a matrix \\(\\mathbf{A}\\), we use \\(\\mathbf{A}^{-1}\\) to denote its inverse. The inverse is defined so that \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\) where \\(\\mathbf{I}_{n \\times n}\\) is the identity matrix. Then, we can solve for \\(\\mathbf{x}\\) by multiplying both sides of the equation by \\(\\mathbf{A}^{-1}\\). \\[\n\\mathbf{A}^{-1} \\mathbf{Ax} = \\mathbf{A}^{-1} \\mathbf{b}\n\\] Since \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\), we have that \\(\\mathbf{I}_{n \\times n} \\mathbf{x} = \\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}\\)."
  },
  {
    "objectID": "notes/00_PageRank.html#math-review",
    "href": "notes/00_PageRank.html#math-review",
    "title": "Review via PageRank",
    "section": "",
    "text": "When I first heard about machine learning, I imagined a computer that was rewarded every time it gave the right answer. Maybe there were electric carrots and sticks that no one had bothered to tell me about? While I now know as little as I did then about computer hardware, I have learned that machine learning is fundamentally a mathematical process.\nLuckily, we’ve been learning about the very mathematical ideas that make machine learning work for years! We’ll review the basics of these concepts and then jump in to linear regression, arguably the foundation of neural networks.\n\n\nImagine a function \\(\\mathcal{L}: \\mathbb{R} \\to \\mathbb{R}\\). (Instead of the usual \\(f\\), we’ll use \\(\\mathcal{L}\\) for reasons that will soon become clear.) The mapping notation means that \\(\\mathcal{L}\\) takes a single real number as input and outputs a single real number. In general, mathematicians tell us to be careful about whether we can differentiate a function. But, we’re computer scientists so we’ll risk it for the biscuit.\nLet \\(z \\in \\mathbb{R}\\) be the input to \\(\\mathcal{L}\\). The derivative of \\(\\mathcal{L}\\) with respect to its input \\(z\\) is mathematically denoted by \\(\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\\).\nFormally, the derivative is defined as \\[\n\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\n= \\lim_{h \\to 0} \\frac{\\mathcal{L}(z + h) - \\mathcal{L}(z)}{h}.\n\\] If we were to plot \\(\\mathcal{L}\\), the derivative at a point \\(z\\) would be the slope of the tangent line to the curve at that point.\nHere are several examples of functions and their derivatives that you might remember from calculus.\n\n\n\nFunction: \\(\\mathcal{L}(z)\\) \n\n\nDerivative: \\(\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\\)\n\n\n\n\n\\[z^2\\]\n\n\n\\[2z\\]\n\n\n\n\n\\[z^a\\]\n\n\n\\[a z^{a-1}\\]\n\n\n\n\n\\[az + b\\]\n\n\n\\[a\\]\n\n\n\n\n\\[\\ln(z)\\]\n\n\n\\[\\frac{1}{z}\\]\n\n\n\n\n\\[e^z\\]\n\n\n\\[e^z\\]\n\n\n\n\n\n\nWhile working with a simple basic function is easy, we’re not always so lucky. Modern machine learning chains many many complicated functions together. Fortunately, we will think of these operations modularly.\nLet \\(g: \\mathbb{R} \\to \\mathbb{R}\\) be another function. Consider the composite function \\(g(\\mathcal{L}(z))\\).\nBy the chain rule, the derivative of \\(g(\\mathcal{L}(z))\\) with respect to \\(z\\) is \\[\n\\frac{\\partial }{\\partial z}[g(\\mathcal{L}(z))]\n= \\frac{\\partial g}{\\partial z}(\\mathcal{L}(z))\n\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)].\n\\]\nOften, we will also multiply functions together. The product rule tells us that \\[\n\\frac{\\partial }{\\partial z}[g(z) \\mathcal{L}(z)]\n= g(z) \\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\n+ \\mathcal{L}(z) \\frac{\\partial}{\\partial z}[g(z)].\n\\]\n\n\n\nIn machine learning, we process high-dimensional data so we are interested in functions with multivariate input. Consider \\(\\mathcal{L}: \\mathbb{R}^d \\to \\mathbb{R}\\). The output of the function is still a real number but the input consists of \\(d\\) real numbers. We will use the vector \\(\\mathbf{z} \\in \\mathbb{R}^d\\) to represent all \\(d\\) inputs \\(z_1, \\ldots, z_d\\).\nInstead of the derivative, we will talk use the partial derivative. The partial derivative with respect to \\(z_i\\) is denoted by \\(\\frac{\\partial}{\\partial z_i}[\\mathcal{L}(\\mathbf{z})]\\). In effect, the partial derivative tells us how \\(\\mathcal{L}\\) changes when we change \\(z_i\\), while keeping all other inputs fixed.\nThe gradient stores all the partial derivatives in a vector. The \\(i\\)th entry of this vector is given by the partial derivative of \\(\\mathcal{L}\\) with respect to \\(z_i\\). In mathematical notation, \\[\n\\nabla_\\mathbf{z} \\mathcal{L} = \\left[\\begin{smallmatrix} \\frac{\\partial}{\\partial z_1}[\\mathcal{L}(\\mathbf{z})] \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial z_d}[\\mathcal{L}(\\mathbf{z})] \\\\ \\end{smallmatrix}\\right]\n\\]\nJust like the derivative in one dimension, the gradient contains information about the slope of \\(\\mathcal{L}\\) with respect to each of the \\(d\\) dimensions in its input.\n\n\n\nVector and matrix multiplication lives at the heart of deep learning. In fact, deep learning really started to take off when researchers realized that the Graphical Processing Unit (GPU) could be used to perform gradient updates in addition to the matrix multiplication it was designed to do for gaming.\nConsider two vectors \\(\\mathbf{u} \\in \\mathbb{R}^d\\) and \\(\\mathbf{v} \\in \\mathbb{R}^d\\). We will use \\(\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^d u_i v_i\\) to denote the inner product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). The \\(\\mathcal{\\ell}_2\\)-norm of \\(v\\) is given by \\(\\|\\mathbf{v}\\|_2 = \\sqrt{\\mathbf{u} \\cdot \\mathbf{u}}\\).\nConsider two matrices: \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times m}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{m \\times n}\\) where \\(d \\neq n\\). We can only multiply two matrices when their inner dimension agrees. Because the number of columns in \\(\\mathbf{A}\\) is the same as the number of rows in \\(\\mathbf{B}\\), we can compute \\(\\mathbf{AB}\\). However, because the number of columns in \\(\\mathbf{B}\\) is not the same as the number of rows in \\(\\mathbf{A}\\), the product \\(\\mathbf{BA}\\) is not defined.\nWhen we can multiply two matrices, the \\((i,j)\\) entry in \\(\\mathbf{AB}\\) is given by the inner product between the \\(i\\)th row of \\(\\mathbf{A}\\) and the \\(j\\)th column of \\(\\mathbf{B}\\). The resulting dimensions of the matrix product will be the number of rows in \\(\\mathbf{A}\\) by the number of columns in \\(\\mathbf{B}\\).\n\n\n\nIf we have a scalar equation \\(ax = b\\), we can simply solve for \\(x\\) by dividing both sides by \\(a\\). In effect, we are applying the inverse of \\(a\\) to \\(a\\) i.e., \\(\\frac1{a} a =1\\). The same principle applies to matrices. The \\(n \\times n\\) identity matrix generalizes the scalar identity \\(1\\). This identity matrix is denoted by \\(\\mathbf{I}_{n \\times n} \\in \\mathbb{R}^{n \\times n}\\): the on-diagonal entries \\((i,i)\\) are 1 while the off-diagonal entries \\((i,j)\\) for \\(i\\neq j\\) are 0.\nConsider the matrix equation \\(\\mathbf{Ax} = \\mathbf{b}\\) where \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times d}\\), \\(\\mathbf{x} \\in \\mathbb{R}^d\\), and \\(\\mathbf{b} \\in \\mathbb{R}^d\\). (Notice that the inner dimensions of \\(\\mathbf{A}\\) and \\(\\mathbf{x}\\) agree so their multiplication is well-defined, and the resulting vector is the same dimension as \\(\\mathbf{b}\\).)\nIf we want to solve for \\(\\mathbf{x}\\), we can use the matrix inverse. For a matrix \\(\\mathbf{A}\\), we use \\(\\mathbf{A}^{-1}\\) to denote its inverse. The inverse is defined so that \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\) where \\(\\mathbf{I}_{n \\times n}\\) is the identity matrix. Then, we can solve for \\(\\mathbf{x}\\) by multiplying both sides of the equation by \\(\\mathbf{A}^{-1}\\). \\[\n\\mathbf{A}^{-1} \\mathbf{Ax} = \\mathbf{A}^{-1} \\mathbf{b}\n\\] Since \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\), we have that \\(\\mathbf{I}_{n \\times n} \\mathbf{x} = \\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}\\)."
  },
  {
    "objectID": "notes/01_LinearRegression.html",
    "href": "notes/01_LinearRegression.html",
    "title": "Linear Regression and Optimization",
    "section": "",
    "text": "We will begin our study of deep learning in the supervised setting. In this setting, we are given labelled data with input features and an outcome. Formally, we will have \\(n\\) labelled observations \\((x^{(1)}, y^{(1)}), \\ldots, (x^{(n)}, y^{(n)})\\). In general, we will have \\(y \\in \\mathbb{R}\\). For simplicity, we will assume for now that \\(x \\in \\mathbb{R}\\).\nOur goal is to process the data and learn a function that approximates the outcomes. In mathematical notation, we want to learn a function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) so that \\(f(x^{(i)}) \\approx y^{(i)}\\) for the \\(n\\) labelled observations \\(i \\in \\{1,\\ldots,n\\}\\).\nInput and output\nStrategy: Empirical risk minimization\nBefore we dive into the specific way we will accomplish this with linear regression, let’s discuss the general deep learning framework. This three-step framework gives a flexible scaffolding that we will use to understand almost every topic in this course.\nThe three-step framework includes:\n• The Model Class: The function that we’ll use to process the input and produce a corresponding output.\n• The Loss: The function that measures the quality of the outputs from our model. (Without loss of generality, we will assume that lower is better.)\n• The Optimizer: The method of updating the model to improve the loss.\nWith these general concepts in mind, we’ll explore linear regression."
  },
  {
    "objectID": "notes/01_LinearRegression.html#the-supervised-learning-problem",
    "href": "notes/01_LinearRegression.html#the-supervised-learning-problem",
    "title": "Linear Regression and Optimization",
    "section": "",
    "text": "We will begin our study of deep learning in the supervised setting. In this setting, we are given labelled data with input features and an outcome. Formally, we will have \\(n\\) labelled observations \\((x^{(1)}, y^{(1)}), \\ldots, (x^{(n)}, y^{(n)})\\). In general, we will have \\(y \\in \\mathbb{R}\\). For simplicity, we will assume for now that \\(x \\in \\mathbb{R}\\).\nOur goal is to process the data and learn a function that approximates the outcomes. In mathematical notation, we want to learn a function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) so that \\(f(x^{(i)}) \\approx y^{(i)}\\) for the \\(n\\) labelled observations \\(i \\in \\{1,\\ldots,n\\}\\).\nInput and output\nStrategy: Empirical risk minimization\nBefore we dive into the specific way we will accomplish this with linear regression, let’s discuss the general deep learning framework. This three-step framework gives a flexible scaffolding that we will use to understand almost every topic in this course.\nThe three-step framework includes:\n• The Model Class: The function that we’ll use to process the input and produce a corresponding output.\n• The Loss: The function that measures the quality of the outputs from our model. (Without loss of generality, we will assume that lower is better.)\n• The Optimizer: The method of updating the model to improve the loss.\nWith these general concepts in mind, we’ll explore linear regression."
  },
  {
    "objectID": "notes/01_LinearRegression.html#univariate-linear-regression",
    "href": "notes/01_LinearRegression.html#univariate-linear-regression",
    "title": "Linear Regression and Optimization",
    "section": "Univariate Linear Regression",
    "text": "Univariate Linear Regression\nLinear regression is a simple and powerful tool that we will use to understand the basics of deep learning.\n\nLinear Model\nAs its name suggests, linear regression uses a linear model to process the input into an approximation of the output. Let \\(w \\in \\mathbb{R}\\) be a weight parameter. The linear model (for one-dimensional inputs) is given by \\(f(x) = wx\\).\nUnlike many deep learning models, we can visualize the linear model since it is given by a line. In the plot, we have the \\(n=10\\) data points plotted in 2 dimensions. There is one linear model \\(f(x) = 2x\\) that closely approximates the data and another linear model \\(f(x)=\\frac12 x\\) that does not approximate the data.\n\n\n\nOur goal is to learn how to find a linear model that fits the data well. Before we can do this though, we will need to define what it means for a model to “fit the data well”.\n\n\nMean Squared Error Loss\nOur goal for the loss function is to measure how closely the data fits the prediction made by our model. Intuitively, we should take the difference between the prediction and the true outcome \\(f(x^{(i)})-y^{(i)}\\).\nThe issue with this approach is that \\(f(x^{(i)})-y^{(i)}\\) can be small (negative) even when \\(f(x^{(i)}) \\neq y^{(i)}\\). A natural fix is to take the absolute value \\(|f(x^{(i)}) - y^{(i)}|\\). The benefit of the absolute value is that the loss is \\(0\\) if and only if \\(f(x^{(i)}) = y^{(i)}\\). However, the absolute value function is not differentiable, which is a property we’ll need for optimization. Instead, we use the squared loss:\n\\(\\mathcal{L}(w) = \\frac1{n} \\sum_{i=1}^n (f(x^{(i)}) - y^{(i)})^2\\)\nHere, we use the mean squared error loss, which is the average squared difference between the prediction and the true output over the dataset. Unlike the absolute value function, the squared function is differentiable everywhere. In addition, the squared error penalizes predictions that are far from the true output even more.\n\n\n\nThe plot above compares the squared function to the absolute value function. While both are \\(0\\) if and only if their input is \\(0\\), the squared function is differentiable everywhere and penalizes large errors more.\n\n\nExact Optimization\nWe now have our model and loss function: the linear model and mean squared error loss. The question becomes how to update the weights of the model to minimize the loss. In particular, we want to find \\(w\\) that minimizes \\(\\mathcal{L}(w)\\). While the language we’re using is new, the problem is not. We’ve actually been studying how to do this since pre-calculus!\nThe squared loss is convex (a bowl facing up versus the downward facing cave of concave); see the plot above for a ‘proof’ by example. In this case, we know there is only one minimum. Not only that but we can find the minimum by setting the derivative to \\(0\\)!\nAs such, our game plan is to set \\(\\frac{\\partial \\mathcal{L}}{\\partial w}\\) to \\(0\\) and solve for \\(w\\). Recall that \\(f(x) = wx\\). We will use the linearity of the derivative, the chain rule, and the power rule to compute the derivative of \\(\\mathcal{L}\\) with respect to \\(w\\):\n\\[\n\\frac{\\partial}{\\partial w}[\\mathcal{L}(w)]\n= \\frac1{n} \\sum_{i=1}^n \\frac{\\partial}{\\partial w} [(f(x^{(i)}) - y^{(i)})^2]\n= \\frac1{n} \\sum_{i=1}^n 2(f(x^{(i)}) - y^{(i)}) \\frac{\\partial}{\\partial w} [(f(x^{(i)}) - y^{(i)})]\n= \\frac1{n} \\sum_{i=1}^n 2(w x^{(i)} - y^{(i)}) x^{(i)}.\n\\]\nSetting the derivative to \\(0\\) and solving for \\(w\\), we get \\(\\frac2{n} \\sum_{i=1}^n w (x^{(i)})^2 = \\frac2{n} \\sum_{i=1}^n y^{(i)} x^{(i)}\\) and so \\[\nw = \\frac{\\sum_{i=1}^n y^{(i)}}{\\sum_{i=1}^n (x^{(i)})^2}.\n\\]\nThis is the exact solution to the univariate linear regression problem! We can now use this formula to find the best linear model for our data. But we’re not done with linear regression yet. We assumed that the input was one-dimensional; however, we often have high-dimensional data."
  },
  {
    "objectID": "notes/01_LinearRegression.html#multivariate-linear-regression",
    "href": "notes/01_LinearRegression.html#multivariate-linear-regression",
    "title": "Linear Regression and Optimization",
    "section": "Multivariate Linear Regression",
    "text": "Multivariate Linear Regression\nConsider the more general setting where the input is \\(d\\)-dimensional. As before, we observe \\(n\\) training observations \\((\\mathbf{x}^{(1)}, y^{(1)}), \\ldots, (\\mathbf{x}^{(n)}, y^{(n)})\\) but now \\(\\mathbf{x}^{(i)} \\in \\mathbb{R}^d\\). We will generalize the ideas from univariate linear regression to the multivariate setting.\n\nLinear Model\nInstead of using a single weight \\(w \\in \\mathbb{R}\\), we will use \\(d\\) weights \\(\\mathbf{w} \\in \\mathbb{R}^d\\). Then the model is given by \\(f(x) = \\mathbf{w} \\cdot \\mathbf{x}\\).\nInstead of using a line to fit the data, we use a hyperplane. While visualizing the model is difficult in high dimensions, we can still see the model when \\(d=2\\).\n\n\n\nIn the plot above, we have \\(n=10\\) data points in 3 dimensions. There is one linear model \\(f(\\mathbf{x}) = \\begin{bmatrix} 2 \\\\ \\frac12 \\end{bmatrix} \\cdot \\mathbf{x}\\) that closely approximates the data and another linear model \\(f(\\mathbf{x}) = \\begin{bmatrix} \\frac12 \\\\ 0 \\end{bmatrix} \\cdot \\mathbf{x}\\) that does not approximate the data.\n\n\nMean Squared Error\nSince the output of \\(f\\) is still a single real number, we do not have to change the loss function. However, we can use our linear algebra notation to write the mean squared error in an elegant way.\nLet \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) be the data matrix where the \\(i\\)th row is \\((\\mathbf{x}^{(i)})^\\top\\). Similarly, let \\(\\mathbf{y} \\in \\mathbf{R}^n\\) be the target vector where the \\(i\\)th entry is \\(y^{(i)}\\). We can write the mean squared error loss as \\[\n\\mathcal{L}(\\mathbf{w}) = \\frac1{n} \\| \\mathbf{X w - y} \\|_2^2.\n\\]\n\n\nExact Optimization\nJust like computing the derivative and setting it to \\(0\\), we can compute the gradient and set it to the zero vector \\(\\mathbf{0} \\in \\mathbb{R}^d\\). In mathematical notation, we will set \\(\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}) = \\mathbf{0}\\) and solve for \\(\\mathbf{w}\\).\nWe will leave this as an exercise on the homework. The final solution is that \\(\\mathbf{w} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\\).\nThis is the exact solution to the multivariate linear regression problem!"
  },
  {
    "objectID": "notes/01_LinearRegression.html#empirical-risk-minimization",
    "href": "notes/01_LinearRegression.html#empirical-risk-minimization",
    "title": "Linear Regression and Optimization",
    "section": "Empirical Risk Minimization",
    "text": "Empirical Risk Minimization"
  },
  {
    "objectID": "notes/01_LinearRegression.html#looking-forward",
    "href": "notes/01_LinearRegression.html#looking-forward",
    "title": "Linear Regression and Optimization",
    "section": "Looking Forward",
    "text": "Looking Forward\nComplexity of computing the exact solution\nWhat happens when the data does not have a linear relationship? I.e., even the best linear model gives a poor approximation."
  }
]