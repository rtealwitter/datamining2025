[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "MATH 166: Syllabus",
    "section": "",
    "text": "Course Description: Data mining is the process of discovering patterns in large data sets using techniques from mathematics, computer science and statistics with applications ranging from biology and neuroscience to history and economics. The goal of the course is to teach students fundamental data mining techniques that are commonly used in practice. Students will learn advanced data mining techniques (including linear classifiers, clustering, dimension reduction, transductive learning and topic modeling).\nPrerequisites: I expect familiarity with calculus, linear regression, probability, and Python. In particular, I expect you are comfortable with derivatives, the chain rule, gradients, matrix multiplication, and probability distributions. If this isn’t the case, please contact me as soon as possible.\nStructure: We will meet on Tuesdays and Thursdays. The first section is from 2:45 to 4pm and the second section is from 4:15 to 5:30pm. I will hold my office hours TBD. If you would like to meet outside of these times, please email me.\nResources: The primary resource for this class are the typed notes on the homepage. I highly recommend reading them before each class (it should take about 15 minutes). In addition, I will post my preparation for the slides the night before each class.\nDiscussion: Please post all your course related questions on Canvas. If your question reveals your solution to a homework problem, please email me instead.\n\nGrading\nYour grade in the class will be based on the number of points \\(p\\) that you earn. You will receive an A if \\(p \\geq 93\\), an A- if \\(93 &gt; p \\geq 90\\), a B+ if \\(90 &gt; p \\geq 87\\), and so on. You may earn points through the following assignments:\n\nParticipation (10 points): The classes at CMC are intentionally small. Unless you have a reasonable excuse (e.g. sickness, family emergency), I expect you to attend every class. Whether you are able to attend or not, I expect you to fill out the form linked from the home page to receive credit for participation (one point per lecture day that you fill it out). Of course, if you are not able to attend in person, you should read the notes before filling out the form.\nProblem Sets (10 Points): Learning requires practice. Your main opportunity to practice the concepts we cover in this class will be on the problem sets. Your grade will be based on turning in solutions to each problem and, so that you engage with the solutions, a self grade of your own work. Because I do not want to incentivize the use of LLMs, I will not grade your solutions for correctness; instead, your problem set grade is based on completion and the accuracy of your own self grade.\nQuizzes (20 Points): In lieu of grading for correctness on the problem sets, I will give short quizzes at the beginning of our Tuesday classes. These quizzes will be based on the problem sets and will test your understanding of the concepts we cover in class. The quizzes will be short (10-15 minutes) and will be graded for correctness.\nWritten Exam (20 Points): The first midterm will be a written exam. It will cover the material we have covered in class up to that point. The exam will be open book and open notes, but you will not be allowed to use any electronic devices (including your phone). The exam will be graded for correctness.\nOral Exam (20 Points): The second midterm will be an oral exam. I will individually ask you questions about the concepts we have covered in class during a 30-minute meeting. The goal is to simultaneously assess your understanding of the material and give you a chance to practice explaining the concepts, as you would in a technical interview. I will provide a list of topics that I will ask about in advance.\nProject (20 Points): The final project will be a chance for you to apply the concepts we have covered in class to a real-world problem. You will select a topic we cover in class and implement an algorithm we discussed on a data set of your choosing. You will write a report describing your results and what you learned. You will also give a presentation showcasing your results to the class. Except in special circumstances, you will complete your project as an individual.\nExtra Credit: This is the first time I am teaching this class, so my typed notes are work in progress and I would love your help improving them! If you find an issue, please email me. I will give extra credit to the first person to find each typo (worth 1/4 point), ambiguous statement (worth 1/2 point), and mistake (worth 1 point)\n\nLate Policy: I expect all assignments to be turned in on time. If you are unable to turn in an assignment on time, you must email me 24 hours before the assignment is due to request an extension.\n\n\nHonor Code\nAcademic integrity is an important part of your learning experience. You are welcome to use online material and discuss problems with others but you must explicitly acknowledge the outside resources (website, person, or LLM) on the work you submit.\nLarge Language Models: LLMs are a powerful tool. However, while they are very good at producing human-like text, they have no inherent sense of ‘correctness’. You may use LLMs (as detailed below) but you are wholly responsible for the material you submit.\nYou may use LLMs for:\n\nImplementing short blocks of code that you can easily check.\nAnswering simple questions whose answers you can easily verify.\n\nDo not use LLMS for:\n\nImplementing extensive blocks of code or code that you don’t understand.\nAnswering complicated questions (like those on the problem sets) that you cannot easily verify.\n\nUltimately, the point of the assignments in this class are for you to practice the concepts. If you use an LLM in lieu of practice, then you deny yourself the chance to learn.\n\n\nAcademic Accommodations\nIf you have a Letter of Accommodation, please contact me as early in the semester as possible. If you do not have a Letter of Accommodation and you believe you are eligible, please reach out to Accessibility Services at accessibilityservices@cmc.edu."
  },
  {
    "objectID": "notes/code.html",
    "href": "notes/code.html",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D"
  },
  {
    "objectID": "notes/02_NonlinearRegression.html",
    "href": "notes/02_NonlinearRegression.html",
    "title": "Gradient Descent and Non-linear Regression",
    "section": "",
    "text": "Today, we continue our discussion of supervised learning, where we have labeled training data and our goal is to train a model that accurately predicts the labels of unseen testing data. Recall that our general approach to supervised learning is to use empirical risk minimization: We focus on a class of models, define a loss function that quantifies how accurately a particular model explains the training data, and search for a model with low loss.\nLast week, we considered the class of linear models, i.e., the prediction is a weighted linear combination of the input features. We chose to measure loss via mean squared error, a choice both rooted in convenience and a compelling modeling assumption (if the data is generated by a linear process with Gaussian, then the mean squared error is the maximum likelihood estimator). In order to find the best parameters of the linear model, we used our knowledge of gradients to exactly compute the parameters that minimize the mean squared error loss.\nThis week, we will address two of the nagging issues with computing the best parameters of a linear model. We begin with the issue of runtime; computing the optimal parameters requires building a large matrix and inverting it, which can be computationally expensive. We will now see how we can use gradient descent to speed up this process."
  },
  {
    "objectID": "notes/02_NonlinearRegression.html#gradient-descent",
    "href": "notes/02_NonlinearRegression.html#gradient-descent",
    "title": "Gradient Descent and Non-linear Regression",
    "section": "Gradient Descent",
    "text": "Gradient Descent\nThe mean squared error of a linear model is particularly well-behaved because it is convex i.e., there is a single minimum. Previously, we computed the parameters where the gradient is 0, which, by convexity, immediately gave us the single optimal point. However, we could instead use a more relaxed approach; rather than jumping immediately to the best parameters, we can iterate towards better parameters by taking steps towards lower loss.\n\n\n\nGradient descent is an iterative method for moving in the direction of steepest descent. Concretely, the process produces a sequence of parameters \\(\\mathbf{w}^{(1)}, \\mathbf{w}^{(2)}, \\ldots\\). At each step, we compute the direction of steepest ascent i.e., the gradient of the loss function with respect to each parameter. The gradient quantifies how the loss function responds as we tweak each parameter. If the partial derivative is positive (increasing the parameter increases the loss), then we will want to decrease the parameter. Analogously, if the partial derivative is negative (increasing the parameter decreases the loss), then we will want to increase the parameter. In both cases, we are moving in the direction away from the gradient. Hence, we reach the next parameter vector by subtracting the gradient from the current parameter vector: \\[\n\\begin{align*}\n\\mathbf{w}^{(t+1)} \\gets \\mathbf{w}^{(t)} - \\alpha \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}^{(t)}),\n\\end{align*}\n\\] where \\(\\alpha\\) is a small positive constant called the step size or learning rate.\nFor linear models, we already know the gradient of the mean squared error loss: \\[\n\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}) = \\frac2{n} \\mathbf{X}^\\top (\\mathbf{X w - y}).\n\\] In contrast to the \\(O(nd^2 + d^3)\\) time required to compute the exact solution, we can now compute the gradient in \\(O(nd)\\) time, as long as we restrict ourselves to matrix-vector multiplications rather than matrix-matrix multiplications. The final time complexity of gradient descent is \\(O(T nd)\\), where \\(T\\) is the number of iterations of gradient descent.\nWhile we have achieved a significant speedup, \\(O(T nd)\\) could still be prohibitively large when we have a large number of data points \\(n\\) and/or a large number of features \\(d\\). Our solution will be a stochastic approach, where we only use a small random subset of the data to compute the gradient.\n\nStochastic Gradient Descent\nOur approach will be similar to gradient descent, except now we will compute the gradient using only the data in the batch. For the mean squared error loss, we can write the loss function for a random subset \\(S\\) of the data as \\[\n\\mathcal{L}_S(\\mathbf{w}) = \\frac1{|S|} \\sum_{i \\in S} (f(\\mathbf{x}^{(i)}) - y^{(i)})^2.\n\\] Then, for our linear model, the gradient of the loss function with respect to the parameters \\(\\mathbf{w}\\) is given by \\[\n\\nabla_\\mathbf{w} \\mathcal{L}_S(\\mathbf{w}) = \\frac2{|S|} \\mathbf{X}_S^\\top (\\mathbf{X}_S \\mathbf{w} - \\mathbf{y}_S),\n\\] where \\(\\mathbf{X}_S\\) is the data matrix for the subset \\(S\\) and \\(\\mathbf{y}_S\\) is the target vector for the subset \\(S\\). One iteration of stochastic gradient descent takes time \\(O(|S|d)\\), which can be much faster than the \\(O(nd)\\) time required to compute the gradient for the full dataset.\n\n\nAdaptive Step Sizes\nThe step size \\(\\alpha\\) is a crucial hyperparameter in gradient descent. If \\(\\alpha\\) is too small, then the algorithm will take a long time to converge because it will take small steps towards the minimum. If \\(\\alpha\\) is too large, then the algorithm may overshoot the minimum and diverge by repeatedly moving in the right direction but by too much. Instead, we want to choose a step size \\(\\alpha\\) that is just right, allowing us to make progress towards the minimum without overshooting.\n\n\n\nThere are several strategies for choosing the step size:\n\nWhen searching manually, we often exponentially increase and decrease the step size i.e., multiply by a factor of \\(2\\) or \\(1/2\\). If the loss consistently improves over several iterations of gradient descent, then we try increasing the step size; if the loss is unstable, then we try decreasing the step size.\nLearning rate schedules offer a more automated approach, where we start with a large step size and then decrease it over time. This is often done by multiplying the step size by a factor less than \\(1\\) after each iteration. The intuition is that we want to take large steps at the beginning to quickly find a good region of the parameter space, and then take smaller steps so as to not overshoot the minima as we get closer.\nAn even more automated approach is to use an adaptive learning rate, where we adjust the step size based on the gradient. If the gradient is large, then we can decrease the step size to avoid overshooting; if the gradient is small, then we can increase the step size to speed up convergence. One implementation of this idea is as follows: \\[\n\\alpha^{(t+1)} \\gets \\frac{\\alpha^{(t)}}{(\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}^{(t)}))^2}.\n\\] Notice that the division is element-wise, so we are adjusting the step size for each parameter individually based on the squared partial derivative of that parameter.\n\nIn addition the step size, the direction of each step is also important.\n\n\nMomentum\nThe idea of gradient descent is to converge to a local minimum of the loss function, but things can go wrong even if we have the right step size: The gradient may not point in the direction of the minima if, for example, the loss function is not symmetric. The plot illustrates this issue for a convex loss function on two parameters \\({w}_1\\) and \\({w}_2\\), where the loss function is given by level sets. In the plot, a standard gradient descent approach takes many steps but the directions cancel out, resulting in a zig-zag pattern that slows convergence.\n\n\n\nOur solution is to keep track of the direction we have been moving in and use that to inform our next step. This idea, called momentum, retains a running average of the gradients, which allows us to smooth out the direction of the steps. We can think of momentum as a ball rolling down a hill, even when the ball is pushed left or right, it will continue to roll downwards. An implementation of momentum is as follows: \\[\n\\begin{align}\n\\mathbf{m}^{(t+1)} &= \\beta \\mathbf{m}^{(t)} + (1 - \\beta) \\nabla_\\mathbf{w} \\mathcal{L}_S(\\mathbf{w}^{(t)}) \\\\\n\\mathbf{w}^{(t+1)} &= \\mathbf{w}^{(t)} - \\alpha \\mathbf{m}^{(t+1)},\n\\end{align}\n\\] where \\(\\beta\\) is a hyperparameter that controls the amount of history we keep in the momentum vector \\(\\mathbf{m}^{(t)}\\)."
  },
  {
    "objectID": "notes/02_NonlinearRegression.html#non-linear-models",
    "href": "notes/02_NonlinearRegression.html#non-linear-models",
    "title": "Gradient Descent and Non-linear Regression",
    "section": "Non-Linear Models",
    "text": "Non-Linear Models\nWe have seen how gradient descent can be used to efficiently optimize linear models, but what if the relationship between the inputs and outputs is not linear? We can still use gradient descent to optimize non-linear models, but we need to expand our model class to include non-linear functions.\n\n\n\nIn the right of the figure, we have a linear model, which uses a linear combination of the inputs to make predictions. We can generalize this approach by repeatedly combining linear models to create a more complex model. In particular, we create several neurons, each of which is a linear model that takes the inputs and produces an output. Each neuron computes a linear combination of the inputs, and then we linearly combine the outputs of the neurons, and so on. The resulting model is a neural network, which combines linear models to create a more complex model.\nWe can think of a fully connected layer (all neurons in one layer are connected to all neurons in the next layer) as matrix multiplication, where the matrix \\(\\mathbf{W}^{(\\ell)} \\in \\mathbb{R}^{d_{\\ell} \\times d_{\\ell+1}}\\) is the weight matrix and \\(\\mathbf{x} \\in \\mathbb{R}^{d_\\text{in}}\\) is the input to the \\(\\ell\\)th layer. Then multiplying several weight matrices together gives \\[\nf(\\mathbf{x}) = \\mathbf{W}^{(k)} \\mathbf{W}^{(k-1)} \\cdots \\mathbf{W}^{(1)} \\mathbf{x}\n\\] where \\(k\\) is the number of layers in the neural network. But, there’s an issue with this approach: the output is still a linear combination of the inputs, which means that the model is still linear. Put differently, we could just multiply all the weight matrices together to get a single weight matrix \\(\\mathbf{W} = \\mathbf{W}_k \\mathbf{W}_{k-1} \\cdots \\mathbf{W}_1\\) before ever seeing the input \\(\\mathbf{x}\\). Our solution is to add a non-linear activation function after each layer. Formally, we apply activation functions \\(\\sigma: \\mathbb{R} \\to \\mathbb{R}\\) element-wise to the output of each layer. Examples of common activation functions include:\n\nReLU (Rectified Linear Unit): \\(\\sigma(x) = \\max(0, x)\\), which is a piecewise linear function that is \\(0\\) for negative inputs and linear for positive inputs.\nSigmoid: \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\), which is a smooth function that maps inputs to the range \\((0, 1)\\).\nTanh (Hyperbolic Tangent): \\(\\sigma(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\), which is another smooth function that maps inputs to the range \\((-1, 1)\\).\n\nNow we can write the output of the neural network as \\[\nf(\\mathbf{x}) = \\sigma(\\mathbf{W}_k \\sigma(\\mathbf{W}_{k-1} \\cdots \\sigma(\\mathbf{W}_1 \\mathbf{x}))).\n\\] Thanks to the non-linear activation functions, the output is no longer a linear combination of the inputs, but rather a complex function that can capture non-linear relationships between the inputs and outputs. Neural networks are so complex, in fact, that we don’t really understand how they work. But we can still apply gradient descent. In summary, we can think of neural networks in following empirical risk minimization formulation:\n\nModel Class: The model class is the set of neural networks with a given architecture (type of layer, number of layers, number of neurons per layer, and activation function).\nLoss: The loss function is the mean squared error loss.\nOptimizer: The optimizer is gradient descent.\n\n\nComplexity vs. Generalization\nNeural networks are incredibly powerful models that can approximate any function, given enough data and parameters. However, this power comes with a cost: neural networks are very complex models that can easily overfit the training data. That is, they can learn to memorize, but fail to generalize to unseen data.\n\n\n\nWhen we believe our data comes from a simpler generating process, it makes sense to use a simpler model. Even when that simpler generating process is not a linear model, we can attempt to find simpler models through regularization. Regularization is a technique that adds a penalty to the loss function to discourage the model from fitting the training data too closely. The idea is that we can keep the model “simple” by penalizing large weights, which would otherwise allow the model to achieve large changes in the output for small changes in the input.\n\n\n\nIn the plot, we see data generated from a quadratic function. The linear model is too simple and fails to capture the underlying relationship, while the standard neural network is too complex and overfits the training data. The regularized neural network, however, is able to capture the underlying relationship while still being simple enough to generalize to unseen data."
  },
  {
    "objectID": "notes/02_NonlinearRegression.html#going-forward",
    "href": "notes/02_NonlinearRegression.html#going-forward",
    "title": "Gradient Descent and Non-linear Regression",
    "section": "Going Forward",
    "text": "Going Forward\nToday, we got a taste of how to use gradient descent to optimize non-linear models, particularly neural networks. This is a rich area that has seen incredible recent advancements, particularly in the context of generative AI. In fact, I teach an entire course dedicated to deep learning. In this course, however, we will instead focus on the mathematical foundations of machine learning.\nWe have so far explored supervised learning in the regression setting, where the labels are real numbers. going forward, we will consider how to handle the case where the labels are categorical, e.g., we want to classify the data into two categories such as “cat” and “dog” or “spam” and “not spam”."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH 166: Data Mining",
    "section": "",
    "text": "A course on the mathematical foundations of machine learning.\n\n\n\n\nInstructor: R. Teal Witter. Please call me Teal.\nClass Times: We meet Tuesdays and Thursdays. The first section is from 2:45 to 4:00pm and the second section is from 4:15 to 5:30pm.\nOffice Hours: I will hold office hours TBD.\nParticipation: I expect you to engage in class, ask questions, and make connections. To receive credit, please fill out this form after every lecture.\nQuizzes: There will be short quizzes at the beginning of our Tues classes. These quizzes will test your understanding of the problem sets and the concepts from the prior week.\n\n\nProblem Sets: Your primary opportunity to learn the material will be on problem sets. You may work with others to solve the problems, but you must write your solutions by yourself, and explicitly acknowledge any outside help (websites, people, LLMs).\nExams: There will be two midterms: The first is a written exam that covers most of the supervised learning topics. The second is a cumulative oral exam.\nProject: You will complete a project on a topic of your choice. You will write a report and give a presentation to the class. The project is due at the end of the semester.\n\n\n\n\n\n\nWeek\n\n\nTopic\n\n\nReading\n\n\nSlides\n\n\nAssignments\n\n\n\n\nLinear Algebra and Probability Review\n\n\n\n\nTues 8/27 + Thurs 8/29\n\n\nPageRank\n\n\nNotes\n\n\n\n\n\n\n\n\nSupervised Learning\n\n\n\n\nTues 9/2 + Thurs 9/4\n\n\nLinear Regression and Optimization\n\n\nNotes\n\n\n\n\n\n\n\n\nTues 9/9 + Thurs 9/11\n\n\nGradient Descent and Non-linear Regression\n\n\nNotes\n\n\n\n\n\n\n\n\nTues 9/16 + Thurs 9/18\n\n\nSupport Vector Machines and Optimization\n\n\n\n\n\n\n\n\n\n\nTues 9/23 + Thurs 9/25\n\n\nLogistic Regression\n\n\n\n\n\n\n\n\n\n\nTues 9/30 + Thurs 10/2\n\n\nDecision Trees and Boosting\n\n\n\n\n\n\n\n\n\n\nTues 10/7 + Thurs 10/9\n\n\nK Nearest Neighbors and Kernel Methods\n\n\n\n\n\n\n\n\n\n\nTues 10/14 + Thurs 10/16\n\n\nWritten Exam\n\n\n\n\n\n\n\n\n\n\nUnsupervised Learning\n\n\n\n\nTues 10/21 + Thurs 10/23\n\n\nAutoencoders\n\n\n\n\n\n\n\n\n\n\nTues 10/28 + Thurs 10/30\n\n\nPrincipal Component Analysis\n\n\n\n\n\n\n\n\n\n\nTues 11/4 + Thurs 11/6\n\n\nSemantic Embeddings\n\n\n\n\n\n\n\n\n\n\nTues 11/11 + Thurs 11/13\n\n\nReinforcement Learning and Policy Gradients\n\n\n\n\n\n\n\n\n\n\nTues 11/18 + Thurs 11/20\n\n\nReinforcement Learning and Q-Learning\n\n\n\n\n\n\n\n\n\n\nTues 11/25 + Thurs 11/27\n\n\nOral Exam\n\n\n\n\n\n\n\n\n\n\nTues 12/2 + Thurs 12/4\n\n\nInterpretability and Active Regression"
  },
  {
    "objectID": "notes/00_PageRank.html",
    "href": "notes/00_PageRank.html",
    "title": "Review via PageRank",
    "section": "",
    "text": "When I first heard about machine learning, I imagined a computer that was rewarded every time it gave the right answer. Maybe there were electric carrots and sticks that no one had bothered to tell me about? While I now know as little as I did then about computer hardware, I have learned that machine learning is fundamentally a mathematical process.\nLuckily, we’ve been learning about the very mathematical ideas that make machine learning work for years! We’ll review the basics of these concepts and then jump in to linear regression, arguably the foundation of neural networks.\n\n\nImagine a function \\(\\mathcal{L}: \\mathbb{R} \\to \\mathbb{R}\\). (Instead of the usual \\(f\\), we’ll use \\(\\mathcal{L}\\) for reasons that will soon become clear.) The mapping notation means that \\(\\mathcal{L}\\) takes a single real number as input and outputs a single real number. In general, mathematicians tell us to be careful about whether we can differentiate a function. But, we’re computer scientists so we’ll risk it for the biscuit.\nLet \\(z \\in \\mathbb{R}\\) be the input to \\(\\mathcal{L}\\). The derivative of \\(\\mathcal{L}\\) with respect to its input \\(z\\) is mathematically denoted by \\(\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\\).\nFormally, the derivative is defined as \\[\n\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\n= \\lim_{h \\to 0} \\frac{\\mathcal{L}(z + h) - \\mathcal{L}(z)}{h}.\n\\] If we were to plot \\(\\mathcal{L}\\), the derivative at a point \\(z\\) would be the slope of the tangent line to the curve at that point.\nHere are several examples of functions and their derivatives that you might remember from calculus.\n\n\n\nFunction: \\(\\mathcal{L}(z)\\) \n\n\nDerivative: \\(\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\\)\n\n\n\n\n\\[z^2\\]\n\n\n\\[2z\\]\n\n\n\n\n\\[z^a\\]\n\n\n\\[a z^{a-1}\\]\n\n\n\n\n\\[az + b\\]\n\n\n\\[a\\]\n\n\n\n\n\\[\\ln(z)\\]\n\n\n\\[\\frac{1}{z}\\]\n\n\n\n\n\\[e^z\\]\n\n\n\\[e^z\\]\n\n\n\n\n\n\nWhile working with a simple basic function is easy, we’re not always so lucky. Modern machine learning chains many many complicated functions together. Fortunately, we will think of these operations modularly.\nLet \\(g: \\mathbb{R} \\to \\mathbb{R}\\) be another function. Consider the composite function \\(g(\\mathcal{L}(z))\\).\nBy the chain rule, the derivative of \\(g(\\mathcal{L}(z))\\) with respect to \\(z\\) is \\[\n\\frac{\\partial }{\\partial z}[g(\\mathcal{L}(z))]\n= \\frac{\\partial g}{\\partial z}(\\mathcal{L}(z))\n\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)].\n\\]\nOften, we will also multiply functions together. The product rule tells us that \\[\n\\frac{\\partial }{\\partial z}[g(z) \\mathcal{L}(z)]\n= g(z) \\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\n+ \\mathcal{L}(z) \\frac{\\partial}{\\partial z}[g(z)].\n\\]\n\n\n\nIn machine learning, we process high-dimensional data so we are interested in functions with multivariate input. Consider \\(\\mathcal{L}: \\mathbb{R}^d \\to \\mathbb{R}\\). The output of the function is still a real number but the input consists of \\(d\\) real numbers. We will use the vector \\(\\mathbf{z} \\in \\mathbb{R}^d\\) to represent all \\(d\\) inputs \\(z_1, \\ldots, z_d\\).\nInstead of the derivative, we will talk use the partial derivative. The partial derivative with respect to \\(z_i\\) is denoted by \\(\\frac{\\partial}{\\partial z_i}[\\mathcal{L}(\\mathbf{z})]\\). In effect, the partial derivative tells us how \\(\\mathcal{L}\\) changes when we change \\(z_i\\), while keeping all other inputs fixed.\nThe gradient stores all the partial derivatives in a vector. The \\(i\\)th entry of this vector is given by the partial derivative of \\(\\mathcal{L}\\) with respect to \\(z_i\\). In mathematical notation, \\[\n\\nabla_\\mathbf{z} \\mathcal{L} = \\left[\\begin{smallmatrix} \\frac{\\partial}{\\partial z_1}[\\mathcal{L}(\\mathbf{z})] \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial z_d}[\\mathcal{L}(\\mathbf{z})] \\\\ \\end{smallmatrix}\\right]\n\\]\nJust like the derivative in one dimension, the gradient contains information about the slope of \\(\\mathcal{L}\\) with respect to each of the \\(d\\) dimensions in its input.\n\n\n\nVector and matrix multiplication lives at the heart of deep learning. In fact, deep learning really started to take off when researchers realized that the Graphical Processing Unit (GPU) could be used to perform gradient updates in addition to the matrix multiplication it was designed to do for gaming.\nConsider two vectors \\(\\mathbf{u} \\in \\mathbb{R}^d\\) and \\(\\mathbf{v} \\in \\mathbb{R}^d\\). We will use \\(\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^d u_i v_i\\) to denote the inner product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). The \\(\\mathcal{\\ell}_2\\)-norm of \\(v\\) is given by \\(\\|\\mathbf{v}\\|_2 = \\sqrt{\\mathbf{u} \\cdot \\mathbf{u}}\\).\nConsider two matrices: \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times m}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{m \\times n}\\) where \\(d \\neq n\\). We can only multiply two matrices when their inner dimension agrees. Because the number of columns in \\(\\mathbf{A}\\) is the same as the number of rows in \\(\\mathbf{B}\\), we can compute \\(\\mathbf{AB}\\). However, because the number of columns in \\(\\mathbf{B}\\) is not the same as the number of rows in \\(\\mathbf{A}\\), the product \\(\\mathbf{BA}\\) is not defined.\nWhen we can multiply two matrices, the \\((i,j)\\) entry in \\(\\mathbf{AB}\\) is given by the inner product between the \\(i\\)th row of \\(\\mathbf{A}\\) and the \\(j\\)th column of \\(\\mathbf{B}\\). The resulting dimensions of the matrix product will be the number of rows in \\(\\mathbf{A}\\) by the number of columns in \\(\\mathbf{B}\\).\n\n\n\nIf we have a scalar equation \\(ax = b\\), we can simply solve for \\(x\\) by dividing both sides by \\(a\\). In effect, we are applying the inverse of \\(a\\) to \\(a\\) i.e., \\(\\frac1{a} a =1\\). The same principle applies to matrices. The \\(n \\times n\\) identity matrix generalizes the scalar identity \\(1\\). This identity matrix is denoted by \\(\\mathbf{I}_{n \\times n} \\in \\mathbb{R}^{n \\times n}\\): the on-diagonal entries \\((i,i)\\) are 1 while the off-diagonal entries \\((i,j)\\) for \\(i\\neq j\\) are 0.\nConsider the matrix equation \\(\\mathbf{Ax} = \\mathbf{b}\\) where \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times d}\\), \\(\\mathbf{x} \\in \\mathbb{R}^d\\), and \\(\\mathbf{b} \\in \\mathbb{R}^d\\). (Notice that the inner dimensions of \\(\\mathbf{A}\\) and \\(\\mathbf{x}\\) agree so their multiplication is well-defined, and the resulting vector is the same dimension as \\(\\mathbf{b}\\).)\nIf we want to solve for \\(\\mathbf{x}\\), we can use the matrix inverse. For a matrix \\(\\mathbf{A}\\), we use \\(\\mathbf{A}^{-1}\\) to denote its inverse. The inverse is defined so that \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\) where \\(\\mathbf{I}_{n \\times n}\\) is the identity matrix. Then, we can solve for \\(\\mathbf{x}\\) by multiplying both sides of the equation by \\(\\mathbf{A}^{-1}\\). \\[\n\\mathbf{A}^{-1} \\mathbf{Ax} = \\mathbf{A}^{-1} \\mathbf{b}\n\\] Since \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\), we have that \\(\\mathbf{I}_{n \\times n} \\mathbf{x} = \\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}\\)."
  },
  {
    "objectID": "notes/00_PageRank.html#math-review",
    "href": "notes/00_PageRank.html#math-review",
    "title": "Review via PageRank",
    "section": "",
    "text": "When I first heard about machine learning, I imagined a computer that was rewarded every time it gave the right answer. Maybe there were electric carrots and sticks that no one had bothered to tell me about? While I now know as little as I did then about computer hardware, I have learned that machine learning is fundamentally a mathematical process.\nLuckily, we’ve been learning about the very mathematical ideas that make machine learning work for years! We’ll review the basics of these concepts and then jump in to linear regression, arguably the foundation of neural networks.\n\n\nImagine a function \\(\\mathcal{L}: \\mathbb{R} \\to \\mathbb{R}\\). (Instead of the usual \\(f\\), we’ll use \\(\\mathcal{L}\\) for reasons that will soon become clear.) The mapping notation means that \\(\\mathcal{L}\\) takes a single real number as input and outputs a single real number. In general, mathematicians tell us to be careful about whether we can differentiate a function. But, we’re computer scientists so we’ll risk it for the biscuit.\nLet \\(z \\in \\mathbb{R}\\) be the input to \\(\\mathcal{L}\\). The derivative of \\(\\mathcal{L}\\) with respect to its input \\(z\\) is mathematically denoted by \\(\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\\).\nFormally, the derivative is defined as \\[\n\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\n= \\lim_{h \\to 0} \\frac{\\mathcal{L}(z + h) - \\mathcal{L}(z)}{h}.\n\\] If we were to plot \\(\\mathcal{L}\\), the derivative at a point \\(z\\) would be the slope of the tangent line to the curve at that point.\nHere are several examples of functions and their derivatives that you might remember from calculus.\n\n\n\nFunction: \\(\\mathcal{L}(z)\\) \n\n\nDerivative: \\(\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\\)\n\n\n\n\n\\[z^2\\]\n\n\n\\[2z\\]\n\n\n\n\n\\[z^a\\]\n\n\n\\[a z^{a-1}\\]\n\n\n\n\n\\[az + b\\]\n\n\n\\[a\\]\n\n\n\n\n\\[\\ln(z)\\]\n\n\n\\[\\frac{1}{z}\\]\n\n\n\n\n\\[e^z\\]\n\n\n\\[e^z\\]\n\n\n\n\n\n\nWhile working with a simple basic function is easy, we’re not always so lucky. Modern machine learning chains many many complicated functions together. Fortunately, we will think of these operations modularly.\nLet \\(g: \\mathbb{R} \\to \\mathbb{R}\\) be another function. Consider the composite function \\(g(\\mathcal{L}(z))\\).\nBy the chain rule, the derivative of \\(g(\\mathcal{L}(z))\\) with respect to \\(z\\) is \\[\n\\frac{\\partial }{\\partial z}[g(\\mathcal{L}(z))]\n= \\frac{\\partial g}{\\partial z}(\\mathcal{L}(z))\n\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)].\n\\]\nOften, we will also multiply functions together. The product rule tells us that \\[\n\\frac{\\partial }{\\partial z}[g(z) \\mathcal{L}(z)]\n= g(z) \\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\n+ \\mathcal{L}(z) \\frac{\\partial}{\\partial z}[g(z)].\n\\]\n\n\n\nIn machine learning, we process high-dimensional data so we are interested in functions with multivariate input. Consider \\(\\mathcal{L}: \\mathbb{R}^d \\to \\mathbb{R}\\). The output of the function is still a real number but the input consists of \\(d\\) real numbers. We will use the vector \\(\\mathbf{z} \\in \\mathbb{R}^d\\) to represent all \\(d\\) inputs \\(z_1, \\ldots, z_d\\).\nInstead of the derivative, we will talk use the partial derivative. The partial derivative with respect to \\(z_i\\) is denoted by \\(\\frac{\\partial}{\\partial z_i}[\\mathcal{L}(\\mathbf{z})]\\). In effect, the partial derivative tells us how \\(\\mathcal{L}\\) changes when we change \\(z_i\\), while keeping all other inputs fixed.\nThe gradient stores all the partial derivatives in a vector. The \\(i\\)th entry of this vector is given by the partial derivative of \\(\\mathcal{L}\\) with respect to \\(z_i\\). In mathematical notation, \\[\n\\nabla_\\mathbf{z} \\mathcal{L} = \\left[\\begin{smallmatrix} \\frac{\\partial}{\\partial z_1}[\\mathcal{L}(\\mathbf{z})] \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial z_d}[\\mathcal{L}(\\mathbf{z})] \\\\ \\end{smallmatrix}\\right]\n\\]\nJust like the derivative in one dimension, the gradient contains information about the slope of \\(\\mathcal{L}\\) with respect to each of the \\(d\\) dimensions in its input.\n\n\n\nVector and matrix multiplication lives at the heart of deep learning. In fact, deep learning really started to take off when researchers realized that the Graphical Processing Unit (GPU) could be used to perform gradient updates in addition to the matrix multiplication it was designed to do for gaming.\nConsider two vectors \\(\\mathbf{u} \\in \\mathbb{R}^d\\) and \\(\\mathbf{v} \\in \\mathbb{R}^d\\). We will use \\(\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^d u_i v_i\\) to denote the inner product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). The \\(\\mathcal{\\ell}_2\\)-norm of \\(v\\) is given by \\(\\|\\mathbf{v}\\|_2 = \\sqrt{\\mathbf{u} \\cdot \\mathbf{u}}\\).\nConsider two matrices: \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times m}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{m \\times n}\\) where \\(d \\neq n\\). We can only multiply two matrices when their inner dimension agrees. Because the number of columns in \\(\\mathbf{A}\\) is the same as the number of rows in \\(\\mathbf{B}\\), we can compute \\(\\mathbf{AB}\\). However, because the number of columns in \\(\\mathbf{B}\\) is not the same as the number of rows in \\(\\mathbf{A}\\), the product \\(\\mathbf{BA}\\) is not defined.\nWhen we can multiply two matrices, the \\((i,j)\\) entry in \\(\\mathbf{AB}\\) is given by the inner product between the \\(i\\)th row of \\(\\mathbf{A}\\) and the \\(j\\)th column of \\(\\mathbf{B}\\). The resulting dimensions of the matrix product will be the number of rows in \\(\\mathbf{A}\\) by the number of columns in \\(\\mathbf{B}\\).\n\n\n\nIf we have a scalar equation \\(ax = b\\), we can simply solve for \\(x\\) by dividing both sides by \\(a\\). In effect, we are applying the inverse of \\(a\\) to \\(a\\) i.e., \\(\\frac1{a} a =1\\). The same principle applies to matrices. The \\(n \\times n\\) identity matrix generalizes the scalar identity \\(1\\). This identity matrix is denoted by \\(\\mathbf{I}_{n \\times n} \\in \\mathbb{R}^{n \\times n}\\): the on-diagonal entries \\((i,i)\\) are 1 while the off-diagonal entries \\((i,j)\\) for \\(i\\neq j\\) are 0.\nConsider the matrix equation \\(\\mathbf{Ax} = \\mathbf{b}\\) where \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times d}\\), \\(\\mathbf{x} \\in \\mathbb{R}^d\\), and \\(\\mathbf{b} \\in \\mathbb{R}^d\\). (Notice that the inner dimensions of \\(\\mathbf{A}\\) and \\(\\mathbf{x}\\) agree so their multiplication is well-defined, and the resulting vector is the same dimension as \\(\\mathbf{b}\\).)\nIf we want to solve for \\(\\mathbf{x}\\), we can use the matrix inverse. For a matrix \\(\\mathbf{A}\\), we use \\(\\mathbf{A}^{-1}\\) to denote its inverse. The inverse is defined so that \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\) where \\(\\mathbf{I}_{n \\times n}\\) is the identity matrix. Then, we can solve for \\(\\mathbf{x}\\) by multiplying both sides of the equation by \\(\\mathbf{A}^{-1}\\). \\[\n\\mathbf{A}^{-1} \\mathbf{Ax} = \\mathbf{A}^{-1} \\mathbf{b}\n\\] Since \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\), we have that \\(\\mathbf{I}_{n \\times n} \\mathbf{x} = \\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}\\)."
  },
  {
    "objectID": "notes/01_LinearRegression.html",
    "href": "notes/01_LinearRegression.html",
    "title": "Linear Regression and Optimization",
    "section": "",
    "text": "Supervised learning is perhaps the most natural setting for machine learning. In this setting, we are given labeled data, and our goal is to train a model to approximately match the labels. The supervised learning problem is versatile and powerful, some examples include:\n\nPredicting temperature based on present weather conditions,\nIdentifying the type of animal in an image, and\nGenerating the next word in a sentence.\n\nWe can encompass all of these settings and more with the following mathematical formulation: Concretely, we are given \\(n\\) data points \\(\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(n)} \\in \\mathbb{R}^d\\), each with \\(d\\) dimensions, and associated labels \\(y^{(1)}, y^{(2)}, \\ldots, y^{(n)} \\in \\mathbb{R}\\). Our goal is to learn a model \\(f: \\mathbb{R}^d \\to \\mathbb{R}\\) so that \\(f(\\mathbf{x}^{(i)}) \\approx y^{(i)}\\) for all data points \\(i \\in \\{1,2,\\ldots,n\\}\\).\nOur general approach to solving supervised learning problems will be to use empirical risk minimization, which gives a flexible scaffolding that encompasses many of the topics we’ll discuss in this course. Given a model class (e.g., linear models or neural networks), the idea is to select the model that most closely explains the data. In particular, there are three components to empirical risk minimization:\n\nModel Class: The model class \\(\\mathcal{F}\\) from which we will select the model \\(f\\) that most closely fits the observed data.\nLoss: The loss function that measures how well a function \\(f\\) fits the observed data. (Without loss of generality, we will assume that lower is better.)\nOptimizer: The method of selecting the model from the model class.\n\nEmpirical risk minimization is an abstract idea. Luckily, we will revisit it again and again. Our first example will be linear regression, where the model class is the set of linear functions and the loss is the squared difference between the true label and our prediction. Let’s dive in!"
  },
  {
    "objectID": "notes/01_LinearRegression.html#the-supervised-learning-problem",
    "href": "notes/01_LinearRegression.html#the-supervised-learning-problem",
    "title": "Linear Regression and Optimization",
    "section": "",
    "text": "Supervised learning is perhaps the most natural setting for machine learning. In this setting, we are given labeled data, and our goal is to train a model to approximately match the labels. The supervised learning problem is versatile and powerful, some examples include:\n\nPredicting temperature based on present weather conditions,\nIdentifying the type of animal in an image, and\nGenerating the next word in a sentence.\n\nWe can encompass all of these settings and more with the following mathematical formulation: Concretely, we are given \\(n\\) data points \\(\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(n)} \\in \\mathbb{R}^d\\), each with \\(d\\) dimensions, and associated labels \\(y^{(1)}, y^{(2)}, \\ldots, y^{(n)} \\in \\mathbb{R}\\). Our goal is to learn a model \\(f: \\mathbb{R}^d \\to \\mathbb{R}\\) so that \\(f(\\mathbf{x}^{(i)}) \\approx y^{(i)}\\) for all data points \\(i \\in \\{1,2,\\ldots,n\\}\\).\nOur general approach to solving supervised learning problems will be to use empirical risk minimization, which gives a flexible scaffolding that encompasses many of the topics we’ll discuss in this course. Given a model class (e.g., linear models or neural networks), the idea is to select the model that most closely explains the data. In particular, there are three components to empirical risk minimization:\n\nModel Class: The model class \\(\\mathcal{F}\\) from which we will select the model \\(f\\) that most closely fits the observed data.\nLoss: The loss function that measures how well a function \\(f\\) fits the observed data. (Without loss of generality, we will assume that lower is better.)\nOptimizer: The method of selecting the model from the model class.\n\nEmpirical risk minimization is an abstract idea. Luckily, we will revisit it again and again. Our first example will be linear regression, where the model class is the set of linear functions and the loss is the squared difference between the true label and our prediction. Let’s dive in!"
  },
  {
    "objectID": "notes/01_LinearRegression.html#univariate-linear-regression",
    "href": "notes/01_LinearRegression.html#univariate-linear-regression",
    "title": "Linear Regression and Optimization",
    "section": "Univariate Linear Regression",
    "text": "Univariate Linear Regression\nLinear regression is a simple but powerful tool that we will use to understand the basics of machine learning. For simplicity, we will first consider the univariate case where the inputs are all one-dimensional i.e., \\(x^{(1)}, \\ldots, x^{(n)} \\in \\mathbb{R}\\).\n\nLinear Models\nAs its name suggests, linear regression uses a linear model to process the input into an approximation of the output. Let \\(w \\in \\mathbb{R}\\) be a weight parameter. The linear model (for one-dimensional inputs) is given by \\(f(x) = wx\\).\nUnlike many machine learning models, we can visualize the linear model since it is given by a line. In the plot, we have the \\(n=10\\) data points plotted in two dimensions. There is one linear model \\(f(x) = 2x\\) that closely approximates the data and another linear model \\(f(x)=\\frac12 x\\) that poorly approximates the data.\n\n\n\nOur goal is to learn how to find a linear model that fits the data well. Before we can do this, though, we will need to define what it means for a model to “fit the data well”.\n\n\nMean Squared Error Loss\nOur goal for the loss function is to measure how closely the data fits the prediction made by our model. Intuitively, we should take the difference between the prediction and the true outcome \\(f(x^{(i)})-y^{(i)}\\).\nThe issue with this approach is that \\(f(x^{(i)})-y^{(i)}\\) can be small (negative) even when \\(f(x^{(i)}) \\neq y^{(i)}\\). A natural fix is to take the absolute value \\(|f(x^{(i)}) - y^{(i)}|\\). The benefit of the absolute value is that the loss is \\(0\\) if and only if \\(f(x^{(i)}) = y^{(i)}\\). However, the absolute value function is not differentiable, which is a property we’ll need for optimization. Instead, we use the squared loss:\n\\(\\mathcal{L}(w) = \\frac1{n} \\sum_{i=1}^n (f(x^{(i)}) - y^{(i)})^2.\\)\nHere, we use the mean squared error loss, which is the average squared difference between the prediction and the true output over the dataset. Unlike the absolute value function, the squared function is differentiable everywhere. In addition, the squared error disproportionately penalizes predictions that are far from the true labels, a property that may be desirable when we want all of our predictions to be reasonably accurate.\n\n\n\nThe plot above compares the squared function to the absolute value function. While both are \\(0\\) if and only if their input is \\(0\\), the squared function is differentiable everywhere and penalizes large errors more.\n\n\nExact Optimization\nWe now have our model class and loss function: linear models and mean squared error loss. The question becomes how to update the weights of the model to minimize the loss. In particular, we want to find \\(w\\) that minimizes \\(\\mathcal{L}(w)\\). While the language we’re using is new, the problem is not. We’ve actually been studying how to do this since pre-calculus!\nThe squared loss is convex (a bowl facing up versus the downward facing cave of concave); see the plot above for a ‘proof’ by example. In this case, we know there is only one minimum. Not only that but we can find the minimum by setting the derivative to \\(0\\).\nAs such, our game plan is to set \\(\\frac{\\partial \\mathcal{L}}{\\partial w}\\) to \\(0\\) and solve for \\(w\\). Recall that \\(f(x) = wx\\). We will use the linearity of the derivative, the chain rule, and the power rule to compute the derivative of \\(\\mathcal{L}\\) with respect to \\(w\\):\n\\[\n\\begin{align}\n\\frac{\\partial}{\\partial w}[\\mathcal{L}(w)]\n&= \\frac1{n} \\sum_{i=1}^n \\frac{\\partial}{\\partial w} [(f(x^{(i)}) - y^{(i)})^2]\n\\notag \\\\&= \\frac1{n} \\sum_{i=1}^n 2(f(x^{(i)}) - y^{(i)}) \\frac{\\partial}{\\partial w} [(f(x^{(i)}) - y^{(i)})]\n\\notag \\\\&= \\frac1{n} \\sum_{i=1}^n 2(w x^{(i)} - y^{(i)}) x^{(i)}.\n\\end{align}\n\\]\nSetting the derivative to \\(0\\) and solving for \\(w\\), we get \\(\\frac2{n} \\sum_{i=1}^n w \\cdot (x^{(i)})^2 = \\frac2{n} \\sum_{i=1}^n y^{(i)} x^{(i)}\\) and so \\[\nw = \\frac{\\sum_{i=1}^n y^{(i)} \\cdot x^{(i)}}{\\sum_{i=1}^n (x^{(i)})^2}.\n\\]\nThis is the exact solution to the univariate linear regression problem! We can now use this formula to find the best linear model for our univariate data. However, we’ll have to work slightly harder for the general case with multidimensional data."
  },
  {
    "objectID": "notes/01_LinearRegression.html#multivariate-linear-regression",
    "href": "notes/01_LinearRegression.html#multivariate-linear-regression",
    "title": "Linear Regression and Optimization",
    "section": "Multivariate Linear Regression",
    "text": "Multivariate Linear Regression\nConsider the more general setting where the input is \\(d\\)-dimensional. As before, we observe \\(n\\) training observations \\((\\mathbf{x}^{(1)}, y^{(1)}), \\ldots, (\\mathbf{x}^{(n)}, y^{(n)})\\) but now \\(\\mathbf{x}^{(i)} \\in \\mathbb{R}^d\\). We will generalize the ideas from univariate linear regression to the multivariate setting.\n\nLinear Model\nInstead of using a single weight \\(w \\in \\mathbb{R}\\), we will use \\(d\\) weights \\(\\mathbf{w} \\in \\mathbb{R}^d\\). Then the model is given by \\(f(x) = \\langle \\mathbf{w}, \\mathbf{x} \\rangle\\).\nInstead of using a line to fit the data, we use a hyperplane. While visualizing the model is difficult in high dimensions, we can still see the model when \\(d=2\\).\n\n\n\nIn the plot above, we have \\(n=10\\) data points in 3 dimensions. There is one linear model \\(\\mathbf{w} = \\begin{bmatrix} 2 \\\\ \\frac12 \\end{bmatrix}\\) that closely approximates the data and another linear model \\(\\mathbf{w} = \\begin{bmatrix} \\frac12 \\\\ 0 \\end{bmatrix}\\) that poorly approximates the data.\n\n\nMean Squared Error\nSince the output of \\(f\\) is still a single real number, we do not have to change the loss function. However, we can use our linear algebra notation to write the mean squared error in an elegant way.\nLet \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) be the data matrix where the \\(i\\)th row is \\((\\mathbf{x}^{(i)})^\\top\\). Similarly, let \\(\\mathbf{y} \\in \\mathbb{R}^n\\) be the target vector where the \\(i\\)th entry is \\(y^{(i)}\\). We can then write the mean squared error loss as \\[\n\\mathcal{L}(\\mathbf{w}) = \\frac1{n} \\| \\mathbf{X w - y} \\|_2^2.\n\\]\n\n\nExact Optimization\nJust like computing the derivative and setting it to \\(0\\), we can compute the gradient and set it to the zero vector \\(\\mathbf{0} \\in \\mathbb{R}^d\\). In mathematical notation, we will set \\(\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}^*) = \\mathbf{0}\\) and solve for \\(\\mathbf{w}^*\\). The intuition is that such a point is a local minimum in every direction; that is, we cannot improve the loss by moving in any of the dimensions. Since the loss is convex (i.e., there can be only one minima), such a point is the unique global minimum and achieves the optimal loss.\nWe will leave computing \\(\\mathbf{w}^*\\) as an exercise on the homework. As you will show, \\(\\mathbf{w}^* = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\\)."
  },
  {
    "objectID": "notes/01_LinearRegression.html#empirical-risk-minimization",
    "href": "notes/01_LinearRegression.html#empirical-risk-minimization",
    "title": "Linear Regression and Optimization",
    "section": "Empirical Risk Minimization",
    "text": "Empirical Risk Minimization\nWe have now seen how to fit a linear model to data using the mean squared error loss. However, we have not given a satisfying answer to the question:\n\nWhy use mean squared error as our loss function?\n\n\nSo far, our answer has been that the quadratic function is differentiable (which we use to find the optimal solution), and that it naturally penalizes predictions which are farther away more. The first point is one of convenience and, a priori, should not be particularly persuasive. The second seems somewhat arbitrary, why penalize at a quadratic rate rather than an e.g., quartic rate? We’ll now consider a more compelling answer.\nOn our way to the answer, let’s take a step back and consider another question:\n\nWhy fit the data with a linear model?\n\n\nWell, we may do so when we expect the data truly has a linear relationship with the labels. To make things interesting, we will assume that there is random noise added to the labels, but that this noise is mean-centered so that, on average, the labels come from the linear model. Concretely, we observe some point \\(\\mathbf{x}\\) with a label that comes from a linear model \\(\\mathbf{w}^*\\) but with added noise, i.e., \\[\ny= \\langle \\mathbf{w}^*, \\mathbf{x} \\rangle + \\eta.\n\\] We will model this noise as distributed from a normal distribution, i.e., \\(\\eta \\sim \\mathcal{N}(0, \\sigma^2)\\) for some unknown standard deviation \\(\\sigma\\). (To justify this choice, we imagine the noise as a sum of random variables from some other distribution(s) which, by the law of large numbers, will follow the normal distribution when the sum contains sufficiently many terms.)\nRecall that the goal of our empirical risk minimization strategy is to find the model which most closely aligns with the data, or, put differently, we want the model that most likely generated the data we observed. In order to compute this likelihood, we will use the probability density function of the normal distribution: The probability we observe a random variable \\(y\\) drawn from a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) is given by \\[\n\\frac1{\\sqrt{2\\pi \\sigma}} \\exp\\left( - \\frac{(y- \\mu)^2}{2\\sigma^2} \\right).\n\\] If the noisy linear model \\(\\mathbf{w}\\) did generate the training data \\((\\mathbf{x}^{(i)}, y^{(i)})\\), then the expectation of the generation would be \\(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle\\). Then, combined with the assumption that the training data was drawn independently, the probability of observing the training data is given by the product of the probabilities of each individual observation: \\[\n\\prod_{i=1}^n\n\\frac1{\\sqrt{2\\pi \\sigma}} \\exp\\left( - \\frac{(y^{(i)}- \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)^2}{2\\sigma^2} \\right).\n\\] Our goal is to find the model \\(\\mathbf{w}\\) that maximizes this likelihood i.e., \\[\n\\begin{align}\n&{\\arg\\max}_{\\mathbf{w} \\in \\mathbb{R}^d}\n\\prod_{i=1}^n\n\\frac{1}{\\sqrt{2\\pi \\sigma}} \\exp\\left( - \\frac{(y^{(i)}- \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)^2}{2\\sigma^2} \\right)\n\\notag \\\\\n&= {\\arg\\min}_{\\mathbf{w} \\in \\mathbb{R}^d}\n- \\log \\left(\n\\frac{1}{\\sqrt{2\\pi \\sigma}} \\exp\\left(\\sum_{i=1}^n - \\frac{(y^{(i)}- \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)^2}{2\\sigma^2} \\right)\\right)\n\\notag \\\\\n&= {\\arg\\min}_{\\mathbf{w} \\in \\mathbb{R}^d}\n- \\sum_{i=1}^n - (y^{(i)}- \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)^2.\n\\end{align}\n\\] Here, we used the following facts: maximizing an objective is equivalent to minimizing the negative of that objective, the logarithmic function is monotonically increasing so minimizing the likelihood is equivalent to minimizing the log-likelihood, the product of exponentials is the exponential of the sum, and removing a constant scalar factor or additive constant does not change the minimum.\nThe punchline is that the model \\(\\mathbf{w}\\) that maximizes the likelihood of observing the training data is the same model that minimizes the mean squared error loss. This is a powerful result that justifies our use of the mean squared error loss."
  },
  {
    "objectID": "notes/01_LinearRegression.html#looking-forward",
    "href": "notes/01_LinearRegression.html#looking-forward",
    "title": "Linear Regression and Optimization",
    "section": "Looking Forward",
    "text": "Looking Forward\nWhile we have seen the benefits of exactly optimizing linear regression models, there are several limitations that we will address.\nComputational Complexity We saw (and you will prove) that the exact solution to linear regression is given by \\(\\mathbf{w}^* = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\\). This requires building the matrix \\(\\mathbf{X}^\\top \\mathbf{X}\\), which takes \\(O(nd^2)\\) time, and then inverting it, which takes \\(O(d^3)\\). When we have a large number of data points \\(n\\) and/or a large number of features \\(d\\), this can be prohibitively expensive.\nModel Class Misspecification We have assumed that the data has a linear relationship (or close to linear relationship) with the labels. What happens when this is not true? That is, even the best linear model gives a poor approximation?"
  },
  {
    "objectID": "notes/code.html#linear-regression-figures",
    "href": "notes/code.html#linear-regression-figures",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "Linear Regression Figures",
    "text": "Linear Regression Figures\n\nnp.random.seed(1234) # Seed randomness\n\nn = 10 # Number of observations\nw = 2 # True parameter\nX = np.random.rand(n) # x-values\ny = X.dot(w).T + np.random.normal(size=n) * .2 #y-values\n\nplt.scatter(X,y, color='black', label=r'Data: $(x^{(i)}, y^{(i)})$')\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\nxaxis = np.arange(0,1,.01)\nplt.plot(xaxis, xaxis*.5, label=r'Line: $f(x) = .5x$', color='red')\nplt.plot(xaxis, xaxis*w, label=r'Line: $f(x) = 2x$', color='green')\nplt.legend()\nplt.title(r'Linear Regression in $\\mathbb{R}^1$')\nplt.savefig('images/regression_1d.pdf')\n\n\n\n\n\n\n\n\n\nplt.xlabel(r'$z$')\nplt.ylabel(r'$\\mathcal{L}(z)$')\nxaxis = np.arange(-1.5,1.5,.001)\nplt.plot(xaxis, xaxis**2, label=r'Squared Loss: $\\mathcal{L}(z)=z^2$', color='blue')\nplt.plot(xaxis, np.abs(xaxis), label=r'Absolute Loss: $\\mathcal{L}(z)=|z|$', color='purple', linestyle='dotted')\nplt.legend()\nplt.title(r'Squared and Absolute Losses')\nplt.savefig('images/regression_losses.pdf')\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\n\n# Seed randomness\nnp.random.seed(1234)\nn = 10  # Number of observations\nw = np.array([2, .5])  # True parameter\nX = np.random.rand(n, 2)  # x-values\ny = X.dot(w).T + np.random.normal(size=n) * .1  # y-values\n\n# Create figure and 3D axis\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Scatter plot for data points\nax.scatter(X[:, 0], X[:, 1], y, color='black', label=r'Data: $(x_1^{(i)}, x_2^{(i)}, y^{(i)})$')\n\n# Hyperplane 1: Green\nx1 = np.arange(0, 1, .01)\nx2 = np.arange(0, 1, .01)\nX1, X2 = np.meshgrid(x1, x2)\nZ = w[0] * X1 + w[1] * X2\nax.plot_surface(X1, X2, Z, alpha=.5, color='green')\n\n# Hyperplane 2: Red\nax.plot_surface(X1, X2, .5 * X1 + 0 * X2, alpha=.5, color='red')\n\n# Labels and title\nax.set_xlabel(r'$x_1$')\nax.set_ylabel(r'$x_2$')\nax.set_zlabel(r'$y$')\nax.set_title(r'Linear Regression in $\\mathbb{R}^2$')\n\n# Manually create custom legend handles for the surfaces\nhandles = [\n    Line2D([0], [0], marker='o', color='black', markerfacecolor='black', markersize=6, label=r'Data: $(x_1^{(i)}, x_2^{(i)}, y^{(i)})$'),\n    Line2D([0], [0], color='green', lw=4, label=r'Hyperplane: $f(x) = 2x_1 + .5x_2$'),\n    Line2D([0], [0], color='red', lw=4, label=r'Hyperplane: $f(x) = .5x_1 + 0x_2$')\n]\n\n# Add legend\nplt.legend(handles=handles, loc='upper left', framealpha=1)\n\n# Save the figure\nplt.savefig('images/regression_2d.pdf', bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "notes/code.html#non-linear-regression-figures",
    "href": "notes/code.html#non-linear-regression-figures",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "Non-linear Regression Figures",
    "text": "Non-linear Regression Figures\n\n## Gradient descent\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom matplotlib import cm\nimport torch\n\n%matplotlib inline\n\nclass QuadFunc:\n  def __init__(self, a, b, c, d, e):\n    self.a = a\n    self.b = b\n    self.c = c\n    self.d = d\n    self.e = e\n\n  def getParams(self, x, y):\n    if y is None:\n      y = x[1]\n      x = x[0]\n    return x,y\n\n  def __call__(self, x, y=None):\n    x,y = self.getParams(x,y)\n    return 0.5 * (self.a*x**2 + self.b*y**2) + self.c * x * y + self.d * x + self.e * y\n\n  def grad(self, x, y=None):\n    #df/dx = ax + cy + d\n    #df/dy = by + cx + e\n    x,y = self.getParams(x,y)\n    return torch.tensor([self.a * x + self.c * y + self.d, self.b * y + self.c * x + self.e])\n\n  def hess(self, x, y=None):\n    #d2f/dx2 = a\n    #d2f/dy2 = b\n    #d2f/dxdy = c\n    #d2f/dydx = c\n    x, y = self.getParams(x,y)\n    return torch.tensor([[self.a, self.c], [self.c, self.b]])\n\nclass GradientDescent:\n    def __init__(self, lr=1, b1=0.9, b2=0.999):\n        # b1 -&gt; Momentum\n        # b2 -&gt; ADAM\n        # ADAM Paper -&gt; https://arxiv.org/abs/1412.6980\n        self.lr = lr # learning rate\n        self.b1 = b1 # grad aggregation param (for Momentum)\n        self.b2 = b2 # grad^2 aggregation param (for ADAM)\n\n        self.v = 0 # grad aggregation param\n        self.w = 0 # grad^2 aggregation param\n        self.t = 0\n\n        self.eps = 1e-9\n\n    def __call__(self, grad,hess):\n\n        self.t += 1\n\n\n        # aggregation\n        self.v = self.b1*self.v + (1-self.b1)*grad\n        self.w = self.b2*self.w + (1-self.b2)*grad**2\n\n        # bias correction\n        vcorr = self.v/(1-self.b1**self.t)\n        wcorr = self.w/(1-self.b2**self.t) if self.b2 != 0 else 1\n\n        return -1*self.lr*vcorr/(wcorr**0.5 + self.eps)\n\nclass Newtons:\n    # https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization\n    def __init__(self, lr=1):\n        self.lr = lr\n\n    def __call__(self,grad,hess):\n        return -1*self.lr*torch.matmul(torch.inverse(hess), grad)\n\ndef runOptim(init,optim,func,steps):\n\n    curpos = init # current position\n    path = [curpos]\n\n\n    for _ in range(steps):\n\n        grad = func.grad(curpos)\n        hess = func.hess(curpos)\n\n        dx = optim(grad,hess)\n        curpos = curpos + dx\n        path.append(curpos)\n\n    return path\n\n\ndef showPath(func,init,paths,labels,colors,levels):\n\n    x = torch.arange(-10,10,0.05)\n    y = torch.arange(-10,10,0.05)\n\n    # create meshgrid\n    xx, yy = torch.meshgrid(x,y)\n    zz = func(xx,yy)\n\n    # create contour\n    fig, ax = plt.subplots(1,1,figsize=(10,4))\n    cp = ax.contourf(xx,yy,zz,levels)\n    fig.colorbar(cp)\n\n    # mark initial point\n    ax.plot(init[0],init[1],'ro')\n    ax.text(init[0]+0.5,init[1],'Intial Point',color='white')\n\n    # Plot paths\n    for pnum in range(len(paths)):\n        for i in range(len(paths[pnum])-1):\n            curpos = paths[pnum][i]\n            d = paths[pnum][i+1] - curpos\n            ax.arrow(curpos[0],curpos[1],d[0],d[1],color=colors[pnum],head_width=0.2)\n            ax.text(curpos[0]+d[0],curpos[1]+d[1],str(i),color='white')\n\n    # Add legend\n    legends = []\n    for col in colors:\n        legends.append(mpatches.Patch(color=col))\n    # Put legend in top left corner\n    ax.legend(legends,labels, loc='upper left')\n\n\na = 1/torch.sqrt(torch.tensor(2.0))\ninit = torch.matmul(torch.tensor([[a,a],[-a,a]]),torch.tensor([-5.0,7.5]))\nell = QuadFunc(a,a,-0.8*a,a,a)\nsteps = 7\nlr = 1.5\nregGD = GradientDescent(lr,0,0) # Without Momentum\nmomGD = GradientDescent(lr,0.9,0) # Momentum\npath1 = runOptim(init,regGD,ell,steps)\npath2 = runOptim(init,momGD,ell,steps)\n# Set figure size\nshowPath(ell,init,[path1,path2],['Gradient Descent','Momentum'],['r','y'], 15)\n# Turn off axis ticks\nplt.xticks([])\nplt.yticks([])\nplt.xlabel(r'$w_1$', fontsize=14)\nplt.ylabel(r'$w_2$', fontsize=14)\nplt.savefig('images/regression_momentum.pdf', bbox_inches='tight', dpi=300)\n\n/opt/homebrew/Caskroom/miniconda/base/envs/rads/lib/python3.10/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:4316.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import StandardScaler\n\n# Step 1: Generate noisy linear data\nn = 10 # Number of observations\nnp.random.seed(0)\nX = np.linspace(0, 10, n).reshape(-1, 1)\ntrue_slope = 2.5\ntrue_intercept = 5\nnoise = np.random.normal(0, 2, n).reshape(-1, 1)  # Gaussian noise\ny = true_slope * X + true_intercept + noise\ny = y.ravel()\n\n# Step 2: Fit linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\ny_lin_pred = lin_reg.predict(X)\n\n# Step 3: Train neural network using scikit-learn\n# It's often helpful to scale data for MLPs\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nmlp = MLPRegressor(hidden_layer_sizes=(16, 32, 64, 32, 16), activation='relu', max_iter=10000)\nmlp.fit(X_scaled, y)\ny_nn_pred = mlp.predict(X_scaled)\n\n# Step 4: Plotting\nplt.figure(figsize=(8, 4))\n\nplt.plot(X, y_nn_pred, color='red', label='Neural Network', linewidth=3, alpha=0.7, zorder=1, linestyle='--')   # Muted Maroon\nplt.plot(X, y_lin_pred, color='green', label='Linear Regression', linewidth=3, alpha=0.7, zorder=1)  # Medium Olive\nplt.scatter(X, y, label='Training Data', marker='o', color='#3B5998', s=60, zorder=2)  # Deep Cornflower Blue\n\nplt.legend()\nplt.title('Linear Regression vs Neural Network (Linear Data)')\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\nplt.tight_layout()\nplt.savefig('images/regression_overfitting.pdf', bbox_inches='tight', dpi=300)\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import StandardScaler\n\n# Step 1: Generate noisy quadratic data\n# seed\nnp.random.seed(0)\nn = 10  # More points for smoother curve\nX = np.linspace(-10, 10, n).reshape(-1, 1)\ntrue_a = 1.2\ntrue_b = -3.4\ntrue_c = 2.0\nnoise = np.random.normal(0, 20, n).reshape(-1, 1)  # Gaussian noise\ny = true_a * X**2 + true_c + noise\ny = y.ravel()\n\n# Step 2: Fit linear regression model (will underfit)\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\ny_lin_pred = lin_reg.predict(X)\n\n# Step 3: Scale data for MLPs\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Step 4: Train standard MLPRegressor\nmlp = MLPRegressor(hidden_layer_sizes=(16, 32, 64, 32, 16), activation='relu',\n                   max_iter=10000, random_state=0)\nmlp.fit(X_scaled, y)\ny_nn_pred = mlp.predict(X_scaled)\n\n# Step 5: Train regularized MLPRegressor (L2 regularization via alpha)\nmlp_reg = MLPRegressor(hidden_layer_sizes=(16, 32), activation='relu',\n                       alpha=20, max_iter=10000, random_state=0)\nmlp_reg.fit(X_scaled, y)\ny_nn_reg_pred = mlp_reg.predict(X_scaled)\n\n# Step 6: Plot results\nplt.figure(figsize=(8, 5))\n\nplt.plot(X, y_nn_pred, color='red', label='Neural Network', linewidth=3, alpha=0.7, zorder=1, linestyle='--')\nplt.plot(X, y_nn_reg_pred, color='green', label='Regularized Neural Network', linewidth=3, zorder=1)\nplt.plot(X, y_lin_pred, color='red', label='Linear Regression', linewidth=3, alpha=0.7, zorder=1, linestyle=':')\nplt.scatter(X, y, label='Training Data', marker='o', color='#3B5998', s=60, zorder=2)\n\nplt.legend()\nplt.title('Linear Regression vs Neural Network (Quadratic Data)')\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\nplt.tight_layout()\nplt.savefig('images/regression_regularization.pdf', bbox_inches='tight', dpi=300)\nplt.show()"
  }
]