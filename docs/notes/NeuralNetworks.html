<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Neural Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../eve.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-c5a5d5e27fcc88644031c24cff017230.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="shortcut icon" href="../eve.ico">
<script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-GZHXTPTRRE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-GZHXTPTRRE');
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Fall 2025</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <span class="nav-link">
<span class="menu-text">Canvas</span>
    </span>
  </li>  
  <li class="nav-item">
    <span class="nav-link">
<span class="menu-text">Gradescope</span>
    </span>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../syllabus.html"> 
<span class="menu-text">Syllabus</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://discord.gg/dES3fSPEeC"> 
<span class="menu-text">Discord</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#neural-network-architectures" id="toc-neural-network-architectures" class="nav-link active" data-scroll-target="#neural-network-architectures">Neural Network Architectures</a></li>
  <li><a href="#backpropagation" id="toc-backpropagation" class="nav-link" data-scroll-target="#backpropagation">Backpropagation</a></li>
  <li><a href="#linear-algebraic-view-of-backpropagation" id="toc-linear-algebraic-view-of-backpropagation" class="nav-link" data-scroll-target="#linear-algebraic-view-of-backpropagation">Linear Algebraic View of Backpropagation</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><strong>Neural Networks</strong></h1>
</div>



<div class="quarto-title-meta column-page-left">

    
  
    
  </div>
  


</header>


<p>Previously, we have built machine learned models in two steps: First, we chose a set of features to represent the data, possibly transforming the data in the process. Second, we optimized a model on the (transformed) features. Neural networks allow us to learn both the feature transformations and the model <em>simultaneously</em>.</p>
<section id="neural-network-architectures" class="level3">
<h3 class="anchored" data-anchor-id="neural-network-architectures">Neural Network Architectures</h3>
<p>Consider training data that is not linearly separable, i.e., we cannot draw a straight line to separate the classes. We saw before how we could represent the data in a higher-dimensional space, where it is linearly separable. The alternative we’ll explore today is to divide the region with multiple linear boundaries.</p>
<center>
<img src="images/neural_data.svg" class="responsive-img">
</center>
<p>We can learn four linear boundaries, each of bounds the region in a different direction.</p>
<center>
<img src="images/neural_regions.svg" class="responsive-img">
</center>
<p>When we pass the input through the learned lines, we will get real numbers representing the distance from the line e.g., <span class="math inline">\(\langle \mathbf{x}, \mathbf{w}^{(i)} \rangle\)</span>. Let’s apply a non-linear activation function to these distances, which will distinguish whether the distance is positive or negative e.g., <span class="math inline">\(\sigma(\langle \mathbf{x}, \mathbf{w}^{(i)} \rangle)\)</span>, where <span class="math inline">\(\sigma(z) = \mathbf{1}[z \geq 0]\)</span>. For example, the point marked by the star would represented as <span class="math inline">\([0, 1, 1, 0]\)</span>, because it is above the first line, below the second line, below the third line, and above the fourth line. Finally, we can combine these values to make a prediction.</p>
<center>
<img src="images/neural_example.svg" class="responsive-img">
</center>
<p>In this way, we can learn a non-linear functions by combining multiple linear models with non-linear activations. By updating the weights of the linear models and the weights of the final combination, we can learn a complex function that fits the data well.</p>
<p>Neural networks are remarkably flexible, with many different architectural options:</p>
<ul>
<li><p>The <strong>activation functions</strong> which process the outputs of the linear models. Options include the ReLU function <span class="math inline">\(\sigma(z) = \max(0, z)\)</span>, the sigmoid function <span class="math inline">\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span>, the hyperbolic tangent function <span class="math inline">\(\sigma(z) = \tanh(z)\)</span>, and more.</p></li>
<li><p>The <strong>number of layers</strong> in the network, i.e., how many times we apply the linear models and activation functions.</p></li>
<li><p>The <strong>number of neurons</strong> in each layer, i.e., how many linear models we apply in each layer.</p></li>
<li><p>The <strong>loss function</strong> to measure the error between the predicted and true labels. We can use the same loss functions we have seen before: cross-entropy for classification and squared error for regression.</p></li>
</ul>
<p>Tensorflow offers an excellent visualization of common neural network architectures, which you can play around with <a href="https://playground.tensorflow.org/">here</a>.</p>
<p>Beyond these options, there are several types of neural network architectures that connect neurons in different ways: Convolutional layers are particularly useful for locally correlated data, such as images, where we can learn filters that capture local patterns. Transformers are particularly useful for sequential data, such as text, and form the backbone of modern foundation models. We will not cover these architectures in our course, but I highly recommend Chinmay Hegde’s <a href="https://chinmayhegde.github.io/dl-notes/notes/lecture04/">convolutional network notes</a> and <a href="https://chinmayhegde.github.io/dl-notes/notes/lecture07/">transformer notes</a> if you would like to learn more.</p>
<p>When we train a neural network, we apply gradient descent to minimize the loss function, just like we have done for linear regression and logistic regression. But we need to be careful about how we compute the gradients, since the neural network is a composition of multiple functions, and there can be <em>many</em> parameters.</p>
</section>
<section id="backpropagation" class="level3">
<h3 class="anchored" data-anchor-id="backpropagation">Backpropagation</h3>
<p>The key step in training a neural network is to compute the gradients of the loss function with respect each of the parameters in the network. <em>Backpropagation</em> allows us to compute these gradients efficiently. The basic idea is to modularly apply the chain rule from calculus to compute the gradients of each layer, starting from the output layer and working backwards to the input layer.</p>
<p>Before we dive into the details, let’s review the chain rule. For a scalar function <span class="math inline">\(f: \mathbb{R} \to \mathbb{R}\)</span>, we write the derivative as <span class="math display">\[
\frac{df}{dx} = \lim_{t \to 0} \frac{f(x + t) - f(x)}{t}.
\]</span></p>
<p>For a multivariate function <span class="math inline">\(f: \mathbb{R}^m \to \mathbb{R}\)</span>, we write the partial derivative as <span class="math display">\[
\frac{\partial f}{\partial x_i} = \lim_{t \to 0} \frac{f(x_1, \ldots, x_i + t, \ldots, x_d) - f(x_1, \ldots, x_i, \ldots, x_d)}{t}.
\]</span></p>
<p>Consider two functions <span class="math inline">\(f: \mathbb{R} \to \mathbb{R}\)</span> be a function, and <span class="math inline">\(y: \mathbb{R} \to \mathbb{R}\)</span>. The chain rule tells us that the derivative of the composition <span class="math inline">\(f(y(x))\)</span> is given by <span class="math inline">\(\frac{df}{dx} = \frac{df}{dy} \cdot \frac{dy}{dx}\)</span>. To see this, we can write <span class="math display">\[
\frac{df}{dx}
= \lim_{t \to 0} \frac{f(y(x + t)) - f(y(x))}{t}
= \lim_{t \to 0} \frac{f(y(x + t)) - f(y(x))}{y(x + t) - y(x)} \frac{y(x + t) - y(x)}{t}.
= \frac{df}{dy} \cdot \frac{dy}{dx},
\]</span> where the last equality follows as long as <span class="math inline">\(\lim_{t \to 0} y(x + t) - y(x) = 0\)</span>.</p>
<p>Let <span class="math inline">\(f: \mathbb{R}^m \to \mathbb{R}\)</span> be a function, and <span class="math inline">\(y_i: \mathbb{R} \to \mathbb{R}\)</span> be a sequence of <span class="math inline">\(m\)</span> functions. The multivariate chain rule tells us that the derivative of the composition <span class="math inline">\(f(y_1(x), \ldots, y_m(x))\)</span> is given by <span class="math display">\[
\frac{df}{dx} = \left(
  \frac{df}{dy_1} \cdot \frac{dy_1}{dx} + \ldots + \frac{df}{dy_m} \cdot \frac{dy_m}{dx}
\right).
\]</span></p>
<p>Backpropagation is simply an application of the multivariate chain rule, and is the natural analog to the forward pass in a neural network.</p>
<center>
<img src="images/neural_forward.svg" class="responsive-img">
</center>
<p>During the forward pass, we successively compute the outputs, and feed them into the <em>next</em> layer. Consider one of the functions in the neural network: The function <span class="math inline">\(v : \mathbb{R}^{d_\ell} \to \mathbb{R}\)</span> maps the input(s) parameters <span class="math inline">\(u_1, \ldots, u_d\)</span> to an output, which itself is passed to another function. (In general, <span class="math inline">\(d\)</span> could be 1, e.g., if the <span class="math inline">\(v\)</span> were an activation function.) When we compute <span class="math inline">\(v(u_1, \ldots, u_d)\)</span>, we crucially only need to know the values of parameters <span class="math inline">\(u_1, \ldots, u_d\)</span>, rather than any of the parameters that we used to compute <span class="math inline">\(u_1, \ldots, u_d\)</span>.</p>
<p>The time complexity of the forward pass is linear in the number of times a parameter is used in a function, which itself is linear in the number of parameters.</p>
<center>
<img src="images/neural_backward.svg" class="responsive-img">
</center>
<p>During the backward pass, we compute the derivative of the loss function with respect to the parameters in one layer, then feed these derivatives into the <em>prior</em> layer. Suppose the parameter <span class="math inline">\(u\)</span> is used in <span class="math inline">\(d\)</span> different functions <span class="math inline">\(v_1, \ldots, v_n\)</span>. By the multivariate chain rule, the derivative of the loss function with respect to <span class="math inline">\(u\)</span> is given by <span class="math display">\[
\frac{\partial \mathcal{L}}{\partial u} = \frac{\partial \mathcal{L}}{\partial v_1} \cdot \frac{\partial v_1}{\partial u} + \ldots + \frac{\partial \mathcal{L}}{\partial v_n} \cdot \frac{\partial v_n}{\partial u}.
\]</span> When we compute this partial derivative, we crucially only need to know the partial derivatives <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial v_1}, \ldots, \frac{\partial \mathcal{L}}{\partial v_n}\)</span>, and the partial derivatives <span class="math inline">\(\frac{\partial v_1}{\partial u}, \ldots, \frac{\partial v_n}{\partial u}\)</span>. By the time we reach <span class="math inline">\(u\)</span> in the backward pass, the first set of partial derivatives are already known from the prior layer, and the second set of partial derivatives only depend on how <span class="math inline">\(u\)</span> is used in the functions <span class="math inline">\(v_1, \ldots, v_n\)</span>.</p>
<p>Like the forward pass, the time complexity of the backward pass is linear in the number of times a parameter appears in a function, which itself is linear in the number of parameters.</p>
</section>
<section id="linear-algebraic-view-of-backpropagation" class="level3">
<h3 class="anchored" data-anchor-id="linear-algebraic-view-of-backpropagation">Linear Algebraic View of Backpropagation</h3>
<p>Neural networks have been studied since the the 1950s, and the backpropagation algorithm was first proposed in the 1980s. So why did it take so long for neural networks to become popular? The answer is that neural networks require <em>many</em> parameters to be effective, and for a long time, we did not have the computational resources to train them at sufficient scale. A breakthrough in the 2010s made neural networks practical: Originally intended for matrix multiplication in graphics, the development of Graphical Processing Units (GPUs) allowed us to efficiently implement backpropagation.</p>
<p>Consider two adjacent layers of neurons: <span class="math inline">\(\mathbf{u} \in \mathbb{R}^d\)</span> and <span class="math inline">\(\mathbf{v} \in \mathbb{R}^n\)</span> in a fully connected neural network. Suppose the weight matrix between the two layers is <span class="math inline">\(\mathbf{W} \in \mathbb{R}^{n \times d}\)</span>. (We will ignore the activation function because it can easily be applied in parallel to each neuron.) During the forward pass, we have already computed the neuron values <span class="math inline">\(\mathbf{u}\)</span>. With this information, we can compute the values in the second layer as <span class="math display">\[
\mathbf{v} = \mathbf{W} \mathbf{u}.
\]</span> In particular, the <span class="math inline">\(i\)</span>th value in the second layer is given by <span class="math display">\[
v_i = \sum_{j=1}^d [\mathbf{W}]_{i,j} u_j.
\]</span> Because the function is linear, observe that the derivatives of the second layer with respect to the first layer are simply given by <span class="math display">\[
\frac{\partial v_i}{\partial u_j} = [\mathbf{W}]_{i,j}.
\]</span></p>
<p>During the backward pass, we have already computed the gradients of the loss function with respect to the second layer <span class="math inline">\(\nabla_{\mathbf{v}} \mathcal{L}\)</span>. With this information, we can compute the gradients of the loss function with respect to the first layer as <span class="math display">\[
\nabla_{\mathbf{u}} \mathcal{L} = \mathbf{W}^\top \nabla_{\mathbf{v}} \mathcal{L}.
\]</span> In particular, the gradient of the loss function with respect to the <span class="math inline">\(j\)</span>th neuron in the first layer is given by <span class="math display">\[
\frac{\partial \mathcal{L}}{\partial u_j}
= \sum_{i=1}^n \frac{\partial \mathcal{L}}{\partial v_i}
\frac{\partial v_i}{\partial u_j}
= \sum_{i=1}^n [\mathbf{W}^\top]_{j,i} \frac{\partial \mathcal{L}}{\partial v_i}.
\]</span></p>
<p><strong>Note:</strong> Instead of the gradients with respect to the neurons, we are actually interested in the gradients of the loss function with respect to the parameters, because we are updating the <em>parameters</em> during training. But, we need the neuron gradients to compute the parameter gradients: Once we have the neuron gradients, we compute the parameter gradients with another application of the chain rule.</p>
<p>The power of the linear algebraic view is that we can use hardware specialized for matrix multiplication to efficiently compute the gradients!</p>
<p>Neural networks are a powerful tool for learning complex functions, but notoriously difficult to understand because of their many parameters and nonlinearities. The big surprise of deep learning is that, even though they are not convex functions, gradient descent works remarkably well to train them. We don’t have good intuition for what happens in high-dimensional space (the loss function depends on the number of parameters, which can be very large), but it <em>seems</em> like there’s almost always a parameter update that at least slightly improves the loss function.</p>



</section>

</main> <!-- /main -->
<script>
document.addEventListener("DOMContentLoaded", function () {
  const wordsPerMinute = 200;
  const text = document.body.innerText;
  const words = text.trim().split(/\s+/).length;
  const readingTime = Math.ceil(words / wordsPerMinute);

  const readTimeEl = document.createElement("div");
  readTimeEl.innerText = `⏱️ ${readingTime} min read`;

  // Style it to appear centered
  readTimeEl.style.fontSize = "0.9em";
  readTimeEl.style.margin = "1em auto";
  readTimeEl.style.textAlign = "left";
  readTimeEl.style.width = "100%";

  const title = document.querySelector("h1");
  if (title) {
    title.parentNode.insertBefore(readTimeEl, title.nextSibling);
  }
});
</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>