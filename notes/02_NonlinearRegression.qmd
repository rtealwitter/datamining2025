---
title: "**Gradient Descent and Non-linear Regression**"
format:
  html:
    toc: true
    math: true
---

Today, we continue our discussion of supervised learning, where we have labeled training data and our goal is to train a model that accurately predicts the labels of unseen testing data.
Recall that our general approach to supervised learning is to use empirical risk minimization:
We focus on a **class of models**, define a **loss function** that quantifies how accurately a particular model explains the training data, and **search for a model with low loss**.

Last week, we considered the class of linear models, i.e., the prediction is a weighted linear combination of the input features.
We chose to measure loss via mean squared error, a choice both rooted in convenience and a compelling modeling assumption (if the data is generated by a linear process with Gaussian, then the mean squared error is the maximum likelihood estimator).
In order to find the best parameters of the linear model, we used our knowledge of gradients to exactly compute the parameters that minimize the mean squared error loss.

This week, we will address two of the nagging issues with computing the best parameters of a linear model.
We begin with the issue of *runtime*; computing the optimal parameters requires building a large matrix and inverting it, which can be computationally expensive.
We will now see how we can use gradient descent to speed up this process.

## Gradient Descent
The mean squared error of a linear model is particularly well-behaved because it is *convex* i.e., there is a single minimum.
Previously, we computed the parameters where the gradient is 0, which, by convexity, immediately gave us the single optimal point.
However, we could instead use a more relaxed approach; rather than jumping immediately to the best parameters, we can iterate towards better parameters by taking steps towards lower loss.

<center><img src="images/regression_descent.pdf" width="500"></center>

Gradient descent is an iterative method for moving in the direction of steepest *descent*.
Concretely, the process produces a sequence of parameters $\mathbf{w}^{(1)}, \mathbf{w}^{(2)}, \ldots$.
At each step, we compute the direction of steepest *ascent* i.e., the gradient of the loss function with respect to each parameter.
The gradient quantifies how the loss function responds as we tweak each parameter.
If the partial derivative is positive (increasing the parameter increases the loss), then we will want to decrease the parameter.
Analogously, if the partial derivative is negative (increasing the parameter decreases the loss), then we will want to increase the parameter.
In both cases, we are moving in the direction away from the gradient.
Hence, we reach the next parameter vector by subtracting the gradient from the current parameter vector:
$$
\begin{align*}
\mathbf{w}^{(t+1)} \gets \mathbf{w}^{(t)} - \alpha \nabla_\mathbf{w} \mathcal{L}(\mathbf{w}^{(t)}),
\end{align*}
$$
where $\alpha$ is a small positive constant called the *step size* or *learning rate*.

Notice that, for one, this approach stops when we reach a point where the gradient is 0, i.e., we have reached a local minimum.

Beyond the stopping condition, why does this work?
Consider the one dimensional setting.
The derivative of the loss function is
$$
\begin{align*}
\mathcal{L}'(w) = \lim_{\Delta \to 0} \frac{\mathcal{L}(w + \Delta) - \mathcal{L}(w)}{\Delta}.
\end{align*}
$$
so, for small $\Delta$, we can approximate the loss function as
$$
\begin{align*}
\mathcal{L}(w+\Delta) - \mathcal{L}(w) &\approx \mathcal{L}'(w) \cdot \Delta \\
\end{align*}
$$
We want $\mathcal{L}(w+\Delta)$ to be smaller than $\mathcal{L}(w)$, so we want $\mathcal{L}'(w) \Delta < 0$.
This can be achieved by setting $\Delta = -\eta \mathcal{L}'(w)$, where $\eta$ is a small positive constant.
Then $w^{(t+1)} = w^{(t)} - \eta \mathcal{L}'(w^{(t)})$ is a step in the direction of descent.

In the multi-dimensional setting, the partial derivative of the loss function with respect to each parameter is given by the gradient:
$$
\frac{\partial \mathcal{L}}{\partial w_i} = \lim_{\Delta \to 0} \frac{\mathcal{L}(\mathbf{w} + \Delta \mathbf{e}_i) - \mathcal{L}(\mathbf{w})}{\Delta},
$$
where $\mathbf{e}_i$ is the $i$-th standard basis vector.
Then, for small $\Delta$, we can approximate the loss function as
$$
\mathcal{L}(\mathbf{w} + \Delta \mathbf{e}_i) - \mathcal{L}(\mathbf{w}) \approx \frac{\partial \mathcal{L}}{\partial w_i} \cdot \Delta
= \langle \nabla_\mathbf{w} \mathcal{L}(\mathbf{w}), \Delta \mathbf{e}_i \rangle.
$$
For a general vector $\mathbf{v}$, we can write
$$
\mathcal{L}(\mathbf{w} + \Delta \mathbf{v}) - \mathcal{L}(\mathbf{w}) \approx \langle \nabla_\mathbf{w} \mathcal{L}(\mathbf{w}), \Delta \mathbf{v} \rangle.
$$
If we want to move in the direction of steepest descent, we can set $\Delta \mathbf{v} = -\eta \nabla_\mathbf{w} \mathcal{L}(\mathbf{w})$, where $\eta$ is a small positive constant.
Then, we have $\mathcal{L}(\mathbf{w} - \eta \nabla_\mathbf{w} \mathcal{L}(\mathbf{w})) - \mathcal{L}(\mathbf{w}) \approx -\eta \langle \nabla_\mathbf{w} \mathcal{L}(\mathbf{w}), \nabla_\mathbf{w} \mathcal{L}(\mathbf{w}) \rangle = -\eta \|\nabla_\mathbf{w} \mathcal{L}(\mathbf{w})\|^2$.
Why is this the right choice? Well, recall for any vectors $\mathbf{a}$ and $\mathbf{b}$, we have $\langle \mathbf{a}, \mathbf{b} \rangle = \|\mathbf{a}\| \|\mathbf{b}\| \cos(\theta)$, where $\theta$ is the angle between the two vectors.
The largest value of $\cos(\theta)$ is $1$, which occurs when the two vectors are in the same direction.
Notice we achieve the largest magnitude of the inner product when we take the step in the direction of the gradient, i.e., $- \nabla_\mathbf{w} \mathcal{L}(\mathbf{w})$.

For linear models, we already know the gradient of the mean squared error loss:
$$
\nabla_\mathbf{w} \mathcal{L}(\mathbf{w}) = \frac2{n} \mathbf{X}^\top (\mathbf{X w - y}).
$$
In contrast to the $O(nd^2 + d^3)$ time required to compute the exact solution, we can now compute the gradient in $O(nd)$ time, as long as we restrict ourselves to matrix-vector multiplications rather than matrix-matrix multiplications.
The final time complexity of gradient descent is $O(T nd)$, where $T$ is the number of iterations of gradient descent.

While we have achieved a significant speedup, $O(T nd)$ could still be prohibitively large when we have a large number of data points $n$ and/or a large number of features $d$.
Our solution will be a *stochastic* approach, where we only use a small random subset of the data to compute the gradient.

### Stochastic Gradient Descent

Our approach will be similar to gradient descent, except now we will compute the gradient using only the data in the batch.
For the mean squared error loss, we can write the loss function for a random subset $S$ of the data as
$$
\mathcal{L}_S(\mathbf{w}) = \frac1{|S|} \sum_{i \in S} (f(\mathbf{x}^{(i)}) - y^{(i)})^2.
$$
Then, for our linear model, the gradient of the loss function with respect to the parameters $\mathbf{w}$ is given by
$$
\nabla_\mathbf{w} \mathcal{L}_S(\mathbf{w}) = \frac2{|S|} \mathbf{X}_S^\top (\mathbf{X}_S \mathbf{w} - \mathbf{y}_S),
$$
where $\mathbf{X}_S$ is the data matrix for the subset $S$ and $\mathbf{y}_S$ is the target vector for the subset $S$.
One iteration of stochastic gradient descent takes time $O(|S|d)$, which can be much faster than the $O(nd)$ time required to compute the gradient for the full dataset.

### Adaptive Step Sizes
The step size $\alpha$ is a crucial hyperparameter in gradient descent.
If 
<span style="color:red;font-size:14px">$\alpha$</span>
is too small, then the algorithm will take a long time to converge because it will take small steps towards the minimum.
If
<span style="color:red;font-size:20px">$\alpha$</span>
is too large, then the algorithm may overshoot the minimum and diverge by repeatedly moving in the right direction but by too much.
Instead, we want to choose a step size
<span style="color:green">$\alpha$</span>
that is just right, allowing us to make progress towards the minimum without overshooting.

<center><img src="images/regression_steps.pdf" width="500"></center>

There are several strategies for choosing the step size:

- When searching manually, we often exponentially increase and decrease the step size i.e., multiply by a factor of $2$ or $1/2$. If the loss consistently improves over several iterations of gradient descent, then we try increasing the step size; if the loss is unstable, then we try decreasing the step size.

- Learning rate schedules offer a more automated approach, where we start with a large step size and then decrease it over time. This is often done by multiplying the step size by a factor less than $1$ after each iteration. The intuition is that we want to take large steps at the beginning to quickly find a good region of the parameter space, and then take smaller steps so as to not overshoot the minima as we get closer. 

- An even more automated approach is to use an adaptive learning rate, where we adjust the step size based on the gradient. If the gradient is large, then we can decrease the step size to avoid overshooting; if the gradient is small, then we can increase the step size to speed up convergence.
One implementation of this idea is as follows:
$$
\alpha^{(t+1)} \gets \frac{\alpha^{(t)}}{(\nabla_\mathbf{w} \mathcal{L}(\mathbf{w}^{(t)}))^2}.
$$
Notice that the division is element-wise, so we are adjusting the step size for each parameter individually based on the squared partial derivative of that parameter.

In addition the step size, the direction of each step is also important.

### Momentum
The idea of gradient descent is to converge to a local minimum of the loss function, but things can go wrong even if we have the right step size:
The gradient may not point in the direction of the minima if, for example, the loss function is not symmetric.
The plot illustrates this issue for a convex loss function on two parameters ${w}_1$ and ${w}_2$, where the loss function is given by level sets.
In the plot, a standard gradient descent approach takes many steps but the directions cancel out, resulting in a zig-zag pattern that slows convergence.

<center><img src="images/regression_momentum.pdf" width="600"></center>

Our solution is to keep track of the direction we have been moving in and use that to inform our next step.
This idea, called *momentum*, retains a running average of the gradients, which allows us to smooth out the direction of the steps.
We can think of momentum as a ball rolling down a hill, even when the ball is pushed left or right, it will continue to roll downwards.
An implementation of momentum is as follows:
$$
\begin{align}
\mathbf{m}^{(t+1)} &= \beta \mathbf{m}^{(t)} + (1 - \beta) \nabla_\mathbf{w} \mathcal{L}_S(\mathbf{w}^{(t)}) \\
\mathbf{w}^{(t+1)} &= \mathbf{w}^{(t)} - \alpha \mathbf{m}^{(t+1)},
\end{align}
$$
where $\beta$ is a hyperparameter that controls the amount of history we keep in the momentum vector $\mathbf{m}^{(t)}$.

## Beyond Linear Models

Introduce variables that give better fit to the data, manipulate the data to create new features, e.g., polynomial regression.

Claim: The fit of the regression model can only be improved by adding additional features.

(Add picture here of polynomial regression with various degrees of polynomial, e.g., linear, quadratic, cubic, etc.)

In picture, we can tell which gives the right fit

How do we do this automatically?

### Generalization Error

Training data versus testing data

Generalization error

If we withhold data, this gives us an unbiased estimate of the generalization error: Expectation of performance on random sample, is performance on true data set.

Can we use data more efficiently? $k$-fold cross validation

In practice, we often bias this process by training, testing, updating the model (hyperparameters, architecture, training method), training, testing, updating the model, etc. The repeated use of the test data means our model depends on the test data and eventually overfits to it, even though we are not using the test data to train the model.

### Regularization

Non-linear models are incredibly powerful models that can approximate any function, given enough data and features.
However, this power comes with a cost: non-linear models can be very complex models and can easily overfit the training data.
That is, they can learn to memorize, but fail to generalize to unseen data.

(Redo this figures with polynomial regression)

<center><img src="images/regression_overfitting.pdf" width="700"></center>

When we believe our data comes from a simpler generating process, it makes sense to use a simpler model.
Even when that simpler generating process is not a linear model, we can attempt to find simpler models through *regularization*.
Regularization is a technique that adds a penalty to the loss function to discourage the model from fitting the training data too closely.

$$
\begin{align*}
\frac1n \sum_{i=1}^n (f(\mathbf{x}^{(i)}) - y^{(i)})^2 + \lambda \|\mathbf{w}\|^2_2,
\end{align*}
$$
where $\lambda$ is a hyperparameter that controls the strength of the penalty and $\|\mathbf{w}\|_2$ is the $\ell-2$ norm of the weights, which is the square root of the sum of the squares of the weights.


The idea is that we can keep the model "simple" by penalizing large weights, which would otherwise allow the model to achieve large changes in the output for small changes in the input.

<center><img src="images/regression_regularization.pdf" width="700"></center>

In the plot, we see data generated from a quadratic function.
The linear model is too simple and fails to capture the underlying relationship, while the standard neural network is too complex and overfits the training data.
The regularized neural network, however, is able to capture the underlying relationship while still being simple enough to generalize to unseen data.

How to control the strength of the regularization $\lambda$?

How do we minimize the regularized loss function for regression?

We can also use a regularizaiton with $\ell_1$ norm, which penalizes the absolute value of the weights.
This is called *Lasso* regression, and it has the effect of encouraging sparsity in the model, i.e., some weights will be exactly zero.
Useful when we have many features, but we believe only a few of them are actually relevant to the prediction.

## Going Forward

Today, we got a taste of how to use gradient descent to optimize non-linear models, particularly neural networks.
This is a rich area that has seen incredible recent advancements, particularly in the context of generative AI.
In fact, I teach an entire [course](https://www.rtealwitter.com/deeplearning2025/) dedicated to deep learning.
In this course, however, we will instead focus on the mathematical foundations of machine learning.

We have so far explored supervised learning in the regression setting, where the labels are real numbers.
going forward, we will consider how to handle the case where the labels are categorical, e.g., we want to classify the data into two categories such as "cat" and "dog" or "spam" and "not spam".