[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "MATH 166: Syllabus",
    "section": "",
    "text": "Course Description: Data mining is the process of discovering patterns in large data sets using techniques from mathematics, computer science and statistics with applications ranging from biology and neuroscience to history and economics. The goal of the course is to teach students fundamental data mining techniques that are commonly used in practice. Students will learn advanced data mining techniques (including linear classifiers, clustering, dimension reduction, transductive learning and topic modeling).\nPrerequisites: I expect familiarity with calculus, linear regression, probability, and Python. In particular, I expect you are comfortable with derivatives, the chain rule, gradients, matrix multiplication, and probability distributions. If this isn’t the case, please contact me as soon as possible.\nStructure: We will meet on Tuesdays and Thursdays. The first section is from 2:45 to 4pm and the second section is from 4:15 to 5:30pm. I will hold my office hours TBD. If you would like to meet outside of these times, please email me.\nResources: The primary resource for this class are the typed notes on the homepage. I highly recommend reading them before each class (it should take about 15 minutes). In addition, I will post my preparation for the slides the night before each class.\nDiscussion: Please post all your course related questions on Canvas. If your question reveals your solution to a homework problem, please email me instead.\n\nGrading\nYour grade in the class will be based on the number of points \\(p\\) that you earn. You will receive an A if \\(p \\geq 93\\), an A- if \\(93 &gt; p \\geq 90\\), a B+ if \\(90 &gt; p \\geq 87\\), and so on. You may earn points through the following assignments:\n\nParticipation (10 points): The classes at CMC are intentionally small. Unless you have a reasonable excuse (e.g. sickness, family emergency), I expect you to attend every class. Whether you are able to attend or not, I expect you to fill out the form linked from the home page to receive credit for participation (one point per lecture day that you fill it out). Of course, if you are not able to attend in person, you should read the notes before filling out the form.\nProblem Sets (10 Points): Learning requires practice. Your main opportunity to practice the concepts we cover in this class will be on the problem sets. Your grade will be based on turning in solutions to each problem and, so that you engage with the solutions, a self grade of your own work. Because I do not want to incentivize the use of LLMs, I will not grade your solutions for correctness; instead, your problem set grade is based on completion and the accuracy of your own self grade.\nQuizzes (20 Points): In lieu of grading for correctness on the problem sets, I will give short quizzes at the beginning of our Tuesday classes. These quizzes will be based on the problem sets and will test your understanding of the concepts we cover in class. The quizzes will be short (10-15 minutes) and will be graded for correctness.\nWritten Exam (20 Points): The first midterm will be a written exam. It will cover the material we have covered in class up to that point. The exam will be open book and open notes, but you will not be allowed to use any electronic devices (including your phone). The exam will be graded for correctness.\nOral Exam (20 Points): The second midterm will be an oral exam. I will individually ask you questions about the concepts we have covered in class during a 30-minute meeting. The goal is to simultaneously assess your understanding of the material and give you a chance to practice explaining the concepts, as you would in a technical interview. I will provide a list of topics that I will ask about in advance.\nProject (20 Points): The final project will be a chance for you to apply the concepts we have covered in class to a real-world problem. You will select a topic we cover in class and implement an algorithm we discussed on a data set of your choosing. You will write a report describing your results and what you learned. You will also give a presentation showcasing your results to the class. Except in special circumstances, you will complete your project as an individual.\nExtra Credit: This is the first time I am teaching this class, so my typed notes are work in progress and I would love your help improving them! If you find an issue, please email me. I will give extra credit to the first person to find each typo (worth 1/4 point), ambiguous statement (worth 1/2 point), and mistake (worth 1 point)\n\nLate Policy: I expect all assignments to be turned in on time. If you are unable to turn in an assignment on time, you must email me 24 hours before the assignment is due to request an extension.\n\n\nHonor Code\nAcademic integrity is an important part of your learning experience. You are welcome to use online material and discuss problems with others but you must explicitly acknowledge the outside resources (website, person, or LLM) on the work you submit.\nLarge Language Models: LLMs are a powerful tool. However, while they are very good at producing human-like text, they have no inherent sense of ‘correctness’. You may use LLMs (as detailed below) but you are wholly responsible for the material you submit.\nYou may use LLMs for:\n\nImplementing short blocks of code that you can easily check.\nAnswering simple questions whose answers you can easily verify.\n\nDo not use LLMS for:\n\nImplementing extensive blocks of code or code that you don’t understand.\nAnswering complicated questions (like those on the problem sets) that you cannot easily verify.\n\nUltimately, the point of the assignments in this class are for you to practice the concepts. If you use an LLM in lieu of practice, then you deny yourself the chance to learn.\n\n\nAcademic Accommodations\nIf you have a Letter of Accommodation, please contact me as early in the semester as possible. If you do not have a Letter of Accommodation and you believe you are eligible, please reach out to Accessibility Services at accessibilityservices@cmc.edu."
  },
  {
    "objectID": "notes/code.html",
    "href": "notes/code.html",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\nLinear Regression Figures\n\nnp.random.seed(1234) # Seed randomness\n\nn = 10 # Number of observations\nw = 2 # True parameter\nX = np.random.rand(n) # x-values\ny = X.dot(w).T + np.random.normal(size=n) * .2 #y-values\n\nplt.scatter(X,y, color='black', label=r'Data: $(x^{(i)}, y^{(i)})$')\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\nxaxis = np.arange(0,1,.01)\nplt.plot(xaxis, xaxis*.5, label=r'Line: $f(x) = .5x$', color='red')\nplt.plot(xaxis, xaxis*w, label=r'Line: $f(x) = 2x$', color='green')\nplt.legend()\nplt.title(r'Linear Regression in $\\mathbb{R}^1$')\nplt.savefig('images/regression_1d.pdf')\n\n\n\n\n\n\n\n\n\nplt.xlabel(r'$z$')\nplt.ylabel(r'$\\mathcal{L}(z)$')\nxaxis = np.arange(-1.5,1.5,.001)\nplt.plot(xaxis, xaxis**2, label=r'Squared Loss: $\\mathcal{L}(z)=z^2$', color='blue')\nplt.plot(xaxis, np.abs(xaxis), label=r'Absolute Loss: $\\mathcal{L}(z)=|z|$', color='purple', linestyle='dotted')\nplt.legend()\nplt.title(r'Squared and Absolute Losses')\nplt.savefig('images/regression_losses.pdf')\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\n\n# Seed randomness\nnp.random.seed(1234)\nn = 10  # Number of observations\nw = np.array([2, .5])  # True parameter\nX = np.random.rand(n, 2)  # x-values\ny = X.dot(w).T + np.random.normal(size=n) * .1  # y-values\n\n# Create figure and 3D axis\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Scatter plot for data points\nax.scatter(X[:, 0], X[:, 1], y, color='black', label=r'Data: $(x_1^{(i)}, x_2^{(i)}, y^{(i)})$')\n\n# Hyperplane 1: Green\nx1 = np.arange(0, 1, .01)\nx2 = np.arange(0, 1, .01)\nX1, X2 = np.meshgrid(x1, x2)\nZ = w[0] * X1 + w[1] * X2\nax.plot_surface(X1, X2, Z, alpha=.5, color='green')\n\n# Hyperplane 2: Red\nax.plot_surface(X1, X2, .5 * X1 + 0 * X2, alpha=.5, color='red')\n\n# Labels and title\nax.set_xlabel(r'$x_1$')\nax.set_ylabel(r'$x_2$')\nax.set_zlabel(r'$y$')\nax.set_title(r'Linear Regression in $\\mathbb{R}^2$')\n\n# Manually create custom legend handles for the surfaces\nhandles = [\n    Line2D([0], [0], marker='o', color='black', markerfacecolor='black', markersize=6, label=r'Data: $(x_1^{(i)}, x_2^{(i)}, y^{(i)})$'),\n    Line2D([0], [0], color='green', lw=4, label=r'Hyperplane: $f(x) = 2x_1 + .5x_2$'),\n    Line2D([0], [0], color='red', lw=4, label=r'Hyperplane: $f(x) = .5x_1 + 0x_2$')\n]\n\n# Add legend\nplt.legend(handles=handles, loc='upper left', framealpha=1)\n\n# Save the figure\nplt.savefig('images/regression_2d.pdf', bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLogistic Regression Figures\n\nplt.xlabel(r'$z$')\n#plt.ylabel(r'$\\sigma(z)$')\nxaxis = np.arange(-10,10,.001)\nsigma = lambda z : 1 / (1+np.exp(-z))\nplt.plot(xaxis, sigma(xaxis), label=r'$\\sigma(z)$', color='blue')\nplt.legend()\nplt.title(r'Sigmoid Function')\nplt.savefig('images/logistic_sigmoid.pdf')\n\n\n\n\n\n\n\n\n\nplt.xlabel(r'$z$')\n#plt.ylabel(r'$\\sigma(z)$')\nxaxis = np.arange(-10,10,.001)\nsigma = lambda z : 1 / (1+np.exp(-z))\nplt.plot(xaxis, sigma(xaxis), label=r'$\\sigma(z)$', color='blue')\nplt.legend()\nplt.title(r'Sigmoid Function')\nplt.savefig('images/logistic_sigmoid.pdf')\n\n\n\n\n\n\n\n\n\n# Create data points\nz = np.linspace(0.1, 5, 1000)  # Avoid z=0 since ln(0) is undefined\ny = -np.log(z)\n\n# Create the figure and axis\nplt.figure(figsize=(10, 6))\nplt.plot(z, y, 'b-', linewidth=2, label='-ln(z)')\n\n# Add grid\nplt.grid(True, linestyle='--', alpha=0.7)\n\n# Add title and labels\nplt.title('Graph of -ln(z)', fontsize=14)\nplt.xlabel('z', fontsize=12)\nplt.ylabel('-ln(z)', fontsize=12)\n\n# Add legend\nplt.legend(fontsize=12)\n\n# Add horizontal and vertical axes\nplt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nplt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n\n# Set reasonable axis limits\nplt.xlim(0, 5)\nplt.ylim(-2, 3)\n\n# Adjust layout\nplt.tight_layout()\n\nplt.savefig('images/NegLog.pdf')\n# Show plot\nplt.show()"
  },
  {
    "objectID": "notes/02_LinearRegressionPlus.html",
    "href": "notes/02_LinearRegressionPlus.html",
    "title": "Linear Regression and Optimization",
    "section": "",
    "text": "Recall set up of supervised learning problem, empirical risk minimization, and the three components: model class, loss, and optimizer. Last time, we explored linear regression and how to exactly fit a linear model to data using the mean squared error loss. There were two issues: computing the exact solution could be computationally expensive, and the data may not have a linear relationship with the labels."
  },
  {
    "objectID": "notes/02_LinearRegressionPlus.html#gradient-descent",
    "href": "notes/02_LinearRegressionPlus.html#gradient-descent",
    "title": "Linear Regression and Optimization",
    "section": "Gradient Descent",
    "text": "Gradient Descent\nIntuition, instead of just going for the exact solution, we can use an iterative method to find a good approximation by taking steps in the right direction.\nPut an image here\nWe compute the direction of the steepest ascent by computing the gradient of the loss function. Intuitively, this gives how the loss function changes as we change the model parameters. Then we move away from steepest ascent a small step size \\(\\eta\\).\nFor linear models, this is computed more quickly as \\(\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w})\\). Recall the gradient of the mean squared error loss is given by \\(\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}) = \\frac2{n} \\mathbf{X}^\\top (\\mathbf{X w - y})\\). Computing this take stime \\(O(nd)\\), which is much faster than the \\(O(nd^2 + d^3)\\) time required to compute the exact solution.\nThis could still be large when we have a large number of data points \\(n\\) and/or a large number of features \\(d\\). We can address this by using stochastic gradient descent.\n\nStochastic Gradient Descent\nThe idea of stochastic gradient descent is to use a small random subset of the data to compute the gradient. This is often called a batch. Same idea as before, but now the loss is given by \\[\n\\mathcal{L}_S(\\mathbf{w}) = \\frac1{|S|} \\sum_{i \\in S} (f(\\mathbf{x}^{(i)}) - y^{(i)})^2,\n\\] where \\(S\\) is a random subset of the data. Then we can compute the gradient as \\[\n\\nabla_\\mathbf{w} \\mathcal{L}_S(\\mathbf{w}) = \\frac2{|S|} \\mathbf{X}_S^\\top (\\mathbf{X}_S \\mathbf{w} - \\mathbf{y}_S),\n\\] where \\(\\mathbf{X}_S\\) is the data matrix for the subset \\(S\\) and \\(\\mathbf{y}_S\\) is the target vector for the subset \\(S\\). This takes time \\(O(|S|d)\\), which can be much faster than the \\(O(nd)\\) time required to compute the gradient for the full dataset.\n\n\nAdaptive Step Sizes\nThe step size \\(\\eta\\) is a hyperparameter that we need to choose. If \\(\\eta\\) is too small, then the algorithm will take a long time to converge because it will take small steps towards the minimum. If \\(\\eta\\) is too large, then the algorithm may overshoot the minimum and diverge by repeatedly moving int he right direction but by too much.\n(Put an image here)\nSeveral strategies exist for choosing the step size:\n\nWhen searching manually, we can exponentially increase and decrease the step size i.e., multiply by a factor of \\(2\\) or \\(1/2\\). If the loss consistently decreases, then we can try increasing the step size; if the loss is unstable, then we can try decreasing the step size.\nLearning rate schedules, where we start with a large step size and then decrease it over time. This is often done by multiplying the step size by a factor less than \\(1\\) after each iteration.\nAdaptive learning rates, where we adjust the step size based on the gradient. For example, if the gradient is large, then we can decrease the step size to avoid overshooting; if the gradient is small, then we can increase the step size to speed up convergence.\n\n\n\nMomentum\nThe convergence of gradient descent is a complex topic, but we can give some intuition. The idea is that the algorithm will converge to a local minimum of the loss function, but things can go wrong even if we have the right step size. The gradient may not point in the direction of the minimum, especially if the loss function is oblong. Here, we take many steps but many of the directions cancel out, and we end up not moving very far.\n(Put an image here)\nOur solution is to keep track of the direction we have been moving in and use that to inform our next step. We can do this by keeping a running average of the gradients, which is called momentum. We can think of momentum as a ball rolling down a hill, even when the ball is pushed left or right, it will continue to roll downwards.\nCombining the ideas of adaptive step sizes and momentum, we can use the following popular update rule: Adam, which combines both methods in the following way: \\[\n\\begin{align}\n\\mathbf{m}_t &= \\beta_1 \\mathbf{m}_{t-1} + (1 - \\beta_1) \\nabla_\\mathbf{w} \\mathcal{L}_S(\\mathbf{w}_{t-1}) \\\\\n\\mathbf{v}_t &= \\beta_2 \\mathbf{v}_{t-1} + (1 - \\beta_2) \\nabla_\\mathbf{w} \\mathcal{L}_S(\\mathbf{w}_{t-1})^2 \\\\\n\\mathbf{w}_t &= \\mathbf{w}_{t-1} - \\eta \\frac{\\mathbf{m}_t}{\\sqrt{\\mathbf{v}_t} + \\epsilon},\n\\end{align}\n\\] where \\(\\beta_1\\) and \\(\\beta_2\\) are hyperparameters that control the momentum and adaptive step size, respectively, and \\(\\epsilon\\) is a small constant to avoid division by zero."
  },
  {
    "objectID": "notes/02_LinearRegressionPlus.html#non-linear-models",
    "href": "notes/02_LinearRegressionPlus.html#non-linear-models",
    "title": "Linear Regression and Optimization",
    "section": "Non-Linear Models",
    "text": "Non-Linear Models\nWe have now seen how to fit linear models to data using the mean squared error loss and gradient descent. However, we have not addressed the issue of model class misspecification. We have assumed that the data has a linear relationship with the labels, but what happens when this is not true?\n(Put an image here, of linear model as a neuron, and then multiple neurons in a network)\nWe can repeatedly combine linear models to create more complex models. We can do this by using a neural network, which is a collection of linear models (called neurons) that are combined together. The idea is the same as before: we take a linear combination of the inputs, just now, we are repeatedly combining linear models to create a more complex model.\nWe can think of a fully connected layer (all neurons in one layer are connected to all neurons in the next layer) as matrix multiplication, where the matrix \\(\\mathbf{W} \\in \\mathbb{R}^{d_\\text{in} \\times d_\\text{out}}\\) is the weight matrix and the input \\(\\mathbf{x} \\in \\mathbb{R}^{d_\\text{in}}\\) is the input vector. Then multiplying several weight matrices together gives \\[\nf(\\mathbf{x}) = \\mathbf{W}_k \\mathbf{W}_{k-1} \\cdots \\mathbf{W}_1 \\mathbf{x},\n\\] where \\(k\\) is the number of layers in the neural network. But, there’s an issue with this approach: the output is still a linear combination of the inputs, which means that the model is still linear. Put differently, we could just multiply all the weight matrices together to get a single weight matrix \\(\\mathbf{W} = \\mathbf{W}_k \\mathbf{W}_{k-1} \\cdots \\mathbf{W}_1\\) before ever seeing the input \\(\\mathbf{x}\\). Our solution is to add a non-linear activation function after each layer. Non-linear activation functions include:\n\nReLU (Rectified Linear Unit): \\(f(x) = \\max(0, x)\\), which is a piecewise linear function that is \\(0\\) for negative inputs and linear for positive inputs.\nSigmoid: \\(f(x) = \\frac{1}{1 + e^{-x}}\\), which is a smooth function that maps inputs to the range \\((0, 1)\\).\nTanh (Hyperbolic Tangent): \\(f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\), which is a smooth function that maps inputs to the range \\((-1, 1)\\).\n\nLet \\(\\sigma\\) be one of these non-linear activation functions. Then we can write the output of the neural network as \\[\nf(\\mathbf{x}) = \\sigma(\\mathbf{W}_k \\sigma(\\mathbf{W}_{k-1} \\cdots \\sigma(\\mathbf{W}_1 \\mathbf{x}))).\n\\]\nWhich is now doing something non-linear and interesting! This is so interesting, in fact, that we don’t really understand mathematically what is going on.\nBut our gradient descent approach for linear regression still works:\n\nModel Class: The model class is now the set of neural networks with a given architecture (number of layers, number of neurons per layer, and activation function).\nLoss: The loss function is still the mean squared error loss, which measures how well the model fits the data.\nOptimizer: The optimizer is still gradient descent, which computes the gradient of the loss function with respect to the model parameters in each layer and updates the parameters in the direction of the negative gradient.\n\n\nComplexity versus Generalization\nIf neural network models are so powerful, why not just use a neural network for every problem? My personal opinion is that neural networks are far less interesting to study because they’re so complex, but there’s a deeper reason as well. Neural networks are so powerful that they can fit any data, even random noise. This is a problem because we want our model to generalize to new data, not just perfectly fit the training data. This is known as overfitting, and it occurs when the model is too complex for the data.\n(Put image here showing overfitting with linear model and neural network)\nWhen we believe our data comes from a simpler generating process, it makes sense to use a simpler model. Even when that simpler generating process is not a linear model, we can attempt to hack the process through regularization. Regularization is a technique that adds a penalty to the loss function to discourage the model from fitting the training data too closely. The idea is that we can keep the model “simple” by penalizing large weights, which would otherwise allow the model to achieve large changes in the output for small changes in the input.\n(Put image here showing fitting quadratic function with linear model and neural network, and regularized neural network)"
  },
  {
    "objectID": "notes/02_LinearRegressionPlus.html#going-forward",
    "href": "notes/02_LinearRegressionPlus.html#going-forward",
    "title": "Linear Regression and Optimization",
    "section": "Going Forward",
    "text": "Going Forward\nThere is a whole deep learning course dedicated to neural networks, which covers different types of layers, activation functions, and methods of cleverly applying these approaches to solve different problems, particularly in generative AI. For our purposes, we will focus on the mathematical foundations of machine learning, such as what to do in different supervised learning settings, how to optimize different types of models, and the interesting tensions between different approaches.\nWe have so far been focused in the regression setting, where the labels are real numbers. The question we will consider next is how to handle the case where the labels are categorical, e.g., we want to classify the data into two categories such as “cat” and “dog” or “spam” and “not spam”."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH 166: Data Mining",
    "section": "",
    "text": "A course on the mathematical foundations of machine learning.\n\n\n\n\nInstructor: R. Teal Witter. Please call me Teal.\nClass Times: We meet Tuesdays and Thursdays. The first section is from 2:45 to 4:00pm and the second section is from 4:15 to 5:30pm.\nOffice Hours: I will hold office hours TBD.\nParticipation: I expect you to engage in class, ask questions, and make connections. To receive credit, please fill out this form after every lecture.\nQuizzes: There will be short quizzes at the beginning of our Tues classes. These quizzes will test your understanding of the problem sets and the concepts from the prior week.\n\n\nProblem Sets: Your primary opportunity to learn the material will be on problem sets. You may work with others to solve the problems, but you must write your solutions by yourself, and explicitly acknowledge any outside help (websites, people, LLMs).\nExams: There will be two midterms: The first is a written exam that covers most of the supervised learning topics. The second is a cumulative oral exam.\nProject: You will complete a project on a topic of your choice. You will write a report and give a presentation to the class. The project is due at the end of the semester.\n\n\n\n\n\n\nWeek\n\n\nTopic\n\n\nReading\n\n\nSlides\n\n\nAssignments\n\n\n\n\nLinear Algebra and Probability Review\n\n\n\n\nTues 8/27 + Thurs 8/29\n\n\nPageRank\n\n\nNotes\n\n\n\n\n\n\n\n\nSupervised Learning\n\n\n\n\nTues 9/2 + Thurs 9/4\n\n\nLinear Regression and Optimization\n\n\nNotes\n\n\n\n\n\n\n\n\nTues 9/9 + Thurs 9/11\n\n\nLinear Regression and Gradient Descent\n\n\nNotes\n\n\n\n\n\n\n\n\nTues 9/16 + Thurs 9/18\n\n\nSupport Vector Machines and Optimization\n\n\n\n\n\n\n\n\n\n\nTues 9/23 + Thurs 9/25\n\n\nLogistic Regression\n\n\n\n\n\n\n\n\n\n\nTues 9/30 + Thurs 10/2\n\n\nNeural Networks\n\n\n\n\n\n\n\n\n\n\nTues 10/7 + Thurs 10/9\n\n\nDecision Trees and Boosting\n\n\n\n\n\n\n\n\n\n\nTues 10/14 + Thurs 10/16\n\n\nWritten Exam\n\n\n\n\n\n\n\n\n\n\nTues 10/21 + Thurs 10/23\n\n\nK Nearest Neighbors and Kernel Methods\n\n\n\n\n\n\n\n\n\n\nUnsupervised Learning\n\n\n\n\nTues 10/28 + Thurs 10/30\n\n\nAutoencoders\n\n\n\n\n\n\n\n\n\n\nTues 11/4 + Thurs 11/6\n\n\nPrincipal Component Analysis\n\n\n\n\n\n\n\n\n\n\nTues 11/11 + Thurs 11/13\n\n\nSemantic Embeddings\n\n\n\n\n\n\n\n\n\n\nTues 11/18 + Thurs 11/20\n\n\n\n\n\n\n\n\n\n\n\n\nTues 11/25 + Thurs 11/27\n\n\nOral Exam\n\n\n\n\n\n\n\n\n\n\nTues 12/2 + Thurs 12/4\n\n\nInterpretability and Active Regression"
  },
  {
    "objectID": "notes/00_PageRank.html",
    "href": "notes/00_PageRank.html",
    "title": "Review via PageRank",
    "section": "",
    "text": "When I first heard about machine learning, I imagined a computer that was rewarded every time it gave the right answer. Maybe there were electric carrots and sticks that no one had bothered to tell me about? While I now know as little as I did then about computer hardware, I have learned that machine learning is fundamentally a mathematical process.\nLuckily, we’ve been learning about the very mathematical ideas that make machine learning work for years! We’ll review the basics of these concepts and then jump in to linear regression, arguably the foundation of neural networks.\n\n\nImagine a function \\(\\mathcal{L}: \\mathbb{R} \\to \\mathbb{R}\\). (Instead of the usual \\(f\\), we’ll use \\(\\mathcal{L}\\) for reasons that will soon become clear.) The mapping notation means that \\(\\mathcal{L}\\) takes a single real number as input and outputs a single real number. In general, mathematicians tell us to be careful about whether we can differentiate a function. But, we’re computer scientists so we’ll risk it for the biscuit.\nLet \\(z \\in \\mathbb{R}\\) be the input to \\(\\mathcal{L}\\). The derivative of \\(\\mathcal{L}\\) with respect to its input \\(z\\) is mathematically denoted by \\(\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\\).\nFormally, the derivative is defined as \\[\n\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\n= \\lim_{h \\to 0} \\frac{\\mathcal{L}(z + h) - \\mathcal{L}(z)}{h}.\n\\] If we were to plot \\(\\mathcal{L}\\), the derivative at a point \\(z\\) would be the slope of the tangent line to the curve at that point.\nHere are several examples of functions and their derivatives that you might remember from calculus.\n\n\n\nFunction: \\(\\mathcal{L}(z)\\) \n\n\nDerivative: \\(\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\\)\n\n\n\n\n\\[z^2\\]\n\n\n\\[2z\\]\n\n\n\n\n\\[z^a\\]\n\n\n\\[a z^{a-1}\\]\n\n\n\n\n\\[az + b\\]\n\n\n\\[a\\]\n\n\n\n\n\\[\\ln(z)\\]\n\n\n\\[\\frac{1}{z}\\]\n\n\n\n\n\\[e^z\\]\n\n\n\\[e^z\\]\n\n\n\n\n\n\nWhile working with a simple basic function is easy, we’re not always so lucky. Modern machine learning chains many many complicated functions together. Fortunately, we will think of these operations modularly.\nLet \\(g: \\mathbb{R} \\to \\mathbb{R}\\) be another function. Consider the composite function \\(g(\\mathcal{L}(z))\\).\nBy the chain rule, the derivative of \\(g(\\mathcal{L}(z))\\) with respect to \\(z\\) is \\[\n\\frac{\\partial }{\\partial z}[g(\\mathcal{L}(z))]\n= \\frac{\\partial g}{\\partial z}(\\mathcal{L}(z))\n\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)].\n\\]\nOften, we will also multiply functions together. The product rule tells us that \\[\n\\frac{\\partial }{\\partial z}[g(z) \\mathcal{L}(z)]\n= g(z) \\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\n+ \\mathcal{L}(z) \\frac{\\partial}{\\partial z}[g(z)].\n\\]\n\n\n\nIn machine learning, we process high-dimensional data so we are interested in functions with multivariate input. Consider \\(\\mathcal{L}: \\mathbb{R}^d \\to \\mathbb{R}\\). The output of the function is still a real number but the input consists of \\(d\\) real numbers. We will use the vector \\(\\mathbf{z} \\in \\mathbb{R}^d\\) to represent all \\(d\\) inputs \\(z_1, \\ldots, z_d\\).\nInstead of the derivative, we will talk use the partial derivative. The partial derivative with respect to \\(z_i\\) is denoted by \\(\\frac{\\partial}{\\partial z_i}[\\mathcal{L}(\\mathbf{z})]\\). In effect, the partial derivative tells us how \\(\\mathcal{L}\\) changes when we change \\(z_i\\), while keeping all other inputs fixed.\nThe gradient stores all the partial derivatives in a vector. The \\(i\\)th entry of this vector is given by the partial derivative of \\(\\mathcal{L}\\) with respect to \\(z_i\\). In mathematical notation, \\[\n\\nabla_\\mathbf{z} \\mathcal{L} = \\left[\\begin{smallmatrix} \\frac{\\partial}{\\partial z_1}[\\mathcal{L}(\\mathbf{z})] \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial z_d}[\\mathcal{L}(\\mathbf{z})] \\\\ \\end{smallmatrix}\\right]\n\\]\nJust like the derivative in one dimension, the gradient contains information about the slope of \\(\\mathcal{L}\\) with respect to each of the \\(d\\) dimensions in its input.\n\n\n\nVector and matrix multiplication lives at the heart of deep learning. In fact, deep learning really started to take off when researchers realized that the Graphical Processing Unit (GPU) could be used to perform gradient updates in addition to the matrix multiplication it was designed to do for gaming.\nConsider two vectors \\(\\mathbf{u} \\in \\mathbb{R}^d\\) and \\(\\mathbf{v} \\in \\mathbb{R}^d\\). We will use \\(\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^d u_i v_i\\) to denote the inner product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). The \\(\\mathcal{\\ell}_2\\)-norm of \\(v\\) is given by \\(\\|\\mathbf{v}\\|_2 = \\sqrt{\\mathbf{u} \\cdot \\mathbf{u}}\\).\nConsider two matrices: \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times m}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{m \\times n}\\) where \\(d \\neq n\\). We can only multiply two matrices when their inner dimension agrees. Because the number of columns in \\(\\mathbf{A}\\) is the same as the number of rows in \\(\\mathbf{B}\\), we can compute \\(\\mathbf{AB}\\). However, because the number of columns in \\(\\mathbf{B}\\) is not the same as the number of rows in \\(\\mathbf{A}\\), the product \\(\\mathbf{BA}\\) is not defined.\nWhen we can multiply two matrices, the \\((i,j)\\) entry in \\(\\mathbf{AB}\\) is given by the inner product between the \\(i\\)th row of \\(\\mathbf{A}\\) and the \\(j\\)th column of \\(\\mathbf{B}\\). The resulting dimensions of the matrix product will be the number of rows in \\(\\mathbf{A}\\) by the number of columns in \\(\\mathbf{B}\\).\n\n\n\nIf we have a scalar equation \\(ax = b\\), we can simply solve for \\(x\\) by dividing both sides by \\(a\\). In effect, we are applying the inverse of \\(a\\) to \\(a\\) i.e., \\(\\frac1{a} a =1\\). The same principle applies to matrices. The \\(n \\times n\\) identity matrix generalizes the scalar identity \\(1\\). This identity matrix is denoted by \\(\\mathbf{I}_{n \\times n} \\in \\mathbb{R}^{n \\times n}\\): the on-diagonal entries \\((i,i)\\) are 1 while the off-diagonal entries \\((i,j)\\) for \\(i\\neq j\\) are 0.\nConsider the matrix equation \\(\\mathbf{Ax} = \\mathbf{b}\\) where \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times d}\\), \\(\\mathbf{x} \\in \\mathbb{R}^d\\), and \\(\\mathbf{b} \\in \\mathbb{R}^d\\). (Notice that the inner dimensions of \\(\\mathbf{A}\\) and \\(\\mathbf{x}\\) agree so their multiplication is well-defined, and the resulting vector is the same dimension as \\(\\mathbf{b}\\).)\nIf we want to solve for \\(\\mathbf{x}\\), we can use the matrix inverse. For a matrix \\(\\mathbf{A}\\), we use \\(\\mathbf{A}^{-1}\\) to denote its inverse. The inverse is defined so that \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\) where \\(\\mathbf{I}_{n \\times n}\\) is the identity matrix. Then, we can solve for \\(\\mathbf{x}\\) by multiplying both sides of the equation by \\(\\mathbf{A}^{-1}\\). \\[\n\\mathbf{A}^{-1} \\mathbf{Ax} = \\mathbf{A}^{-1} \\mathbf{b}\n\\] Since \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\), we have that \\(\\mathbf{I}_{n \\times n} \\mathbf{x} = \\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}\\)."
  },
  {
    "objectID": "notes/00_PageRank.html#math-review",
    "href": "notes/00_PageRank.html#math-review",
    "title": "Review via PageRank",
    "section": "",
    "text": "When I first heard about machine learning, I imagined a computer that was rewarded every time it gave the right answer. Maybe there were electric carrots and sticks that no one had bothered to tell me about? While I now know as little as I did then about computer hardware, I have learned that machine learning is fundamentally a mathematical process.\nLuckily, we’ve been learning about the very mathematical ideas that make machine learning work for years! We’ll review the basics of these concepts and then jump in to linear regression, arguably the foundation of neural networks.\n\n\nImagine a function \\(\\mathcal{L}: \\mathbb{R} \\to \\mathbb{R}\\). (Instead of the usual \\(f\\), we’ll use \\(\\mathcal{L}\\) for reasons that will soon become clear.) The mapping notation means that \\(\\mathcal{L}\\) takes a single real number as input and outputs a single real number. In general, mathematicians tell us to be careful about whether we can differentiate a function. But, we’re computer scientists so we’ll risk it for the biscuit.\nLet \\(z \\in \\mathbb{R}\\) be the input to \\(\\mathcal{L}\\). The derivative of \\(\\mathcal{L}\\) with respect to its input \\(z\\) is mathematically denoted by \\(\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\\).\nFormally, the derivative is defined as \\[\n\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\n= \\lim_{h \\to 0} \\frac{\\mathcal{L}(z + h) - \\mathcal{L}(z)}{h}.\n\\] If we were to plot \\(\\mathcal{L}\\), the derivative at a point \\(z\\) would be the slope of the tangent line to the curve at that point.\nHere are several examples of functions and their derivatives that you might remember from calculus.\n\n\n\nFunction: \\(\\mathcal{L}(z)\\) \n\n\nDerivative: \\(\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\\)\n\n\n\n\n\\[z^2\\]\n\n\n\\[2z\\]\n\n\n\n\n\\[z^a\\]\n\n\n\\[a z^{a-1}\\]\n\n\n\n\n\\[az + b\\]\n\n\n\\[a\\]\n\n\n\n\n\\[\\ln(z)\\]\n\n\n\\[\\frac{1}{z}\\]\n\n\n\n\n\\[e^z\\]\n\n\n\\[e^z\\]\n\n\n\n\n\n\nWhile working with a simple basic function is easy, we’re not always so lucky. Modern machine learning chains many many complicated functions together. Fortunately, we will think of these operations modularly.\nLet \\(g: \\mathbb{R} \\to \\mathbb{R}\\) be another function. Consider the composite function \\(g(\\mathcal{L}(z))\\).\nBy the chain rule, the derivative of \\(g(\\mathcal{L}(z))\\) with respect to \\(z\\) is \\[\n\\frac{\\partial }{\\partial z}[g(\\mathcal{L}(z))]\n= \\frac{\\partial g}{\\partial z}(\\mathcal{L}(z))\n\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)].\n\\]\nOften, we will also multiply functions together. The product rule tells us that \\[\n\\frac{\\partial }{\\partial z}[g(z) \\mathcal{L}(z)]\n= g(z) \\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\n+ \\mathcal{L}(z) \\frac{\\partial}{\\partial z}[g(z)].\n\\]\n\n\n\nIn machine learning, we process high-dimensional data so we are interested in functions with multivariate input. Consider \\(\\mathcal{L}: \\mathbb{R}^d \\to \\mathbb{R}\\). The output of the function is still a real number but the input consists of \\(d\\) real numbers. We will use the vector \\(\\mathbf{z} \\in \\mathbb{R}^d\\) to represent all \\(d\\) inputs \\(z_1, \\ldots, z_d\\).\nInstead of the derivative, we will talk use the partial derivative. The partial derivative with respect to \\(z_i\\) is denoted by \\(\\frac{\\partial}{\\partial z_i}[\\mathcal{L}(\\mathbf{z})]\\). In effect, the partial derivative tells us how \\(\\mathcal{L}\\) changes when we change \\(z_i\\), while keeping all other inputs fixed.\nThe gradient stores all the partial derivatives in a vector. The \\(i\\)th entry of this vector is given by the partial derivative of \\(\\mathcal{L}\\) with respect to \\(z_i\\). In mathematical notation, \\[\n\\nabla_\\mathbf{z} \\mathcal{L} = \\left[\\begin{smallmatrix} \\frac{\\partial}{\\partial z_1}[\\mathcal{L}(\\mathbf{z})] \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial z_d}[\\mathcal{L}(\\mathbf{z})] \\\\ \\end{smallmatrix}\\right]\n\\]\nJust like the derivative in one dimension, the gradient contains information about the slope of \\(\\mathcal{L}\\) with respect to each of the \\(d\\) dimensions in its input.\n\n\n\nVector and matrix multiplication lives at the heart of deep learning. In fact, deep learning really started to take off when researchers realized that the Graphical Processing Unit (GPU) could be used to perform gradient updates in addition to the matrix multiplication it was designed to do for gaming.\nConsider two vectors \\(\\mathbf{u} \\in \\mathbb{R}^d\\) and \\(\\mathbf{v} \\in \\mathbb{R}^d\\). We will use \\(\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^d u_i v_i\\) to denote the inner product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). The \\(\\mathcal{\\ell}_2\\)-norm of \\(v\\) is given by \\(\\|\\mathbf{v}\\|_2 = \\sqrt{\\mathbf{u} \\cdot \\mathbf{u}}\\).\nConsider two matrices: \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times m}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{m \\times n}\\) where \\(d \\neq n\\). We can only multiply two matrices when their inner dimension agrees. Because the number of columns in \\(\\mathbf{A}\\) is the same as the number of rows in \\(\\mathbf{B}\\), we can compute \\(\\mathbf{AB}\\). However, because the number of columns in \\(\\mathbf{B}\\) is not the same as the number of rows in \\(\\mathbf{A}\\), the product \\(\\mathbf{BA}\\) is not defined.\nWhen we can multiply two matrices, the \\((i,j)\\) entry in \\(\\mathbf{AB}\\) is given by the inner product between the \\(i\\)th row of \\(\\mathbf{A}\\) and the \\(j\\)th column of \\(\\mathbf{B}\\). The resulting dimensions of the matrix product will be the number of rows in \\(\\mathbf{A}\\) by the number of columns in \\(\\mathbf{B}\\).\n\n\n\nIf we have a scalar equation \\(ax = b\\), we can simply solve for \\(x\\) by dividing both sides by \\(a\\). In effect, we are applying the inverse of \\(a\\) to \\(a\\) i.e., \\(\\frac1{a} a =1\\). The same principle applies to matrices. The \\(n \\times n\\) identity matrix generalizes the scalar identity \\(1\\). This identity matrix is denoted by \\(\\mathbf{I}_{n \\times n} \\in \\mathbb{R}^{n \\times n}\\): the on-diagonal entries \\((i,i)\\) are 1 while the off-diagonal entries \\((i,j)\\) for \\(i\\neq j\\) are 0.\nConsider the matrix equation \\(\\mathbf{Ax} = \\mathbf{b}\\) where \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times d}\\), \\(\\mathbf{x} \\in \\mathbb{R}^d\\), and \\(\\mathbf{b} \\in \\mathbb{R}^d\\). (Notice that the inner dimensions of \\(\\mathbf{A}\\) and \\(\\mathbf{x}\\) agree so their multiplication is well-defined, and the resulting vector is the same dimension as \\(\\mathbf{b}\\).)\nIf we want to solve for \\(\\mathbf{x}\\), we can use the matrix inverse. For a matrix \\(\\mathbf{A}\\), we use \\(\\mathbf{A}^{-1}\\) to denote its inverse. The inverse is defined so that \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\) where \\(\\mathbf{I}_{n \\times n}\\) is the identity matrix. Then, we can solve for \\(\\mathbf{x}\\) by multiplying both sides of the equation by \\(\\mathbf{A}^{-1}\\). \\[\n\\mathbf{A}^{-1} \\mathbf{Ax} = \\mathbf{A}^{-1} \\mathbf{b}\n\\] Since \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\), we have that \\(\\mathbf{I}_{n \\times n} \\mathbf{x} = \\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}\\)."
  },
  {
    "objectID": "notes/01_LinearRegression.html",
    "href": "notes/01_LinearRegression.html",
    "title": "Linear Regression and Optimization",
    "section": "",
    "text": "Supervised learning is perhaps the most natural setting for machine learning. In this setting, we are given labeled data, and our goal is to train a model to approximately match the labels. The supervised learning problem is versatile and powerful, some examples include:\n\nPredicting temperature based on present weather conditions,\nIdentifying the type of animal in an image, and\nGenerating the next word in a sentence.\n\nWe can encompass all of these settings and more with the following mathematical formulation: Concretely, we are given \\(n\\) data points \\(\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(n)} \\in \\mathbb{R}^d\\), each with \\(d\\) dimensions, and associated labels \\(y^{(1)}, y^{(2)}, \\ldots, y^{(n)} \\in \\mathbb{R}\\). Our goal is to learn a model \\(f: \\mathbb{R}^d \\to \\mathbb{R}\\) so that \\(f(\\mathbf{x}^{(i)}) \\approx y^{(i)}\\) for all data points \\(i \\in \\{1,2,\\ldots,n\\}\\).\nOur general approach to solving supervised learning problems will be to use empirical risk minimization, which gives a flexible scaffolding that encompasses many of the topics we’ll discuss in this course. Given a model class (e.g., linear models or neural networks), the idea is to select the model that most closely explains the data. In particular, there are three components to empirical risk minimization:\n\nModel Class: The model class \\(\\mathcal{F}\\) from which we will select the model \\(f\\) that most closely fits the observed data.\nLoss: The loss function that measures how well a function \\(f\\) fits the observed data. (Without loss of generality, we will assume that lower is better.)\nOptimizer: The method of selecting the model from the model class.\n\nEmpirical risk minimization is an abstract idea. Luckily, we will revisit it again and again. Our first example will be linear regression, where the model class is the set of linear functions and the loss is the squared difference between the true label and our prediction. Let’s dive in!"
  },
  {
    "objectID": "notes/01_LinearRegression.html#the-supervised-learning-problem",
    "href": "notes/01_LinearRegression.html#the-supervised-learning-problem",
    "title": "Linear Regression and Optimization",
    "section": "",
    "text": "Supervised learning is perhaps the most natural setting for machine learning. In this setting, we are given labeled data, and our goal is to train a model to approximately match the labels. The supervised learning problem is versatile and powerful, some examples include:\n\nPredicting temperature based on present weather conditions,\nIdentifying the type of animal in an image, and\nGenerating the next word in a sentence.\n\nWe can encompass all of these settings and more with the following mathematical formulation: Concretely, we are given \\(n\\) data points \\(\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(n)} \\in \\mathbb{R}^d\\), each with \\(d\\) dimensions, and associated labels \\(y^{(1)}, y^{(2)}, \\ldots, y^{(n)} \\in \\mathbb{R}\\). Our goal is to learn a model \\(f: \\mathbb{R}^d \\to \\mathbb{R}\\) so that \\(f(\\mathbf{x}^{(i)}) \\approx y^{(i)}\\) for all data points \\(i \\in \\{1,2,\\ldots,n\\}\\).\nOur general approach to solving supervised learning problems will be to use empirical risk minimization, which gives a flexible scaffolding that encompasses many of the topics we’ll discuss in this course. Given a model class (e.g., linear models or neural networks), the idea is to select the model that most closely explains the data. In particular, there are three components to empirical risk minimization:\n\nModel Class: The model class \\(\\mathcal{F}\\) from which we will select the model \\(f\\) that most closely fits the observed data.\nLoss: The loss function that measures how well a function \\(f\\) fits the observed data. (Without loss of generality, we will assume that lower is better.)\nOptimizer: The method of selecting the model from the model class.\n\nEmpirical risk minimization is an abstract idea. Luckily, we will revisit it again and again. Our first example will be linear regression, where the model class is the set of linear functions and the loss is the squared difference between the true label and our prediction. Let’s dive in!"
  },
  {
    "objectID": "notes/01_LinearRegression.html#univariate-linear-regression",
    "href": "notes/01_LinearRegression.html#univariate-linear-regression",
    "title": "Linear Regression and Optimization",
    "section": "Univariate Linear Regression",
    "text": "Univariate Linear Regression\nLinear regression is a simple but powerful tool that we will use to understand the basics of machine learning. For simplicity, we will first consider the univariate case where the inputs are all one-dimensional i.e., \\(x^{(1)}, \\ldots, x^{(n)} \\in \\mathbb{R}\\).\n\nLinear Models\nAs its name suggests, linear regression uses a linear model to process the input into an approximation of the output. Let \\(w \\in \\mathbb{R}\\) be a weight parameter. The linear model (for one-dimensional inputs) is given by \\(f(x) = wx\\).\nUnlike many machine learning models, we can visualize the linear model since it is given by a line. In the plot, we have the \\(n=10\\) data points plotted in two dimensions. There is one linear model \\(f(x) = 2x\\) that closely approximates the data and another linear model \\(f(x)=\\frac12 x\\) that poorly approximates the data.\n\n\n\nOur goal is to learn how to find a linear model that fits the data well. Before we can do this, though, we will need to define what it means for a model to “fit the data well”.\n\n\nMean Squared Error Loss\nOur goal for the loss function is to measure how closely the data fits the prediction made by our model. Intuitively, we should take the difference between the prediction and the true outcome \\(f(x^{(i)})-y^{(i)}\\).\nThe issue with this approach is that \\(f(x^{(i)})-y^{(i)}\\) can be small (negative) even when \\(f(x^{(i)}) \\neq y^{(i)}\\). A natural fix is to take the absolute value \\(|f(x^{(i)}) - y^{(i)}|\\). The benefit of the absolute value is that the loss is \\(0\\) if and only if \\(f(x^{(i)}) = y^{(i)}\\). However, the absolute value function is not differentiable, which is a property we’ll need for optimization. Instead, we use the squared loss:\n\\(\\mathcal{L}(w) = \\frac1{n} \\sum_{i=1}^n (f(x^{(i)}) - y^{(i)})^2.\\)\nHere, we use the mean squared error loss, which is the average squared difference between the prediction and the true output over the dataset. Unlike the absolute value function, the squared function is differentiable everywhere. In addition, the squared error disproportionately penalizes predictions that are far from the true labels, a property that may be desirable when we want all of our predictions to be reasonably accurate.\n\n\n\nThe plot above compares the squared function to the absolute value function. While both are \\(0\\) if and only if their input is \\(0\\), the squared function is differentiable everywhere and penalizes large errors more.\n\n\nExact Optimization\nWe now have our model class and loss function: linear models and mean squared error loss. The question becomes how to update the weights of the model to minimize the loss. In particular, we want to find \\(w\\) that minimizes \\(\\mathcal{L}(w)\\). While the language we’re using is new, the problem is not. We’ve actually been studying how to do this since pre-calculus!\nThe squared loss is convex (a bowl facing up versus the downward facing cave of concave); see the plot above for a ‘proof’ by example. In this case, we know there is only one minimum. Not only that but we can find the minimum by setting the derivative to \\(0\\).\nAs such, our game plan is to set \\(\\frac{\\partial \\mathcal{L}}{\\partial w}\\) to \\(0\\) and solve for \\(w\\). Recall that \\(f(x) = wx\\). We will use the linearity of the derivative, the chain rule, and the power rule to compute the derivative of \\(\\mathcal{L}\\) with respect to \\(w\\):\n\\[\n\\begin{align}\n\\frac{\\partial}{\\partial w}[\\mathcal{L}(w)]\n&= \\frac1{n} \\sum_{i=1}^n \\frac{\\partial}{\\partial w} [(f(x^{(i)}) - y^{(i)})^2]\n\\notag \\\\&= \\frac1{n} \\sum_{i=1}^n 2(f(x^{(i)}) - y^{(i)}) \\frac{\\partial}{\\partial w} [(f(x^{(i)}) - y^{(i)})]\n\\notag \\\\&= \\frac1{n} \\sum_{i=1}^n 2(w x^{(i)} - y^{(i)}) x^{(i)}.\n\\end{align}\n\\]\nSetting the derivative to \\(0\\) and solving for \\(w\\), we get \\(\\frac2{n} \\sum_{i=1}^n w \\cdot (x^{(i)})^2 = \\frac2{n} \\sum_{i=1}^n y^{(i)} x^{(i)}\\) and so \\[\nw = \\frac{\\sum_{i=1}^n y^{(i)} \\cdot x^{(i)}}{\\sum_{i=1}^n (x^{(i)})^2}.\n\\]\nThis is the exact solution to the univariate linear regression problem! We can now use this formula to find the best linear model for our univariate data. However, we’ll have to work slightly harder for the general case with multidimensional data."
  },
  {
    "objectID": "notes/01_LinearRegression.html#multivariate-linear-regression",
    "href": "notes/01_LinearRegression.html#multivariate-linear-regression",
    "title": "Linear Regression and Optimization",
    "section": "Multivariate Linear Regression",
    "text": "Multivariate Linear Regression\nConsider the more general setting where the input is \\(d\\)-dimensional. As before, we observe \\(n\\) training observations \\((\\mathbf{x}^{(1)}, y^{(1)}), \\ldots, (\\mathbf{x}^{(n)}, y^{(n)})\\) but now \\(\\mathbf{x}^{(i)} \\in \\mathbb{R}^d\\). We will generalize the ideas from univariate linear regression to the multivariate setting.\n\nLinear Model\nInstead of using a single weight \\(w \\in \\mathbb{R}\\), we will use \\(d\\) weights \\(\\mathbf{w} \\in \\mathbb{R}^d\\). Then the model is given by \\(f(x) = \\langle \\mathbf{w}, \\mathbf{x} \\rangle\\).\nInstead of using a line to fit the data, we use a hyperplane. While visualizing the model is difficult in high dimensions, we can still see the model when \\(d=2\\).\n\n\n\nIn the plot above, we have \\(n=10\\) data points in 3 dimensions. There is one linear model \\(\\mathbf{w} = \\begin{bmatrix} 2 \\\\ \\frac12 \\end{bmatrix}\\) that closely approximates the data and another linear model \\(\\mathbf{w} = \\begin{bmatrix} \\frac12 \\\\ 0 \\end{bmatrix}\\) that poorly approximates the data.\n\n\nMean Squared Error\nSince the output of \\(f\\) is still a single real number, we do not have to change the loss function. However, we can use our linear algebra notation to write the mean squared error in an elegant way.\nLet \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) be the data matrix where the \\(i\\)th row is \\((\\mathbf{x}^{(i)})^\\top\\). Similarly, let \\(\\mathbf{y} \\in \\mathbb{R}^n\\) be the target vector where the \\(i\\)th entry is \\(y^{(i)}\\). We can then write the mean squared error loss as \\[\n\\mathcal{L}(\\mathbf{w}) = \\frac1{n} \\| \\mathbf{X w - y} \\|_2^2.\n\\]\n\n\nExact Optimization\nJust like computing the derivative and setting it to \\(0\\), we can compute the gradient and set it to the zero vector \\(\\mathbf{0} \\in \\mathbb{R}^d\\). In mathematical notation, we will set \\(\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}^*) = \\mathbf{0}\\) and solve for \\(\\mathbf{w}^*\\). The intuition is that such a point is a local minimum in every direction; that is, we cannot improve the loss by moving in any of the dimensions. Since the loss is convex (i.e., there can be only one minima), such a point is the unique global minimum and achieves the optimal loss.\nWe will leave computing \\(\\mathbf{w}^*\\) as an exercise on the homework. As you will show, \\(\\mathbf{w}^* = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\\)."
  },
  {
    "objectID": "notes/01_LinearRegression.html#empirical-risk-minimization",
    "href": "notes/01_LinearRegression.html#empirical-risk-minimization",
    "title": "Linear Regression and Optimization",
    "section": "Empirical Risk Minimization",
    "text": "Empirical Risk Minimization\nWe have now seen how to fit a linear model to data using the mean squared error loss. However, we have not given a satisfying answer to the question:\n\nWhy use mean squared error as our loss function?\n\n\nSo far, our answer has been that the quadratic function is differentiable (which we use to find the optimal solution), and that it naturally penalizes predictions which are farther away more. The first point is one of convenience and, a priori, should not be particularly persuasive. The second seems somewhat arbitrary, why penalize at a quadratic rate rather than an e.g., quartic rate? We’ll now consider a more compelling answer.\nOn our way to the answer, let’s take a step back and consider another question:\n\nWhy fit the data with a linear model?\n\n\nWell, we may do so when we expect the data truly has a linear relationship with the labels. To make things interesting, we will assume that there is random noise added to the labels, but that this noise is mean-centered so that, on average, the labels come from the linear model. Concretely, we observe some point \\(\\mathbf{x}\\) with a label that comes from a linear model \\(\\mathbf{w}^*\\) but with added noise, i.e., \\[\ny= \\langle \\mathbf{w}^*, \\mathbf{x} \\rangle + \\eta.\n\\] We will model this noise as distributed from a normal distribution, i.e., \\(\\eta \\sim \\mathcal{N}(0, \\sigma^2)\\) for some unknown standard deviation \\(\\sigma\\). (To justify this choice, we imagine the noise as a sum of random variables from some other distribution(s) which, by the law of large numbers, will follow the normal distribution when the sum contains sufficiently many terms.)\nRecall that the goal of our empirical risk minimization strategy is to find the model which most closely aligns with the data, or, put differently, we want the model that most likely generated the data we observed. In order to compute this likelihood, we will use the probability density function of the normal distribution: The probability we observe a random variable \\(y\\) drawn from a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) is given by \\[\n\\frac1{\\sqrt{2\\pi \\sigma}} \\exp\\left( - \\frac{(y- \\mu)^2}{2\\sigma^2} \\right).\n\\] If the noisy linear model \\(\\mathbf{w}\\) did generate the training data \\((\\mathbf{x}^{(i)}, y^{(i)})\\), then the expectation of the generation would be \\(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle\\). Then, combined with the assumption that the training data was drawn independently, the probability of observing the training data is given by the product of the probabilities of each individual observation: \\[\n\\prod_{i=1}^n\n\\frac1{\\sqrt{2\\pi \\sigma}} \\exp\\left( - \\frac{(y^{(i)}- \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)^2}{2\\sigma^2} \\right).\n\\] Our goal is to find the model \\(\\mathbf{w}\\) that maximizes this likelihood i.e., \\[\n\\begin{align}\n&{\\arg\\max}_{\\mathbf{w} \\in \\mathbb{R}^d}\n\\prod_{i=1}^n\n\\frac{1}{\\sqrt{2\\pi \\sigma}} \\exp\\left( - \\frac{(y^{(i)}- \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)^2}{2\\sigma^2} \\right)\n\\notag \\\\\n&= {\\arg\\min}_{\\mathbf{w} \\in \\mathbb{R}^d}\n- \\log \\left(\n\\frac{1}{\\sqrt{2\\pi \\sigma}} \\exp\\left(\\sum_{i=1}^n - \\frac{(y^{(i)}- \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)^2}{2\\sigma^2} \\right)\\right)\n\\notag \\\\\n&= {\\arg\\min}_{\\mathbf{w} \\in \\mathbb{R}^d}\n- \\sum_{i=1}^n - (y^{(i)}- \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)^2.\n\\end{align}\n\\] Here, we used the following facts: maximizing an objective is equivalent to minimizing the negative of that objective, the logarithmic function is monotonically increasing so minimizing the likelihood is equivalent to minimizing the log-likelihood, the product of exponentials is the exponential of the sum, and removing a constant scalar factor or additive constant does not change the minimum.\nThe punchline is that the model \\(\\mathbf{w}\\) that maximizes the likelihood of observing the training data is the same model that minimizes the mean squared error loss. This is a powerful result that justifies our use of the mean squared error loss."
  },
  {
    "objectID": "notes/01_LinearRegression.html#looking-forward",
    "href": "notes/01_LinearRegression.html#looking-forward",
    "title": "Linear Regression and Optimization",
    "section": "Looking Forward",
    "text": "Looking Forward\nWhile we have seen the benefits of exactly optimizing linear regression models, there are several limitations that we will address.\nComputational Complexity We saw (and you will prove) that the exact solution to linear regression is given by \\(\\mathbf{w}^* = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\\). This requires building the matrix \\(\\mathbf{X}^\\top \\mathbf{X}\\), which takes \\(O(nd^2)\\) time, and then inverting it, which takes \\(O(d^3)\\). When we have a large number of data points \\(n\\) and/or a large number of features \\(d\\), this can be prohibitively expensive.\nModel Class Misspecification We have assumed that the data has a linear relationship (or close to linear relationship) with the labels. What happens when this is not true? That is, even the best linear model gives a poor approximation?"
  }
]